{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":27281,"status":"ok","timestamp":1684893034026,"user":{"displayName":"Quang Pham","userId":"17287612277936898722"},"user_tz":-420},"id":"BUgZjLeCiLYa","outputId":"a7497922-9f76-4685-b8d8-abab12fc651a"},"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[33m\r0% [Working]\u001b[0m\r            \rHit:1 http://security.ubuntu.com/ubuntu focal-security InRelease\n","\u001b[33m\r0% [Connecting to archive.ubuntu.com (185.125.190.39)] [Connected to cloud.r-pr\u001b[0m\r                                                                               \rHit:2 http://archive.ubuntu.com/ubuntu focal InRelease\n","\r                                                                               \rHit:3 http://archive.ubuntu.com/ubuntu focal-updates InRelease\n","\r                                                                               \rHit:4 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu focal InRelease\n","\r                                                                               \rHit:5 http://ppa.launchpad.net/cran/libgit2/ubuntu focal InRelease\n","\u001b[33m\r0% [Waiting for headers] [Connected to cloud.r-project.org (65.9.86.12)] [Conne\u001b[0m\r                                                                               \rHit:6 http://archive.ubuntu.com/ubuntu focal-backports InRelease\n","\u001b[33m\r0% [Connected to cloud.r-project.org (65.9.86.12)] [Waiting for headers] [Waiti\u001b[0m\r                                                                               \rHit:7 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu focal InRelease\n","\u001b[33m\r0% [Waiting for headers] [Connecting to ppa.launchpad.net (185.125.190.52)] [Wa\u001b[0m\r                                                                               \rHit:8 https://cloud.r-project.org/bin/linux/ubuntu focal-cran40/ InRelease\n","\u001b[33m\r                                                                               \r0% [Connecting to ppa.launchpad.net (185.125.190.52)] [Waiting for headers]\u001b[0m\r                                                                           \rHit:9 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu focal InRelease\n","\u001b[33m\r0% [Connecting to ppa.launchpad.net (185.125.190.52)] [Waiting for headers]\u001b[0m\r                                                                           \rHit:10 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64  InRelease\n","Hit:11 http://ppa.launchpad.net/ubuntugis/ppa/ubuntu focal InRelease\n","Reading package lists... Done\n","Building dependency tree       \n","Reading state information... Done\n","33 packages can be upgraded. Run 'apt list --upgradable' to see them.\n","spark-3.3.0-bin-hadoop3/\n","spark-3.3.0-bin-hadoop3/NOTICE\n","spark-3.3.0-bin-hadoop3/kubernetes/\n","spark-3.3.0-bin-hadoop3/kubernetes/tests/\n","spark-3.3.0-bin-hadoop3/kubernetes/tests/python_executable_check.py\n","spark-3.3.0-bin-hadoop3/kubernetes/tests/autoscale.py\n","spark-3.3.0-bin-hadoop3/kubernetes/tests/worker_memory_check.py\n","spark-3.3.0-bin-hadoop3/kubernetes/tests/py_container_checks.py\n","spark-3.3.0-bin-hadoop3/kubernetes/tests/decommissioning.py\n","spark-3.3.0-bin-hadoop3/kubernetes/tests/pyfiles.py\n","spark-3.3.0-bin-hadoop3/kubernetes/tests/decommissioning_cleanup.py\n","spark-3.3.0-bin-hadoop3/kubernetes/dockerfiles/\n","spark-3.3.0-bin-hadoop3/kubernetes/dockerfiles/spark/\n","spark-3.3.0-bin-hadoop3/kubernetes/dockerfiles/spark/decom.sh\n","spark-3.3.0-bin-hadoop3/kubernetes/dockerfiles/spark/entrypoint.sh\n","spark-3.3.0-bin-hadoop3/kubernetes/dockerfiles/spark/bindings/\n","spark-3.3.0-bin-hadoop3/kubernetes/dockerfiles/spark/bindings/R/\n","spark-3.3.0-bin-hadoop3/kubernetes/dockerfiles/spark/bindings/R/Dockerfile\n","spark-3.3.0-bin-hadoop3/kubernetes/dockerfiles/spark/bindings/python/\n","spark-3.3.0-bin-hadoop3/kubernetes/dockerfiles/spark/bindings/python/Dockerfile\n","spark-3.3.0-bin-hadoop3/kubernetes/dockerfiles/spark/Dockerfile\n","spark-3.3.0-bin-hadoop3/kubernetes/dockerfiles/spark/Dockerfile.java17\n","spark-3.3.0-bin-hadoop3/jars/\n","spark-3.3.0-bin-hadoop3/jars/spark-network-shuffle_2.12-3.3.0.jar\n","spark-3.3.0-bin-hadoop3/jars/shapeless_2.12-2.3.7.jar\n","spark-3.3.0-bin-hadoop3/jars/arrow-format-7.0.0.jar\n","spark-3.3.0-bin-hadoop3/jars/metrics-jvm-4.2.7.jar\n","spark-3.3.0-bin-hadoop3/jars/spark-tags_2.12-3.3.0.jar\n","spark-3.3.0-bin-hadoop3/jars/okhttp-3.12.12.jar\n","spark-3.3.0-bin-hadoop3/jars/hive-shims-common-2.3.9.jar\n","spark-3.3.0-bin-hadoop3/jars/spire_2.12-0.17.0.jar\n","spark-3.3.0-bin-hadoop3/jars/logging-interceptor-3.12.12.jar\n","spark-3.3.0-bin-hadoop3/jars/hive-serde-2.3.9.jar\n","spark-3.3.0-bin-hadoop3/jars/objenesis-3.2.jar\n","spark-3.3.0-bin-hadoop3/jars/spark-streaming_2.12-3.3.0.jar\n","spark-3.3.0-bin-hadoop3/jars/avro-1.11.0.jar\n","spark-3.3.0-bin-hadoop3/jars/zookeeper-3.6.2.jar\n","spark-3.3.0-bin-hadoop3/jars/spark-kvstore_2.12-3.3.0.jar\n","spark-3.3.0-bin-hadoop3/jars/kryo-shaded-4.0.2.jar\n","spark-3.3.0-bin-hadoop3/jars/hive-jdbc-2.3.9.jar\n","spark-3.3.0-bin-hadoop3/jars/jackson-annotations-2.13.3.jar\n","spark-3.3.0-bin-hadoop3/jars/commons-logging-1.1.3.jar\n","spark-3.3.0-bin-hadoop3/jars/commons-compiler-3.0.16.jar\n","spark-3.3.0-bin-hadoop3/jars/spark-sql_2.12-3.3.0.jar\n","spark-3.3.0-bin-hadoop3/jars/jdo-api-3.0.1.jar\n","spark-3.3.0-bin-hadoop3/jars/pickle-1.2.jar\n","spark-3.3.0-bin-hadoop3/jars/JLargeArrays-1.5.jar\n","spark-3.3.0-bin-hadoop3/jars/kubernetes-model-apiextensions-5.12.2.jar\n","spark-3.3.0-bin-hadoop3/jars/parquet-common-1.12.2.jar\n","spark-3.3.0-bin-hadoop3/jars/httpcore-4.4.14.jar\n","spark-3.3.0-bin-hadoop3/jars/commons-codec-1.15.jar\n","spark-3.3.0-bin-hadoop3/jars/parquet-format-structures-1.12.2.jar\n","spark-3.3.0-bin-hadoop3/jars/commons-io-2.11.0.jar\n","spark-3.3.0-bin-hadoop3/jars/stax-api-1.0.1.jar\n","spark-3.3.0-bin-hadoop3/jars/hive-storage-api-2.7.2.jar\n","spark-3.3.0-bin-hadoop3/jars/kubernetes-model-certificates-5.12.2.jar\n","spark-3.3.0-bin-hadoop3/jars/zookeeper-jute-3.6.2.jar\n","spark-3.3.0-bin-hadoop3/jars/spark-yarn_2.12-3.3.0.jar\n","spark-3.3.0-bin-hadoop3/jars/commons-collections4-4.4.jar\n","spark-3.3.0-bin-hadoop3/jars/jline-2.14.6.jar\n","spark-3.3.0-bin-hadoop3/jars/dropwizard-metrics-hadoop-metrics2-reporter-0.1.2.jar\n","spark-3.3.0-bin-hadoop3/jars/commons-pool-1.5.4.jar\n","spark-3.3.0-bin-hadoop3/jars/snakeyaml-1.30.jar\n","spark-3.3.0-bin-hadoop3/jars/xbean-asm9-shaded-4.20.jar\n","spark-3.3.0-bin-hadoop3/jars/lz4-java-1.8.0.jar\n","spark-3.3.0-bin-hadoop3/jars/cats-kernel_2.12-2.1.1.jar\n","spark-3.3.0-bin-hadoop3/jars/jta-1.1.jar\n","spark-3.3.0-bin-hadoop3/jars/istack-commons-runtime-3.0.8.jar\n","spark-3.3.0-bin-hadoop3/jars/jackson-module-scala_2.12-2.13.3.jar\n","spark-3.3.0-bin-hadoop3/jars/hive-common-2.3.9.jar\n","spark-3.3.0-bin-hadoop3/jars/shims-0.9.25.jar\n","spark-3.3.0-bin-hadoop3/jars/compress-lzf-1.1.jar\n","spark-3.3.0-bin-hadoop3/jars/commons-collections-3.2.2.jar\n","spark-3.3.0-bin-hadoop3/jars/audience-annotations-0.5.0.jar\n","spark-3.3.0-bin-hadoop3/jars/joda-time-2.10.13.jar\n","spark-3.3.0-bin-hadoop3/jars/jpam-1.1.jar\n","spark-3.3.0-bin-hadoop3/jars/jakarta.annotation-api-1.3.5.jar\n","spark-3.3.0-bin-hadoop3/jars/kubernetes-model-coordination-5.12.2.jar\n","spark-3.3.0-bin-hadoop3/jars/tink-1.6.1.jar\n","spark-3.3.0-bin-hadoop3/jars/metrics-json-4.2.7.jar\n","spark-3.3.0-bin-hadoop3/jars/spark-kubernetes_2.12-3.3.0.jar\n","spark-3.3.0-bin-hadoop3/jars/jersey-common-2.34.jar\n","spark-3.3.0-bin-hadoop3/jars/kubernetes-model-common-5.12.2.jar\n","spark-3.3.0-bin-hadoop3/jars/spark-graphx_2.12-3.3.0.jar\n","spark-3.3.0-bin-hadoop3/jars/parquet-hadoop-1.12.2.jar\n","spark-3.3.0-bin-hadoop3/jars/curator-recipes-2.13.0.jar\n","spark-3.3.0-bin-hadoop3/jars/zstd-jni-1.5.2-1.jar\n","spark-3.3.0-bin-hadoop3/jars/commons-math3-3.6.1.jar\n","spark-3.3.0-bin-hadoop3/jars/spark-tags_2.12-3.3.0-tests.jar\n","spark-3.3.0-bin-hadoop3/jars/orc-shims-1.7.4.jar\n","spark-3.3.0-bin-hadoop3/jars/netty-transport-4.1.74.Final.jar\n","spark-3.3.0-bin-hadoop3/jars/netty-tcnative-classes-2.0.48.Final.jar\n","spark-3.3.0-bin-hadoop3/jars/spark-launcher_2.12-3.3.0.jar\n","spark-3.3.0-bin-hadoop3/jars/antlr4-runtime-4.8.jar\n","spark-3.3.0-bin-hadoop3/jars/javax.jdo-3.2.0-m3.jar\n","spark-3.3.0-bin-hadoop3/jars/spark-hive-thriftserver_2.12-3.3.0.jar\n","spark-3.3.0-bin-hadoop3/jars/paranamer-2.8.jar\n","spark-3.3.0-bin-hadoop3/jars/spark-network-common_2.12-3.3.0.jar\n","spark-3.3.0-bin-hadoop3/jars/netty-all-4.1.74.Final.jar\n","spark-3.3.0-bin-hadoop3/jars/arrow-memory-core-7.0.0.jar\n","spark-3.3.0-bin-hadoop3/jars/orc-core-1.7.4.jar\n","spark-3.3.0-bin-hadoop3/jars/kubernetes-model-storageclass-5.12.2.jar\n","spark-3.3.0-bin-hadoop3/jars/flatbuffers-java-1.12.0.jar\n","spark-3.3.0-bin-hadoop3/jars/kubernetes-model-autoscaling-5.12.2.jar\n","spark-3.3.0-bin-hadoop3/jars/json4s-core_2.12-3.7.0-M11.jar\n","spark-3.3.0-bin-hadoop3/jars/stream-2.9.6.jar\n","spark-3.3.0-bin-hadoop3/jars/log4j-core-2.17.2.jar\n","spark-3.3.0-bin-hadoop3/jars/arpack-2.2.1.jar\n","spark-3.3.0-bin-hadoop3/jars/avro-ipc-1.11.0.jar\n","spark-3.3.0-bin-hadoop3/jars/hive-shims-2.3.9.jar\n","spark-3.3.0-bin-hadoop3/jars/datanucleus-api-jdo-4.2.4.jar\n","spark-3.3.0-bin-hadoop3/jars/gson-2.2.4.jar\n","spark-3.3.0-bin-hadoop3/jars/jersey-container-servlet-2.34.jar\n","spark-3.3.0-bin-hadoop3/jars/commons-compress-1.21.jar\n","spark-3.3.0-bin-hadoop3/jars/curator-client-2.13.0.jar\n","spark-3.3.0-bin-hadoop3/jars/opencsv-2.3.jar\n","spark-3.3.0-bin-hadoop3/jars/hive-exec-2.3.9-core.jar\n","spark-3.3.0-bin-hadoop3/jars/jsr305-3.0.0.jar\n","spark-3.3.0-bin-hadoop3/jars/netty-transport-native-epoll-4.1.74.Final-linux-x86_64.jar\n","spark-3.3.0-bin-hadoop3/jars/derby-10.14.2.0.jar\n","spark-3.3.0-bin-hadoop3/jars/spark-catalyst_2.12-3.3.0.jar\n","spark-3.3.0-bin-hadoop3/jars/commons-dbcp-1.4.jar\n","spark-3.3.0-bin-hadoop3/jars/scala-reflect-2.12.15.jar\n","spark-3.3.0-bin-hadoop3/jars/okio-1.14.0.jar\n","spark-3.3.0-bin-hadoop3/jars/hk2-api-2.6.1.jar\n","spark-3.3.0-bin-hadoop3/jars/netty-transport-native-epoll-4.1.74.Final-linux-aarch_64.jar\n","spark-3.3.0-bin-hadoop3/jars/hive-shims-0.23-2.3.9.jar\n","spark-3.3.0-bin-hadoop3/jars/spark-hive_2.12-3.3.0.jar\n","spark-3.3.0-bin-hadoop3/jars/jakarta.servlet-api-4.0.3.jar\n","spark-3.3.0-bin-hadoop3/jars/commons-lang3-3.12.0.jar\n","spark-3.3.0-bin-hadoop3/jars/automaton-1.11-8.jar\n","spark-3.3.0-bin-hadoop3/jars/commons-lang-2.6.jar\n","spark-3.3.0-bin-hadoop3/jars/kubernetes-model-core-5.12.2.jar\n","spark-3.3.0-bin-hadoop3/jars/ivy-2.5.0.jar\n","spark-3.3.0-bin-hadoop3/jars/hk2-utils-2.6.1.jar\n","spark-3.3.0-bin-hadoop3/jars/parquet-column-1.12.2.jar\n","spark-3.3.0-bin-hadoop3/jars/kubernetes-model-events-5.12.2.jar\n","spark-3.3.0-bin-hadoop3/jars/kubernetes-model-node-5.12.2.jar\n","spark-3.3.0-bin-hadoop3/jars/velocity-1.5.jar\n","spark-3.3.0-bin-hadoop3/jars/netty-buffer-4.1.74.Final.jar\n","spark-3.3.0-bin-hadoop3/jars/httpclient-4.5.13.jar\n","spark-3.3.0-bin-hadoop3/jars/JTransforms-3.1.jar\n","spark-3.3.0-bin-hadoop3/jars/netty-handler-4.1.74.Final.jar\n","spark-3.3.0-bin-hadoop3/jars/json4s-scalap_2.12-3.7.0-M11.jar\n","spark-3.3.0-bin-hadoop3/jars/slf4j-api-1.7.32.jar\n","spark-3.3.0-bin-hadoop3/jars/kubernetes-model-rbac-5.12.2.jar\n","spark-3.3.0-bin-hadoop3/jars/spark-repl_2.12-3.3.0.jar\n","spark-3.3.0-bin-hadoop3/jars/kubernetes-model-discovery-5.12.2.jar\n","spark-3.3.0-bin-hadoop3/jars/RoaringBitmap-0.9.25.jar\n","spark-3.3.0-bin-hadoop3/jars/jersey-server-2.34.jar\n","spark-3.3.0-bin-hadoop3/jars/netty-transport-classes-kqueue-4.1.74.Final.jar\n","spark-3.3.0-bin-hadoop3/jars/aircompressor-0.21.jar\n","spark-3.3.0-bin-hadoop3/jars/arrow-vector-7.0.0.jar\n","spark-3.3.0-bin-hadoop3/jars/jackson-core-2.13.3.jar\n","spark-3.3.0-bin-hadoop3/jars/jcl-over-slf4j-1.7.32.jar\n","spark-3.3.0-bin-hadoop3/jars/metrics-core-4.2.7.jar\n","spark-3.3.0-bin-hadoop3/jars/spark-core_2.12-3.3.0.jar\n","spark-3.3.0-bin-hadoop3/jars/jakarta.validation-api-2.0.2.jar\n","spark-3.3.0-bin-hadoop3/jars/jul-to-slf4j-1.7.32.jar\n","spark-3.3.0-bin-hadoop3/jars/netty-transport-native-kqueue-4.1.74.Final-osx-x86_64.jar\n","spark-3.3.0-bin-hadoop3/jars/guava-14.0.1.jar\n","spark-3.3.0-bin-hadoop3/jars/parquet-jackson-1.12.2.jar\n","spark-3.3.0-bin-hadoop3/jars/zjsonpatch-0.3.0.jar\n","spark-3.3.0-bin-hadoop3/jars/hive-service-rpc-3.1.2.jar\n","spark-3.3.0-bin-hadoop3/jars/metrics-graphite-4.2.7.jar\n","spark-3.3.0-bin-hadoop3/jars/oro-2.0.8.jar\n","spark-3.3.0-bin-hadoop3/jars/commons-text-1.9.jar\n","spark-3.3.0-bin-hadoop3/jars/libfb303-0.9.3.jar\n","spark-3.3.0-bin-hadoop3/jars/core-1.1.2.jar\n","spark-3.3.0-bin-hadoop3/jars/avro-mapred-1.11.0.jar\n","spark-3.3.0-bin-hadoop3/jars/datanucleus-rdbms-4.1.19.jar\n","spark-3.3.0-bin-hadoop3/jars/super-csv-2.2.0.jar\n","spark-3.3.0-bin-hadoop3/jars/hadoop-yarn-server-web-proxy-3.3.2.jar\n","spark-3.3.0-bin-hadoop3/jars/scala-compiler-2.12.15.jar\n","spark-3.3.0-bin-hadoop3/jars/log4j-api-2.17.2.jar\n","spark-3.3.0-bin-hadoop3/jars/osgi-resource-locator-1.0.3.jar\n","spark-3.3.0-bin-hadoop3/jars/spark-sketch_2.12-3.3.0.jar\n","spark-3.3.0-bin-hadoop3/jars/hadoop-client-api-3.3.2.jar\n","spark-3.3.0-bin-hadoop3/jars/arrow-memory-netty-7.0.0.jar\n","spark-3.3.0-bin-hadoop3/jars/json-1.8.jar\n","spark-3.3.0-bin-hadoop3/jars/kubernetes-model-policy-5.12.2.jar\n","spark-3.3.0-bin-hadoop3/jars/antlr-runtime-3.5.2.jar\n","spark-3.3.0-bin-hadoop3/jars/jackson-datatype-jsr310-2.13.3.jar\n","spark-3.3.0-bin-hadoop3/jars/jackson-databind-2.13.3.jar\n","spark-3.3.0-bin-hadoop3/jars/spark-mllib_2.12-3.3.0.jar\n","spark-3.3.0-bin-hadoop3/jars/netty-transport-classes-epoll-4.1.74.Final.jar\n","spark-3.3.0-bin-hadoop3/jars/threeten-extra-1.5.0.jar\n","spark-3.3.0-bin-hadoop3/jars/kubernetes-client-5.12.2.jar\n","spark-3.3.0-bin-hadoop3/jars/scala-library-2.12.15.jar\n","spark-3.3.0-bin-hadoop3/jars/kubernetes-model-metrics-5.12.2.jar\n","spark-3.3.0-bin-hadoop3/jars/snappy-java-1.1.8.4.jar\n","spark-3.3.0-bin-hadoop3/jars/activation-1.1.1.jar\n","spark-3.3.0-bin-hadoop3/jars/annotations-17.0.0.jar\n","spark-3.3.0-bin-hadoop3/jars/commons-crypto-1.1.0.jar\n","spark-3.3.0-bin-hadoop3/jars/log4j-1.2-api-2.17.2.jar\n","spark-3.3.0-bin-hadoop3/jars/libthrift-0.12.0.jar\n","spark-3.3.0-bin-hadoop3/jars/HikariCP-2.5.1.jar\n","spark-3.3.0-bin-hadoop3/jars/generex-1.0.2.jar\n","spark-3.3.0-bin-hadoop3/jars/leveldbjni-all-1.8.jar\n","spark-3.3.0-bin-hadoop3/jars/jaxb-runtime-2.3.2.jar\n","spark-3.3.0-bin-hadoop3/jars/rocksdbjni-6.20.3.jar\n","spark-3.3.0-bin-hadoop3/jars/netty-transport-native-unix-common-4.1.74.Final.jar\n","spark-3.3.0-bin-hadoop3/jars/lapack-2.2.1.jar\n","spark-3.3.0-bin-hadoop3/jars/jackson-core-asl-1.9.13.jar\n","spark-3.3.0-bin-hadoop3/jars/spark-mllib-local_2.12-3.3.0.jar\n","spark-3.3.0-bin-hadoop3/jars/hive-cli-2.3.9.jar\n","spark-3.3.0-bin-hadoop3/jars/hive-beeline-2.3.9.jar\n","spark-3.3.0-bin-hadoop3/jars/scala-xml_2.12-1.2.0.jar\n","spark-3.3.0-bin-hadoop3/jars/hk2-locator-2.6.1.jar\n","spark-3.3.0-bin-hadoop3/jars/kubernetes-model-apps-5.12.2.jar\n","spark-3.3.0-bin-hadoop3/jars/metrics-jmx-4.2.7.jar\n","spark-3.3.0-bin-hadoop3/jars/jackson-mapper-asl-1.9.13.jar\n","spark-3.3.0-bin-hadoop3/jars/blas-2.2.1.jar\n","spark-3.3.0-bin-hadoop3/jars/ST4-4.0.4.jar\n","spark-3.3.0-bin-hadoop3/jars/jackson-dataformat-yaml-2.13.3.jar\n","spark-3.3.0-bin-hadoop3/jars/chill_2.12-0.10.0.jar\n","spark-3.3.0-bin-hadoop3/jars/json4s-jackson_2.12-3.7.0-M11.jar\n","spark-3.3.0-bin-hadoop3/jars/netty-resolver-4.1.74.Final.jar\n","spark-3.3.0-bin-hadoop3/jars/breeze-macros_2.12-1.2.jar\n","spark-3.3.0-bin-hadoop3/jars/xz-1.8.jar\n","spark-3.3.0-bin-hadoop3/jars/minlog-1.3.0.jar\n","spark-3.3.0-bin-hadoop3/jars/jersey-hk2-2.34.jar\n","spark-3.3.0-bin-hadoop3/jars/netty-common-4.1.74.Final.jar\n","spark-3.3.0-bin-hadoop3/jars/jakarta.ws.rs-api-2.1.6.jar\n","spark-3.3.0-bin-hadoop3/jars/spark-unsafe_2.12-3.3.0.jar\n","spark-3.3.0-bin-hadoop3/jars/jersey-container-servlet-core-2.34.jar\n","spark-3.3.0-bin-hadoop3/jars/aopalliance-repackaged-2.6.1.jar\n","spark-3.3.0-bin-hadoop3/jars/netty-transport-native-kqueue-4.1.74.Final-osx-aarch_64.jar\n","spark-3.3.0-bin-hadoop3/jars/bonecp-0.8.0.RELEASE.jar\n","spark-3.3.0-bin-hadoop3/jars/jodd-core-3.5.2.jar\n","spark-3.3.0-bin-hadoop3/jars/orc-mapreduce-1.7.4.jar\n","spark-3.3.0-bin-hadoop3/jars/spire-util_2.12-0.17.0.jar\n","spark-3.3.0-bin-hadoop3/jars/protobuf-java-2.5.0.jar\n","spark-3.3.0-bin-hadoop3/jars/jersey-client-2.34.jar\n","spark-3.3.0-bin-hadoop3/jars/datanucleus-core-4.1.17.jar\n","spark-3.3.0-bin-hadoop3/jars/janino-3.0.16.jar\n","spark-3.3.0-bin-hadoop3/jars/spire-platform_2.12-0.17.0.jar\n","spark-3.3.0-bin-hadoop3/jars/javolution-5.5.1.jar\n","spark-3.3.0-bin-hadoop3/jars/transaction-api-1.1.jar\n","spark-3.3.0-bin-hadoop3/jars/mesos-1.4.3-shaded-protobuf.jar\n","spark-3.3.0-bin-hadoop3/jars/curator-framework-2.13.0.jar\n","spark-3.3.0-bin-hadoop3/jars/parquet-encoding-1.12.2.jar\n","spark-3.3.0-bin-hadoop3/jars/jakarta.inject-2.6.1.jar\n","spark-3.3.0-bin-hadoop3/jars/hive-llap-common-2.3.9.jar\n","spark-3.3.0-bin-hadoop3/jars/commons-cli-1.5.0.jar\n","spark-3.3.0-bin-hadoop3/jars/jakarta.xml.bind-api-2.3.2.jar\n","spark-3.3.0-bin-hadoop3/jars/kubernetes-model-flowcontrol-5.12.2.jar\n","spark-3.3.0-bin-hadoop3/jars/algebra_2.12-2.0.1.jar\n","spark-3.3.0-bin-hadoop3/jars/hadoop-shaded-guava-1.1.1.jar\n","spark-3.3.0-bin-hadoop3/jars/arpack_combined_all-0.1.jar\n","spark-3.3.0-bin-hadoop3/jars/hive-vector-code-gen-2.3.9.jar\n","spark-3.3.0-bin-hadoop3/jars/kubernetes-model-admissionregistration-5.12.2.jar\n","spark-3.3.0-bin-hadoop3/jars/spark-mesos_2.12-3.3.0.jar\n","spark-3.3.0-bin-hadoop3/jars/json4s-ast_2.12-3.7.0-M11.jar\n","spark-3.3.0-bin-hadoop3/jars/log4j-slf4j-impl-2.17.2.jar\n","spark-3.3.0-bin-hadoop3/jars/javassist-3.25.0-GA.jar\n","spark-3.3.0-bin-hadoop3/jars/scala-collection-compat_2.12-2.1.1.jar\n","spark-3.3.0-bin-hadoop3/jars/kubernetes-model-networking-5.12.2.jar\n","spark-3.3.0-bin-hadoop3/jars/py4j-0.10.9.5.jar\n","spark-3.3.0-bin-hadoop3/jars/netty-codec-4.1.74.Final.jar\n","spark-3.3.0-bin-hadoop3/jars/hadoop-client-runtime-3.3.2.jar\n","spark-3.3.0-bin-hadoop3/jars/univocity-parsers-2.9.1.jar\n","spark-3.3.0-bin-hadoop3/jars/hive-shims-scheduler-2.3.9.jar\n","spark-3.3.0-bin-hadoop3/jars/kubernetes-model-extensions-5.12.2.jar\n","spark-3.3.0-bin-hadoop3/jars/breeze_2.12-1.2.jar\n","spark-3.3.0-bin-hadoop3/jars/chill-java-0.10.0.jar\n","spark-3.3.0-bin-hadoop3/jars/spire-macros_2.12-0.17.0.jar\n","spark-3.3.0-bin-hadoop3/jars/scala-parser-combinators_2.12-1.1.2.jar\n","spark-3.3.0-bin-hadoop3/jars/hive-metastore-2.3.9.jar\n","spark-3.3.0-bin-hadoop3/jars/kubernetes-model-scheduling-5.12.2.jar\n","spark-3.3.0-bin-hadoop3/jars/kubernetes-model-batch-5.12.2.jar\n","spark-3.3.0-bin-hadoop3/data/\n","spark-3.3.0-bin-hadoop3/data/mllib/\n","spark-3.3.0-bin-hadoop3/data/mllib/sample_lda_data.txt\n","spark-3.3.0-bin-hadoop3/data/mllib/sample_libsvm_data.txt\n","spark-3.3.0-bin-hadoop3/data/mllib/sample_svm_data.txt\n","spark-3.3.0-bin-hadoop3/data/mllib/sample_multiclass_classification_data.txt\n","spark-3.3.0-bin-hadoop3/data/mllib/sample_linear_regression_data.txt\n","spark-3.3.0-bin-hadoop3/data/mllib/sample_lda_libsvm_data.txt\n","spark-3.3.0-bin-hadoop3/data/mllib/sample_fpgrowth.txt\n","spark-3.3.0-bin-hadoop3/data/mllib/sample_binary_classification_data.txt\n","spark-3.3.0-bin-hadoop3/data/mllib/sample_isotonic_regression_libsvm_data.txt\n","spark-3.3.0-bin-hadoop3/data/mllib/sample_movielens_data.txt\n","spark-3.3.0-bin-hadoop3/data/mllib/als/\n","spark-3.3.0-bin-hadoop3/data/mllib/als/test.data\n","spark-3.3.0-bin-hadoop3/data/mllib/als/sample_movielens_ratings.txt\n","spark-3.3.0-bin-hadoop3/data/mllib/sample_kmeans_data.txt\n","spark-3.3.0-bin-hadoop3/data/mllib/pic_data.txt\n","spark-3.3.0-bin-hadoop3/data/mllib/images/\n","spark-3.3.0-bin-hadoop3/data/mllib/images/origin/\n","spark-3.3.0-bin-hadoop3/data/mllib/images/origin/kittens/\n","spark-3.3.0-bin-hadoop3/data/mllib/images/origin/kittens/54893.jpg\n","spark-3.3.0-bin-hadoop3/data/mllib/images/origin/kittens/not-image.txt\n","spark-3.3.0-bin-hadoop3/data/mllib/images/origin/kittens/DP802813.jpg\n","spark-3.3.0-bin-hadoop3/data/mllib/images/origin/kittens/29.5.a_b_EGDP022204.jpg\n","spark-3.3.0-bin-hadoop3/data/mllib/images/origin/kittens/DP153539.jpg\n","spark-3.3.0-bin-hadoop3/data/mllib/images/origin/license.txt\n","spark-3.3.0-bin-hadoop3/data/mllib/images/origin/multi-channel/\n","spark-3.3.0-bin-hadoop3/data/mllib/images/origin/multi-channel/BGRA.png\n","spark-3.3.0-bin-hadoop3/data/mllib/images/origin/multi-channel/BGRA_alpha_60.png\n","spark-3.3.0-bin-hadoop3/data/mllib/images/origin/multi-channel/grayscale.jpg\n","spark-3.3.0-bin-hadoop3/data/mllib/images/origin/multi-channel/chr30.4.184.jpg\n","spark-3.3.0-bin-hadoop3/data/mllib/images/license.txt\n","spark-3.3.0-bin-hadoop3/data/mllib/ridge-data/\n","spark-3.3.0-bin-hadoop3/data/mllib/ridge-data/lpsa.data\n","spark-3.3.0-bin-hadoop3/data/mllib/kmeans_data.txt\n","spark-3.3.0-bin-hadoop3/data/mllib/streaming_kmeans_data_test.txt\n","spark-3.3.0-bin-hadoop3/data/mllib/pagerank_data.txt\n","spark-3.3.0-bin-hadoop3/data/mllib/gmm_data.txt\n","spark-3.3.0-bin-hadoop3/data/graphx/\n","spark-3.3.0-bin-hadoop3/data/graphx/users.txt\n","spark-3.3.0-bin-hadoop3/data/graphx/followers.txt\n","spark-3.3.0-bin-hadoop3/data/streaming/\n","spark-3.3.0-bin-hadoop3/data/streaming/AFINN-111.txt\n","spark-3.3.0-bin-hadoop3/R/\n","spark-3.3.0-bin-hadoop3/R/lib/\n","spark-3.3.0-bin-hadoop3/R/lib/sparkr.zip\n","spark-3.3.0-bin-hadoop3/R/lib/SparkR/\n","spark-3.3.0-bin-hadoop3/R/lib/SparkR/tests/\n","spark-3.3.0-bin-hadoop3/R/lib/SparkR/tests/testthat/\n","spark-3.3.0-bin-hadoop3/R/lib/SparkR/tests/testthat/test_basic.R\n","spark-3.3.0-bin-hadoop3/R/lib/SparkR/DESCRIPTION\n","spark-3.3.0-bin-hadoop3/R/lib/SparkR/profile/\n","spark-3.3.0-bin-hadoop3/R/lib/SparkR/profile/shell.R\n","spark-3.3.0-bin-hadoop3/R/lib/SparkR/profile/general.R\n","spark-3.3.0-bin-hadoop3/R/lib/SparkR/INDEX\n","spark-3.3.0-bin-hadoop3/R/lib/SparkR/Meta/\n","spark-3.3.0-bin-hadoop3/R/lib/SparkR/Meta/features.rds\n","spark-3.3.0-bin-hadoop3/R/lib/SparkR/Meta/links.rds\n","spark-3.3.0-bin-hadoop3/R/lib/SparkR/Meta/nsInfo.rds\n","spark-3.3.0-bin-hadoop3/R/lib/SparkR/Meta/package.rds\n","spark-3.3.0-bin-hadoop3/R/lib/SparkR/Meta/vignette.rds\n","spark-3.3.0-bin-hadoop3/R/lib/SparkR/Meta/Rd.rds\n","spark-3.3.0-bin-hadoop3/R/lib/SparkR/Meta/hsearch.rds\n","spark-3.3.0-bin-hadoop3/R/lib/SparkR/help/\n","spark-3.3.0-bin-hadoop3/R/lib/SparkR/help/SparkR.rdb\n","spark-3.3.0-bin-hadoop3/R/lib/SparkR/help/aliases.rds\n","spark-3.3.0-bin-hadoop3/R/lib/SparkR/help/SparkR.rdx\n","spark-3.3.0-bin-hadoop3/R/lib/SparkR/help/AnIndex\n","spark-3.3.0-bin-hadoop3/R/lib/SparkR/help/paths.rds\n","spark-3.3.0-bin-hadoop3/R/lib/SparkR/R/\n","spark-3.3.0-bin-hadoop3/R/lib/SparkR/R/SparkR.rdb\n","spark-3.3.0-bin-hadoop3/R/lib/SparkR/R/SparkR.rdx\n","spark-3.3.0-bin-hadoop3/R/lib/SparkR/R/SparkR\n","spark-3.3.0-bin-hadoop3/R/lib/SparkR/NAMESPACE\n","spark-3.3.0-bin-hadoop3/R/lib/SparkR/doc/\n","spark-3.3.0-bin-hadoop3/R/lib/SparkR/doc/sparkr-vignettes.Rmd\n","spark-3.3.0-bin-hadoop3/R/lib/SparkR/doc/index.html\n","spark-3.3.0-bin-hadoop3/R/lib/SparkR/doc/sparkr-vignettes.html\n","spark-3.3.0-bin-hadoop3/R/lib/SparkR/doc/sparkr-vignettes.R\n","spark-3.3.0-bin-hadoop3/R/lib/SparkR/html/\n","spark-3.3.0-bin-hadoop3/R/lib/SparkR/html/00Index.html\n","spark-3.3.0-bin-hadoop3/R/lib/SparkR/html/R.css\n","spark-3.3.0-bin-hadoop3/R/lib/SparkR/worker/\n","spark-3.3.0-bin-hadoop3/R/lib/SparkR/worker/worker.R\n","spark-3.3.0-bin-hadoop3/R/lib/SparkR/worker/daemon.R\n","spark-3.3.0-bin-hadoop3/README.md\n","spark-3.3.0-bin-hadoop3/RELEASE\n","spark-3.3.0-bin-hadoop3/yarn/\n","spark-3.3.0-bin-hadoop3/yarn/spark-3.3.0-yarn-shuffle.jar\n","spark-3.3.0-bin-hadoop3/LICENSE\n","spark-3.3.0-bin-hadoop3/sbin/\n","spark-3.3.0-bin-hadoop3/sbin/start-workers.sh\n","spark-3.3.0-bin-hadoop3/sbin/stop-mesos-shuffle-service.sh\n","spark-3.3.0-bin-hadoop3/sbin/start-master.sh\n","spark-3.3.0-bin-hadoop3/sbin/workers.sh\n","spark-3.3.0-bin-hadoop3/sbin/start-worker.sh\n","spark-3.3.0-bin-hadoop3/sbin/spark-config.sh\n","spark-3.3.0-bin-hadoop3/sbin/start-history-server.sh\n","spark-3.3.0-bin-hadoop3/sbin/start-slaves.sh\n","spark-3.3.0-bin-hadoop3/sbin/spark-daemon.sh\n","spark-3.3.0-bin-hadoop3/sbin/stop-worker.sh\n","spark-3.3.0-bin-hadoop3/sbin/stop-mesos-dispatcher.sh\n","spark-3.3.0-bin-hadoop3/sbin/decommission-worker.sh\n","spark-3.3.0-bin-hadoop3/sbin/start-mesos-shuffle-service.sh\n","spark-3.3.0-bin-hadoop3/sbin/decommission-slave.sh\n","spark-3.3.0-bin-hadoop3/sbin/slaves.sh\n","spark-3.3.0-bin-hadoop3/sbin/stop-history-server.sh\n","spark-3.3.0-bin-hadoop3/sbin/start-thriftserver.sh\n","spark-3.3.0-bin-hadoop3/sbin/stop-thriftserver.sh\n","spark-3.3.0-bin-hadoop3/sbin/start-slave.sh\n","spark-3.3.0-bin-hadoop3/sbin/start-all.sh\n","spark-3.3.0-bin-hadoop3/sbin/stop-slave.sh\n","spark-3.3.0-bin-hadoop3/sbin/spark-daemons.sh\n","spark-3.3.0-bin-hadoop3/sbin/stop-workers.sh\n","spark-3.3.0-bin-hadoop3/sbin/stop-slaves.sh\n","spark-3.3.0-bin-hadoop3/sbin/stop-all.sh\n","spark-3.3.0-bin-hadoop3/sbin/start-mesos-dispatcher.sh\n","spark-3.3.0-bin-hadoop3/sbin/stop-master.sh\n","spark-3.3.0-bin-hadoop3/examples/\n","spark-3.3.0-bin-hadoop3/examples/src/\n","spark-3.3.0-bin-hadoop3/examples/src/main/\n","spark-3.3.0-bin-hadoop3/examples/src/main/r/\n","spark-3.3.0-bin-hadoop3/examples/src/main/r/ml/\n","spark-3.3.0-bin-hadoop3/examples/src/main/r/ml/survreg.R\n","spark-3.3.0-bin-hadoop3/examples/src/main/r/ml/glm.R\n","spark-3.3.0-bin-hadoop3/examples/src/main/r/ml/prefixSpan.R\n","spark-3.3.0-bin-hadoop3/examples/src/main/r/ml/lm_with_elastic_net.R\n","spark-3.3.0-bin-hadoop3/examples/src/main/r/ml/powerIterationClustering.R\n","spark-3.3.0-bin-hadoop3/examples/src/main/r/ml/lda.R\n","spark-3.3.0-bin-hadoop3/examples/src/main/r/ml/kstest.R\n","spark-3.3.0-bin-hadoop3/examples/src/main/r/ml/isoreg.R\n","spark-3.3.0-bin-hadoop3/examples/src/main/r/ml/ml.R\n","spark-3.3.0-bin-hadoop3/examples/src/main/r/ml/naiveBayes.R\n","spark-3.3.0-bin-hadoop3/examples/src/main/r/ml/fmRegressor.R\n","spark-3.3.0-bin-hadoop3/examples/src/main/r/ml/mlp.R\n","spark-3.3.0-bin-hadoop3/examples/src/main/r/ml/als.R\n","spark-3.3.0-bin-hadoop3/examples/src/main/r/ml/kmeans.R\n","spark-3.3.0-bin-hadoop3/examples/src/main/r/ml/svmLinear.R\n","spark-3.3.0-bin-hadoop3/examples/src/main/r/ml/fmClassifier.R\n","spark-3.3.0-bin-hadoop3/examples/src/main/r/ml/logit.R\n","spark-3.3.0-bin-hadoop3/examples/src/main/r/ml/randomForest.R\n","spark-3.3.0-bin-hadoop3/examples/src/main/r/ml/gbt.R\n","spark-3.3.0-bin-hadoop3/examples/src/main/r/ml/decisionTree.R\n","spark-3.3.0-bin-hadoop3/examples/src/main/r/ml/gaussianMixture.R\n","spark-3.3.0-bin-hadoop3/examples/src/main/r/ml/bisectingKmeans.R\n","spark-3.3.0-bin-hadoop3/examples/src/main/r/ml/fpm.R\n","spark-3.3.0-bin-hadoop3/examples/src/main/r/dataframe.R\n","spark-3.3.0-bin-hadoop3/examples/src/main/r/RSparkSQLExample.R\n","spark-3.3.0-bin-hadoop3/examples/src/main/r/data-manipulation.R\n","spark-3.3.0-bin-hadoop3/examples/src/main/r/streaming/\n","spark-3.3.0-bin-hadoop3/examples/src/main/r/streaming/structured_network_wordcount.R\n","spark-3.3.0-bin-hadoop3/examples/src/main/scala/\n","spark-3.3.0-bin-hadoop3/examples/src/main/scala/org/\n","spark-3.3.0-bin-hadoop3/examples/src/main/scala/org/apache/\n","spark-3.3.0-bin-hadoop3/examples/src/main/scala/org/apache/spark/\n","spark-3.3.0-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/\n","spark-3.3.0-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/SparkPageRank.scala\n","spark-3.3.0-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/SkewedGroupByTest.scala\n","spark-3.3.0-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/AccumulatorMetricsTest.scala\n","spark-3.3.0-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/HdfsTest.scala\n","spark-3.3.0-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/ml/\n","spark-3.3.0-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/ml/Word2VecExample.scala\n","spark-3.3.0-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/ml/DeveloperApiExample.scala\n","spark-3.3.0-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/ml/MinHashLSHExample.scala\n","spark-3.3.0-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/ml/IndexToStringExample.scala\n","spark-3.3.0-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/ml/BisectingKMeansExample.scala\n","spark-3.3.0-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/ml/CorrelationExample.scala\n","spark-3.3.0-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/ml/MulticlassLogisticRegressionWithElasticNetExample.scala\n","spark-3.3.0-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/ml/VectorSizeHintExample.scala\n","spark-3.3.0-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/ml/NGramExample.scala\n","spark-3.3.0-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/ml/BucketedRandomProjectionLSHExample.scala\n","spark-3.3.0-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/ml/BinarizerExample.scala\n","spark-3.3.0-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/ml/RandomForestExample.scala\n","spark-3.3.0-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/ml/StandardScalerExample.scala\n","spark-3.3.0-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/ml/LogisticRegressionExample.scala\n","spark-3.3.0-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/ml/FeatureHasherExample.scala\n","spark-3.3.0-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/ml/OneVsRestExample.scala\n","spark-3.3.0-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/ml/StopWordsRemoverExample.scala\n","spark-3.3.0-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/ml/LinearSVCExample.scala\n","spark-3.3.0-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/ml/GaussianMixtureExample.scala\n","spark-3.3.0-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/ml/MaxAbsScalerExample.scala\n","spark-3.3.0-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/ml/VarianceThresholdSelectorExample.scala\n","spark-3.3.0-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/ml/GBTExample.scala\n","spark-3.3.0-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/ml/IsotonicRegressionExample.scala\n","spark-3.3.0-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/ml/ModelSelectionViaCrossValidationExample.scala\n","spark-3.3.0-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/ml/EstimatorTransformerParamExample.scala\n","spark-3.3.0-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/ml/DecisionTreeRegressionExample.scala\n","spark-3.3.0-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/ml/RandomForestRegressorExample.scala\n","spark-3.3.0-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/ml/PrefixSpanExample.scala\n","spark-3.3.0-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/ml/KMeansExample.scala\n","spark-3.3.0-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/ml/ModelSelectionViaTrainValidationSplitExample.scala\n","spark-3.3.0-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/ml/LogisticRegressionSummaryExample.scala\n","spark-3.3.0-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/ml/MultilayerPerceptronClassifierExample.scala\n","spark-3.3.0-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/ml/PCAExample.scala\n","spark-3.3.0-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/ml/DCTExample.scala\n","spark-3.3.0-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/ml/FPGrowthExample.scala\n","spark-3.3.0-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/ml/ElementwiseProductExample.scala\n","spark-3.3.0-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/ml/GeneralizedLinearRegressionExample.scala\n","spark-3.3.0-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/ml/UnivariateFeatureSelectorExample.scala\n","spark-3.3.0-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/ml/CountVectorizerExample.scala\n","spark-3.3.0-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/ml/MinMaxScalerExample.scala\n","spark-3.3.0-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/ml/ChiSquareTestExample.scala\n","spark-3.3.0-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/ml/LogisticRegressionWithElasticNetExample.scala\n","spark-3.3.0-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/ml/FMRegressorExample.scala\n","spark-3.3.0-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/ml/VectorSlicerExample.scala\n","spark-3.3.0-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/ml/LinearRegressionExample.scala\n","spark-3.3.0-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/ml/StringIndexerExample.scala\n","spark-3.3.0-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/ml/GradientBoostedTreeRegressorExample.scala\n","spark-3.3.0-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/ml/ALSExample.scala\n","spark-3.3.0-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/ml/DecisionTreeClassificationExample.scala\n","spark-3.3.0-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/ml/PolynomialExpansionExample.scala\n","spark-3.3.0-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/ml/FMClassifierExample.scala\n","spark-3.3.0-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/ml/UnaryTransformerExample.scala\n","spark-3.3.0-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/ml/ImputerExample.scala\n","spark-3.3.0-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/ml/TokenizerExample.scala\n","spark-3.3.0-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/ml/DecisionTreeExample.scala\n","spark-3.3.0-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/ml/GradientBoostedTreeClassifierExample.scala\n","spark-3.3.0-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/ml/PowerIterationClusteringExample.scala\n","spark-3.3.0-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/ml/SQLTransformerExample.scala\n","spark-3.3.0-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/ml/SummarizerExample.scala\n","spark-3.3.0-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/ml/DataFrameExample.scala\n","spark-3.3.0-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/ml/BucketizerExample.scala\n","spark-3.3.0-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/ml/TfIdfExample.scala\n","spark-3.3.0-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/ml/AFTSurvivalRegressionExample.scala\n","spark-3.3.0-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/ml/QuantileDiscretizerExample.scala\n","spark-3.3.0-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/ml/OneHotEncoderExample.scala\n","spark-3.3.0-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/ml/PipelineExample.scala\n","spark-3.3.0-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/ml/InteractionExample.scala\n","spark-3.3.0-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/ml/LDAExample.scala\n","spark-3.3.0-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/ml/RFormulaExample.scala\n","spark-3.3.0-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/ml/VectorIndexerExample.scala\n","spark-3.3.0-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/ml/VectorAssemblerExample.scala\n","spark-3.3.0-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/ml/NormalizerExample.scala\n","spark-3.3.0-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/ml/ChiSqSelectorExample.scala\n","spark-3.3.0-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/ml/NaiveBayesExample.scala\n","spark-3.3.0-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/ml/RandomForestClassifierExample.scala\n","spark-3.3.0-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/ml/LinearRegressionWithElasticNetExample.scala\n","spark-3.3.0-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/ml/RobustScalerExample.scala\n","spark-3.3.0-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/LogQuery.scala\n","spark-3.3.0-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/SparkRemoteFileTest.scala\n","spark-3.3.0-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/ExceptionHandlingTest.scala\n","spark-3.3.0-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/pythonconverters/\n","spark-3.3.0-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/pythonconverters/AvroConverters.scala\n","spark-3.3.0-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/BroadcastTest.scala\n","spark-3.3.0-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/SparkHdfsLR.scala\n","spark-3.3.0-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/mllib/\n","spark-3.3.0-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/mllib/Word2VecExample.scala\n","spark-3.3.0-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/mllib/GradientBoostedTreesRunner.scala\n","spark-3.3.0-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/mllib/RandomRDDGeneration.scala\n","spark-3.3.0-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/mllib/PMMLModelExportExample.scala\n","spark-3.3.0-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/mllib/BisectingKMeansExample.scala\n","spark-3.3.0-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/mllib/DecisionTreeRunner.scala\n","spark-3.3.0-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/mllib/MultiLabelMetricsExample.scala\n","spark-3.3.0-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/mllib/CorrelationsExample.scala\n","spark-3.3.0-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/mllib/LBFGSExample.scala\n","spark-3.3.0-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/mllib/TFIDFExample.scala\n","spark-3.3.0-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/mllib/StandardScalerExample.scala\n","spark-3.3.0-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/mllib/LogisticRegressionWithLBFGSExample.scala\n","spark-3.3.0-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/mllib/StreamingTestExample.scala\n","spark-3.3.0-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/mllib/BinaryClassification.scala\n","spark-3.3.0-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/mllib/DenseKMeans.scala\n","spark-3.3.0-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/mllib/GradientBoostingClassificationExample.scala\n","spark-3.3.0-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/mllib/RankingMetricsExample.scala\n","spark-3.3.0-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/mllib/GaussianMixtureExample.scala\n","spark-3.3.0-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/mllib/MultivariateSummarizer.scala\n","spark-3.3.0-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/mllib/IsotonicRegressionExample.scala\n","spark-3.3.0-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/mllib/PCAOnSourceVectorExample.scala\n","spark-3.3.0-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/mllib/Correlations.scala\n","spark-3.3.0-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/mllib/HypothesisTestingExample.scala\n","spark-3.3.0-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/mllib/DecisionTreeRegressionExample.scala\n","spark-3.3.0-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/mllib/AssociationRulesExample.scala\n","spark-3.3.0-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/mllib/StreamingLogisticRegression.scala\n","spark-3.3.0-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/mllib/SVDExample.scala\n","spark-3.3.0-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/mllib/PrefixSpanExample.scala\n","spark-3.3.0-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/mllib/KMeansExample.scala\n","spark-3.3.0-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/mllib/SparseNaiveBayes.scala\n","spark-3.3.0-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/mllib/AbstractParams.scala\n","spark-3.3.0-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/mllib/BinaryClassificationMetricsExample.scala\n","spark-3.3.0-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/mllib/SampledRDDs.scala\n","spark-3.3.0-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/mllib/FPGrowthExample.scala\n","spark-3.3.0-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/mllib/ElementwiseProductExample.scala\n","spark-3.3.0-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/mllib/RecommendationExample.scala\n","spark-3.3.0-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/mllib/TallSkinnySVD.scala\n","spark-3.3.0-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/mllib/HypothesisTestingKolmogorovSmirnovTestExample.scala\n","spark-3.3.0-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/mllib/GradientBoostingRegressionExample.scala\n","spark-3.3.0-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/mllib/StreamingKMeansExample.scala\n","spark-3.3.0-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/mllib/CosineSimilarity.scala\n","spark-3.3.0-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/mllib/RandomForestRegressionExample.scala\n","spark-3.3.0-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/mllib/StreamingLinearRegressionExample.scala\n","spark-3.3.0-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/mllib/StratifiedSamplingExample.scala\n","spark-3.3.0-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/mllib/DecisionTreeClassificationExample.scala\n","spark-3.3.0-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/mllib/TallSkinnyPCA.scala\n","spark-3.3.0-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/mllib/RandomForestClassificationExample.scala\n","spark-3.3.0-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/mllib/SimpleFPGrowth.scala\n","spark-3.3.0-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/mllib/LatentDirichletAllocationExample.scala\n","spark-3.3.0-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/mllib/KernelDensityEstimationExample.scala\n","spark-3.3.0-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/mllib/PowerIterationClusteringExample.scala\n","spark-3.3.0-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/mllib/SVMWithSGDExample.scala\n","spark-3.3.0-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/mllib/LDAExample.scala\n","spark-3.3.0-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/mllib/MovieLensALS.scala\n","spark-3.3.0-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/mllib/NormalizerExample.scala\n","spark-3.3.0-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/mllib/MulticlassMetricsExample.scala\n","spark-3.3.0-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/mllib/ChiSqSelectorExample.scala\n","spark-3.3.0-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/mllib/SummaryStatisticsExample.scala\n","spark-3.3.0-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/mllib/PCAOnRowMatrixExample.scala\n","spark-3.3.0-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/mllib/NaiveBayesExample.scala\n","spark-3.3.0-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/SparkTC.scala\n","spark-3.3.0-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/DriverSubmissionTest.scala\n","spark-3.3.0-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/MultiBroadcastTest.scala\n","spark-3.3.0-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/SparkLR.scala\n","spark-3.3.0-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/SparkPi.scala\n","spark-3.3.0-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/graphx/\n","spark-3.3.0-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/graphx/ConnectedComponentsExample.scala\n","spark-3.3.0-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/graphx/LiveJournalPageRank.scala\n","spark-3.3.0-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/graphx/AggregateMessagesExample.scala\n","spark-3.3.0-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/graphx/SSSPExample.scala\n","spark-3.3.0-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/graphx/SynthBenchmark.scala\n","spark-3.3.0-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/graphx/TriangleCountingExample.scala\n","spark-3.3.0-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/graphx/PageRankExample.scala\n","spark-3.3.0-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/graphx/ComprehensiveExample.scala\n","spark-3.3.0-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/graphx/Analytics.scala\n","spark-3.3.0-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/SparkKMeans.scala\n","spark-3.3.0-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/streaming/\n","spark-3.3.0-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/streaming/clickstream/\n","spark-3.3.0-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/streaming/clickstream/PageViewGenerator.scala\n","spark-3.3.0-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/streaming/clickstream/PageViewStream.scala\n","spark-3.3.0-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/streaming/DirectKafkaWordCount.scala\n","spark-3.3.0-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/streaming/HdfsWordCount.scala\n","spark-3.3.0-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/streaming/DirectKerberizedKafkaWordCount.scala\n","spark-3.3.0-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/streaming/RecoverableNetworkWordCount.scala\n","spark-3.3.0-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/streaming/NetworkWordCount.scala\n","spark-3.3.0-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/streaming/QueueStream.scala\n","spark-3.3.0-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/streaming/SqlNetworkWordCount.scala\n","spark-3.3.0-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/streaming/CustomReceiver.scala\n","spark-3.3.0-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/streaming/RawNetworkGrep.scala\n","spark-3.3.0-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/streaming/StatefulNetworkWordCount.scala\n","spark-3.3.0-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/streaming/StreamingExamples.scala\n","spark-3.3.0-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/LocalLR.scala\n","spark-3.3.0-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/LocalKMeans.scala\n","spark-3.3.0-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/GroupByTest.scala\n","spark-3.3.0-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/LocalALS.scala\n","spark-3.3.0-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/sql/\n","spark-3.3.0-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/sql/SimpleTypedAggregator.scala\n","spark-3.3.0-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/sql/UserDefinedUntypedAggregation.scala\n","spark-3.3.0-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/sql/hive/\n","spark-3.3.0-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/sql/hive/SparkHiveExample.scala\n","spark-3.3.0-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/sql/RDDRelation.scala\n","spark-3.3.0-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/sql/SparkSQLExample.scala\n","spark-3.3.0-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/sql/SQLDataSourceExample.scala\n","spark-3.3.0-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/sql/streaming/\n","spark-3.3.0-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/sql/streaming/StructuredKerberizedKafkaWordCount.scala\n","spark-3.3.0-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/sql/streaming/StructuredComplexSessionization.scala\n","spark-3.3.0-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/sql/streaming/StructuredNetworkWordCount.scala\n","spark-3.3.0-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/sql/streaming/StructuredNetworkWordCountWindowed.scala\n","spark-3.3.0-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/sql/streaming/StructuredKafkaWordCount.scala\n","spark-3.3.0-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/sql/streaming/StructuredSessionization.scala\n","spark-3.3.0-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/sql/UserDefinedTypedAggregation.scala\n","spark-3.3.0-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/sql/UserDefinedScalar.scala\n","spark-3.3.0-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/sql/jdbc/\n","spark-3.3.0-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/sql/jdbc/ExampleJdbcConnectionProvider.scala\n","spark-3.3.0-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/SimpleSkewedGroupByTest.scala\n","spark-3.3.0-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/extensions/\n","spark-3.3.0-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/extensions/SparkSessionExtensionsTest.scala\n","spark-3.3.0-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/extensions/SessionExtensionsWithoutLoader.scala\n","spark-3.3.0-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/extensions/SessionExtensionsWithLoader.scala\n","spark-3.3.0-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/extensions/AgeExample.scala\n","spark-3.3.0-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/LocalFileLR.scala\n","spark-3.3.0-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/MiniReadWriteTest.scala\n","spark-3.3.0-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/LocalPi.scala\n","spark-3.3.0-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/DFSReadWriteTest.scala\n","spark-3.3.0-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/SparkALS.scala\n","spark-3.3.0-bin-hadoop3/examples/src/main/resources/\n","spark-3.3.0-bin-hadoop3/examples/src/main/resources/people.json\n","spark-3.3.0-bin-hadoop3/examples/src/main/resources/users.avro\n","spark-3.3.0-bin-hadoop3/examples/src/main/resources/people.csv\n","spark-3.3.0-bin-hadoop3/examples/src/main/resources/users.parquet\n","spark-3.3.0-bin-hadoop3/examples/src/main/resources/META-INF/\n","spark-3.3.0-bin-hadoop3/examples/src/main/resources/META-INF/services/\n","spark-3.3.0-bin-hadoop3/examples/src/main/resources/META-INF/services/org.apache.spark.sql.SparkSessionExtensionsProvider\n","spark-3.3.0-bin-hadoop3/examples/src/main/resources/META-INF/services/org.apache.spark.sql.jdbc.JdbcConnectionProvider\n","spark-3.3.0-bin-hadoop3/examples/src/main/resources/users.orc\n","spark-3.3.0-bin-hadoop3/examples/src/main/resources/dir1/\n","spark-3.3.0-bin-hadoop3/examples/src/main/resources/dir1/file1.parquet\n","spark-3.3.0-bin-hadoop3/examples/src/main/resources/dir1/dir2/\n","spark-3.3.0-bin-hadoop3/examples/src/main/resources/dir1/dir2/file2.parquet\n","spark-3.3.0-bin-hadoop3/examples/src/main/resources/dir1/file3.json\n","spark-3.3.0-bin-hadoop3/examples/src/main/resources/user.avsc\n","spark-3.3.0-bin-hadoop3/examples/src/main/resources/full_user.avsc\n","spark-3.3.0-bin-hadoop3/examples/src/main/resources/kv1.txt\n","spark-3.3.0-bin-hadoop3/examples/src/main/resources/people.txt\n","spark-3.3.0-bin-hadoop3/examples/src/main/resources/employees.json\n","spark-3.3.0-bin-hadoop3/examples/src/main/java/\n","spark-3.3.0-bin-hadoop3/examples/src/main/java/org/\n","spark-3.3.0-bin-hadoop3/examples/src/main/java/org/apache/\n","spark-3.3.0-bin-hadoop3/examples/src/main/java/org/apache/spark/\n","spark-3.3.0-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/\n","spark-3.3.0-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/ml/\n","spark-3.3.0-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/ml/JavaStringIndexerExample.java\n","spark-3.3.0-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/ml/JavaChiSqSelectorExample.java\n","spark-3.3.0-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/ml/JavaLabeledDocument.java\n","spark-3.3.0-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/ml/JavaLogisticRegressionWithElasticNetExample.java\n","spark-3.3.0-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/ml/JavaPolynomialExpansionExample.java\n","spark-3.3.0-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/ml/JavaTfIdfExample.java\n","spark-3.3.0-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/ml/JavaBucketizerExample.java\n","spark-3.3.0-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/ml/JavaLogisticRegressionSummaryExample.java\n","spark-3.3.0-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/ml/JavaSummarizerExample.java\n","spark-3.3.0-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/ml/JavaQuantileDiscretizerExample.java\n","spark-3.3.0-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/ml/JavaBinarizerExample.java\n","spark-3.3.0-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/ml/JavaOneHotEncoderExample.java\n","spark-3.3.0-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/ml/JavaStandardScalerExample.java\n","spark-3.3.0-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/ml/JavaDCTExample.java\n","spark-3.3.0-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/ml/JavaDecisionTreeRegressionExample.java\n","spark-3.3.0-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/ml/JavaRandomForestClassifierExample.java\n","spark-3.3.0-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/ml/JavaOneVsRestExample.java\n","spark-3.3.0-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/ml/JavaGeneralizedLinearRegressionExample.java\n","spark-3.3.0-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/ml/JavaGaussianMixtureExample.java\n","spark-3.3.0-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/ml/JavaWord2VecExample.java\n","spark-3.3.0-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/ml/JavaNormalizerExample.java\n","spark-3.3.0-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/ml/JavaLDAExample.java\n","spark-3.3.0-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/ml/JavaCountVectorizerExample.java\n","spark-3.3.0-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/ml/JavaMultilayerPerceptronClassifierExample.java\n","spark-3.3.0-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/ml/JavaMinMaxScalerExample.java\n","spark-3.3.0-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/ml/JavaCorrelationExample.java\n","spark-3.3.0-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/ml/JavaNGramExample.java\n","spark-3.3.0-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/ml/JavaImputerExample.java\n","spark-3.3.0-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/ml/JavaMinHashLSHExample.java\n","spark-3.3.0-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/ml/JavaVectorSizeHintExample.java\n","spark-3.3.0-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/ml/JavaFMClassifierExample.java\n","spark-3.3.0-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/ml/JavaFeatureHasherExample.java\n","spark-3.3.0-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/ml/JavaModelSelectionViaTrainValidationSplitExample.java\n","spark-3.3.0-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/ml/JavaSQLTransformerExample.java\n","spark-3.3.0-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/ml/JavaKMeansExample.java\n","spark-3.3.0-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/ml/JavaVectorAssemblerExample.java\n","spark-3.3.0-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/ml/JavaGradientBoostedTreeRegressorExample.java\n","spark-3.3.0-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/ml/JavaPrefixSpanExample.java\n","spark-3.3.0-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/ml/JavaFMRegressorExample.java\n","spark-3.3.0-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/ml/JavaNaiveBayesExample.java\n","spark-3.3.0-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/ml/JavaMulticlassLogisticRegressionWithElasticNetExample.java\n","spark-3.3.0-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/ml/JavaModelSelectionViaCrossValidationExample.java\n","spark-3.3.0-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/ml/JavaDocument.java\n","spark-3.3.0-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/ml/JavaUnivariateFeatureSelectorExample.java\n","spark-3.3.0-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/ml/JavaGradientBoostedTreeClassifierExample.java\n","spark-3.3.0-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/ml/JavaRandomForestRegressorExample.java\n","spark-3.3.0-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/ml/JavaAFTSurvivalRegressionExample.java\n","spark-3.3.0-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/ml/JavaIsotonicRegressionExample.java\n","spark-3.3.0-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/ml/JavaElementwiseProductExample.java\n","spark-3.3.0-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/ml/JavaALSExample.java\n","spark-3.3.0-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/ml/JavaBucketedRandomProjectionLSHExample.java\n","spark-3.3.0-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/ml/JavaVarianceThresholdSelectorExample.java\n","spark-3.3.0-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/ml/JavaVectorSlicerExample.java\n","spark-3.3.0-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/ml/JavaTokenizerExample.java\n","spark-3.3.0-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/ml/JavaRFormulaExample.java\n","spark-3.3.0-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/ml/JavaEstimatorTransformerParamExample.java\n","spark-3.3.0-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/ml/JavaPowerIterationClusteringExample.java\n","spark-3.3.0-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/ml/JavaBisectingKMeansExample.java\n","spark-3.3.0-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/ml/JavaMaxAbsScalerExample.java\n","spark-3.3.0-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/ml/JavaDecisionTreeClassificationExample.java\n","spark-3.3.0-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/ml/JavaPCAExample.java\n","spark-3.3.0-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/ml/JavaPipelineExample.java\n","spark-3.3.0-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/ml/JavaStopWordsRemoverExample.java\n","spark-3.3.0-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/ml/JavaIndexToStringExample.java\n","spark-3.3.0-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/ml/JavaFPGrowthExample.java\n","spark-3.3.0-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/ml/JavaVectorIndexerExample.java\n","spark-3.3.0-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/ml/JavaInteractionExample.java\n","spark-3.3.0-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/ml/JavaChiSquareTestExample.java\n","spark-3.3.0-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/ml/JavaRobustScalerExample.java\n","spark-3.3.0-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/ml/JavaLinearRegressionWithElasticNetExample.java\n","spark-3.3.0-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/ml/JavaLinearSVCExample.java\n","spark-3.3.0-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/JavaWordCount.java\n","spark-3.3.0-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/JavaHdfsLR.java\n","spark-3.3.0-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/JavaStatusTrackerDemo.java\n","spark-3.3.0-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/mllib/\n","spark-3.3.0-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/mllib/JavaChiSqSelectorExample.java\n","spark-3.3.0-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/mllib/JavaHypothesisTestingKolmogorovSmirnovTestExample.java\n","spark-3.3.0-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/mllib/JavaRandomForestRegressionExample.java\n","spark-3.3.0-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/mllib/JavaSimpleFPGrowth.java\n","spark-3.3.0-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/mllib/JavaRandomForestClassificationExample.java\n","spark-3.3.0-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/mllib/JavaMulticlassClassificationMetricsExample.java\n","spark-3.3.0-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/mllib/JavaKernelDensityEstimationExample.java\n","spark-3.3.0-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/mllib/JavaSummaryStatisticsExample.java\n","spark-3.3.0-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/mllib/JavaDecisionTreeRegressionExample.java\n","spark-3.3.0-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/mllib/JavaGradientBoostingClassificationExample.java\n","spark-3.3.0-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/mllib/JavaGaussianMixtureExample.java\n","spark-3.3.0-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/mllib/JavaStratifiedSamplingExample.java\n","spark-3.3.0-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/mllib/JavaSVDExample.java\n","spark-3.3.0-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/mllib/JavaAssociationRulesExample.java\n","spark-3.3.0-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/mllib/JavaALS.java\n","spark-3.3.0-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/mllib/JavaKMeansExample.java\n","spark-3.3.0-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/mllib/JavaSVMWithSGDExample.java\n","spark-3.3.0-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/mllib/JavaPrefixSpanExample.java\n","spark-3.3.0-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/mllib/JavaNaiveBayesExample.java\n","spark-3.3.0-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/mllib/JavaLogisticRegressionWithLBFGSExample.java\n","spark-3.3.0-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/mllib/JavaLBFGSExample.java\n","spark-3.3.0-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/mllib/JavaGradientBoostingRegressionExample.java\n","spark-3.3.0-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/mllib/JavaRankingMetricsExample.java\n","spark-3.3.0-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/mllib/JavaIsotonicRegressionExample.java\n","spark-3.3.0-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/mllib/JavaElementwiseProductExample.java\n","spark-3.3.0-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/mllib/JavaCorrelationsExample.java\n","spark-3.3.0-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/mllib/JavaPowerIterationClusteringExample.java\n","spark-3.3.0-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/mllib/JavaBisectingKMeansExample.java\n","spark-3.3.0-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/mllib/JavaDecisionTreeClassificationExample.java\n","spark-3.3.0-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/mllib/JavaPCAExample.java\n","spark-3.3.0-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/mllib/JavaRecommendationExample.java\n","spark-3.3.0-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/mllib/JavaLatentDirichletAllocationExample.java\n","spark-3.3.0-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/mllib/JavaHypothesisTestingExample.java\n","spark-3.3.0-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/mllib/JavaMultiLabelClassificationMetricsExample.java\n","spark-3.3.0-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/mllib/JavaBinaryClassificationMetricsExample.java\n","spark-3.3.0-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/mllib/JavaStreamingTestExample.java\n","spark-3.3.0-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/JavaPageRank.java\n","spark-3.3.0-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/JavaLogQuery.java\n","spark-3.3.0-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/streaming/\n","spark-3.3.0-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/streaming/JavaStatefulNetworkWordCount.java\n","spark-3.3.0-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/streaming/JavaCustomReceiver.java\n","spark-3.3.0-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/streaming/JavaRecord.java\n","spark-3.3.0-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/streaming/JavaNetworkWordCount.java\n","spark-3.3.0-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/streaming/JavaDirectKerberizedKafkaWordCount.java\n","spark-3.3.0-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/streaming/JavaDirectKafkaWordCount.java\n","spark-3.3.0-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/streaming/JavaSqlNetworkWordCount.java\n","spark-3.3.0-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/streaming/JavaRecoverableNetworkWordCount.java\n","spark-3.3.0-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/streaming/JavaQueueStream.java\n","spark-3.3.0-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/JavaSparkPi.java\n","spark-3.3.0-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/sql/\n","spark-3.3.0-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/sql/JavaUserDefinedScalar.java\n","spark-3.3.0-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/sql/hive/\n","spark-3.3.0-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/sql/hive/JavaSparkHiveExample.java\n","spark-3.3.0-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/sql/JavaUserDefinedTypedAggregation.java\n","spark-3.3.0-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/sql/JavaSQLDataSourceExample.java\n","spark-3.3.0-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/sql/JavaSparkSQLExample.java\n","spark-3.3.0-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/sql/JavaUserDefinedUntypedAggregation.java\n","spark-3.3.0-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/sql/streaming/\n","spark-3.3.0-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/sql/streaming/JavaStructuredKafkaWordCount.java\n","spark-3.3.0-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/sql/streaming/JavaStructuredComplexSessionization.java\n","spark-3.3.0-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/sql/streaming/JavaStructuredNetworkWordCountWindowed.java\n","spark-3.3.0-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/sql/streaming/JavaStructuredSessionization.java\n","spark-3.3.0-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/sql/streaming/JavaStructuredNetworkWordCount.java\n","spark-3.3.0-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/sql/streaming/JavaStructuredKerberizedKafkaWordCount.java\n","spark-3.3.0-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/JavaTC.java\n","spark-3.3.0-bin-hadoop3/examples/src/main/scripts/\n","spark-3.3.0-bin-hadoop3/examples/src/main/scripts/getGpusResources.sh\n","spark-3.3.0-bin-hadoop3/examples/src/main/python/\n","spark-3.3.0-bin-hadoop3/examples/src/main/python/kmeans.py\n","spark-3.3.0-bin-hadoop3/examples/src/main/python/ml/\n","spark-3.3.0-bin-hadoop3/examples/src/main/python/ml/dct_example.py\n","spark-3.3.0-bin-hadoop3/examples/src/main/python/ml/count_vectorizer_example.py\n","spark-3.3.0-bin-hadoop3/examples/src/main/python/ml/chisq_selector_example.py\n","spark-3.3.0-bin-hadoop3/examples/src/main/python/ml/gradient_boosted_tree_regressor_example.py\n","spark-3.3.0-bin-hadoop3/examples/src/main/python/ml/tf_idf_example.py\n","spark-3.3.0-bin-hadoop3/examples/src/main/python/ml/cross_validator.py\n","spark-3.3.0-bin-hadoop3/examples/src/main/python/ml/linear_regression_with_elastic_net.py\n","spark-3.3.0-bin-hadoop3/examples/src/main/python/ml/normalizer_example.py\n","spark-3.3.0-bin-hadoop3/examples/src/main/python/ml/fm_regressor_example.py\n","spark-3.3.0-bin-hadoop3/examples/src/main/python/ml/univariate_feature_selector_example.py\n","spark-3.3.0-bin-hadoop3/examples/src/main/python/ml/polynomial_expansion_example.py\n","spark-3.3.0-bin-hadoop3/examples/src/main/python/ml/pipeline_example.py\n","spark-3.3.0-bin-hadoop3/examples/src/main/python/ml/generalized_linear_regression_example.py\n","spark-3.3.0-bin-hadoop3/examples/src/main/python/ml/multilayer_perceptron_classification.py\n","spark-3.3.0-bin-hadoop3/examples/src/main/python/ml/bisecting_k_means_example.py\n","spark-3.3.0-bin-hadoop3/examples/src/main/python/ml/chi_square_test_example.py\n","spark-3.3.0-bin-hadoop3/examples/src/main/python/ml/train_validation_split.py\n","spark-3.3.0-bin-hadoop3/examples/src/main/python/ml/stopwords_remover_example.py\n","spark-3.3.0-bin-hadoop3/examples/src/main/python/ml/linearsvc.py\n","spark-3.3.0-bin-hadoop3/examples/src/main/python/ml/lda_example.py\n","spark-3.3.0-bin-hadoop3/examples/src/main/python/ml/random_forest_regressor_example.py\n","spark-3.3.0-bin-hadoop3/examples/src/main/python/ml/vector_assembler_example.py\n","spark-3.3.0-bin-hadoop3/examples/src/main/python/ml/word2vec_example.py\n","spark-3.3.0-bin-hadoop3/examples/src/main/python/ml/string_indexer_example.py\n","spark-3.3.0-bin-hadoop3/examples/src/main/python/ml/decision_tree_classification_example.py\n","spark-3.3.0-bin-hadoop3/examples/src/main/python/ml/index_to_string_example.py\n","spark-3.3.0-bin-hadoop3/examples/src/main/python/ml/bucketizer_example.py\n","spark-3.3.0-bin-hadoop3/examples/src/main/python/ml/vector_size_hint_example.py\n","spark-3.3.0-bin-hadoop3/examples/src/main/python/ml/vector_indexer_example.py\n","spark-3.3.0-bin-hadoop3/examples/src/main/python/ml/tokenizer_example.py\n","spark-3.3.0-bin-hadoop3/examples/src/main/python/ml/robust_scaler_example.py\n","spark-3.3.0-bin-hadoop3/examples/src/main/python/ml/standard_scaler_example.py\n","spark-3.3.0-bin-hadoop3/examples/src/main/python/ml/kmeans_example.py\n","spark-3.3.0-bin-hadoop3/examples/src/main/python/ml/power_iteration_clustering_example.py\n","spark-3.3.0-bin-hadoop3/examples/src/main/python/ml/naive_bayes_example.py\n","spark-3.3.0-bin-hadoop3/examples/src/main/python/ml/pca_example.py\n","spark-3.3.0-bin-hadoop3/examples/src/main/python/ml/aft_survival_regression.py\n","spark-3.3.0-bin-hadoop3/examples/src/main/python/ml/min_max_scaler_example.py\n","spark-3.3.0-bin-hadoop3/examples/src/main/python/ml/elementwise_product_example.py\n","spark-3.3.0-bin-hadoop3/examples/src/main/python/ml/dataframe_example.py\n","spark-3.3.0-bin-hadoop3/examples/src/main/python/ml/n_gram_example.py\n","spark-3.3.0-bin-hadoop3/examples/src/main/python/ml/rformula_example.py\n","spark-3.3.0-bin-hadoop3/examples/src/main/python/ml/one_vs_rest_example.py\n","spark-3.3.0-bin-hadoop3/examples/src/main/python/ml/als_example.py\n","spark-3.3.0-bin-hadoop3/examples/src/main/python/ml/onehot_encoder_example.py\n","spark-3.3.0-bin-hadoop3/examples/src/main/python/ml/vector_slicer_example.py\n","spark-3.3.0-bin-hadoop3/examples/src/main/python/ml/logistic_regression_with_elastic_net.py\n","spark-3.3.0-bin-hadoop3/examples/src/main/python/ml/sql_transformer.py\n","spark-3.3.0-bin-hadoop3/examples/src/main/python/ml/multiclass_logistic_regression_with_elastic_net.py\n","spark-3.3.0-bin-hadoop3/examples/src/main/python/ml/summarizer_example.py\n","spark-3.3.0-bin-hadoop3/examples/src/main/python/ml/quantile_discretizer_example.py\n","spark-3.3.0-bin-hadoop3/examples/src/main/python/ml/feature_hasher_example.py\n","spark-3.3.0-bin-hadoop3/examples/src/main/python/ml/imputer_example.py\n","spark-3.3.0-bin-hadoop3/examples/src/main/python/ml/bucketed_random_projection_lsh_example.py\n","spark-3.3.0-bin-hadoop3/examples/src/main/python/ml/estimator_transformer_param_example.py\n","spark-3.3.0-bin-hadoop3/examples/src/main/python/ml/gradient_boosted_tree_classifier_example.py\n","spark-3.3.0-bin-hadoop3/examples/src/main/python/ml/logistic_regression_summary_example.py\n","spark-3.3.0-bin-hadoop3/examples/src/main/python/ml/decision_tree_regression_example.py\n","spark-3.3.0-bin-hadoop3/examples/src/main/python/ml/__init__,py\n","spark-3.3.0-bin-hadoop3/examples/src/main/python/ml/fm_classifier_example.py\n","spark-3.3.0-bin-hadoop3/examples/src/main/python/ml/gaussian_mixture_example.py\n","spark-3.3.0-bin-hadoop3/examples/src/main/python/ml/fpgrowth_example.py\n","spark-3.3.0-bin-hadoop3/examples/src/main/python/ml/variance_threshold_selector_example.py\n","spark-3.3.0-bin-hadoop3/examples/src/main/python/ml/random_forest_classifier_example.py\n","spark-3.3.0-bin-hadoop3/examples/src/main/python/ml/correlation_example.py\n","spark-3.3.0-bin-hadoop3/examples/src/main/python/ml/min_hash_lsh_example.py\n","spark-3.3.0-bin-hadoop3/examples/src/main/python/ml/binarizer_example.py\n","spark-3.3.0-bin-hadoop3/examples/src/main/python/ml/prefixspan_example.py\n","spark-3.3.0-bin-hadoop3/examples/src/main/python/ml/max_abs_scaler_example.py\n","spark-3.3.0-bin-hadoop3/examples/src/main/python/ml/isotonic_regression_example.py\n","spark-3.3.0-bin-hadoop3/examples/src/main/python/ml/interaction_example.py\n","spark-3.3.0-bin-hadoop3/examples/src/main/python/logistic_regression.py\n","spark-3.3.0-bin-hadoop3/examples/src/main/python/__init__.py\n","spark-3.3.0-bin-hadoop3/examples/src/main/python/als.py\n","spark-3.3.0-bin-hadoop3/examples/src/main/python/mllib/\n","spark-3.3.0-bin-hadoop3/examples/src/main/python/mllib/word2vec.py\n","spark-3.3.0-bin-hadoop3/examples/src/main/python/mllib/correlations_example.py\n","spark-3.3.0-bin-hadoop3/examples/src/main/python/mllib/gradient_boosting_classification_example.py\n","spark-3.3.0-bin-hadoop3/examples/src/main/python/mllib/kernel_density_estimation_example.py\n","spark-3.3.0-bin-hadoop3/examples/src/main/python/mllib/hypothesis_testing_example.py\n","spark-3.3.0-bin-hadoop3/examples/src/main/python/mllib/svd_example.py\n","spark-3.3.0-bin-hadoop3/examples/src/main/python/mllib/kmeans.py\n","spark-3.3.0-bin-hadoop3/examples/src/main/python/mllib/latent_dirichlet_allocation_example.py\n","spark-3.3.0-bin-hadoop3/examples/src/main/python/mllib/tf_idf_example.py\n","spark-3.3.0-bin-hadoop3/examples/src/main/python/mllib/correlations.py\n","spark-3.3.0-bin-hadoop3/examples/src/main/python/mllib/random_forest_regression_example.py\n","spark-3.3.0-bin-hadoop3/examples/src/main/python/mllib/normalizer_example.py\n","spark-3.3.0-bin-hadoop3/examples/src/main/python/mllib/logistic_regression.py\n","spark-3.3.0-bin-hadoop3/examples/src/main/python/mllib/multi_label_metrics_example.py\n","spark-3.3.0-bin-hadoop3/examples/src/main/python/mllib/gradient_boosting_regression_example.py\n","spark-3.3.0-bin-hadoop3/examples/src/main/python/mllib/random_forest_classification_example.py\n","spark-3.3.0-bin-hadoop3/examples/src/main/python/mllib/__init__.py\n","spark-3.3.0-bin-hadoop3/examples/src/main/python/mllib/streaming_linear_regression_example.py\n","spark-3.3.0-bin-hadoop3/examples/src/main/python/mllib/sampled_rdds.py\n","spark-3.3.0-bin-hadoop3/examples/src/main/python/mllib/bisecting_k_means_example.py\n","spark-3.3.0-bin-hadoop3/examples/src/main/python/mllib/logistic_regression_with_lbfgs_example.py\n","spark-3.3.0-bin-hadoop3/examples/src/main/python/mllib/k_means_example.py\n","spark-3.3.0-bin-hadoop3/examples/src/main/python/mllib/streaming_k_means_example.py\n","spark-3.3.0-bin-hadoop3/examples/src/main/python/mllib/word2vec_example.py\n","spark-3.3.0-bin-hadoop3/examples/src/main/python/mllib/multi_class_metrics_example.py\n","spark-3.3.0-bin-hadoop3/examples/src/main/python/mllib/decision_tree_classification_example.py\n","spark-3.3.0-bin-hadoop3/examples/src/main/python/mllib/summary_statistics_example.py\n","spark-3.3.0-bin-hadoop3/examples/src/main/python/mllib/binary_classification_metrics_example.py\n","spark-3.3.0-bin-hadoop3/examples/src/main/python/mllib/standard_scaler_example.py\n","spark-3.3.0-bin-hadoop3/examples/src/main/python/mllib/power_iteration_clustering_example.py\n","spark-3.3.0-bin-hadoop3/examples/src/main/python/mllib/gaussian_mixture_model.py\n","spark-3.3.0-bin-hadoop3/examples/src/main/python/mllib/regression_metrics_example.py\n","spark-3.3.0-bin-hadoop3/examples/src/main/python/mllib/naive_bayes_example.py\n","spark-3.3.0-bin-hadoop3/examples/src/main/python/mllib/elementwise_product_example.py\n","spark-3.3.0-bin-hadoop3/examples/src/main/python/mllib/linear_regression_with_sgd_example.py\n","spark-3.3.0-bin-hadoop3/examples/src/main/python/mllib/pca_rowmatrix_example.py\n","spark-3.3.0-bin-hadoop3/examples/src/main/python/mllib/hypothesis_testing_kolmogorov_smirnov_test_example.py\n","spark-3.3.0-bin-hadoop3/examples/src/main/python/mllib/random_rdd_generation.py\n","spark-3.3.0-bin-hadoop3/examples/src/main/python/mllib/decision_tree_regression_example.py\n","spark-3.3.0-bin-hadoop3/examples/src/main/python/mllib/gaussian_mixture_example.py\n","spark-3.3.0-bin-hadoop3/examples/src/main/python/mllib/recommendation_example.py\n","spark-3.3.0-bin-hadoop3/examples/src/main/python/mllib/fpgrowth_example.py\n","spark-3.3.0-bin-hadoop3/examples/src/main/python/mllib/ranking_metrics_example.py\n","spark-3.3.0-bin-hadoop3/examples/src/main/python/mllib/svm_with_sgd_example.py\n","spark-3.3.0-bin-hadoop3/examples/src/main/python/mllib/isotonic_regression_example.py\n","spark-3.3.0-bin-hadoop3/examples/src/main/python/mllib/stratified_sampling_example.py\n","spark-3.3.0-bin-hadoop3/examples/src/main/python/wordcount.py\n","spark-3.3.0-bin-hadoop3/examples/src/main/python/status_api_demo.py\n","spark-3.3.0-bin-hadoop3/examples/src/main/python/pagerank.py\n","spark-3.3.0-bin-hadoop3/examples/src/main/python/sort.py\n","spark-3.3.0-bin-hadoop3/examples/src/main/python/transitive_closure.py\n","spark-3.3.0-bin-hadoop3/examples/src/main/python/avro_inputformat.py\n","spark-3.3.0-bin-hadoop3/examples/src/main/python/pi.py\n","spark-3.3.0-bin-hadoop3/examples/src/main/python/streaming/\n","spark-3.3.0-bin-hadoop3/examples/src/main/python/streaming/queue_stream.py\n","spark-3.3.0-bin-hadoop3/examples/src/main/python/streaming/__init__.py\n","spark-3.3.0-bin-hadoop3/examples/src/main/python/streaming/stateful_network_wordcount.py\n","spark-3.3.0-bin-hadoop3/examples/src/main/python/streaming/network_wordjoinsentiments.py\n","spark-3.3.0-bin-hadoop3/examples/src/main/python/streaming/sql_network_wordcount.py\n","spark-3.3.0-bin-hadoop3/examples/src/main/python/streaming/network_wordcount.py\n","spark-3.3.0-bin-hadoop3/examples/src/main/python/streaming/hdfs_wordcount.py\n","spark-3.3.0-bin-hadoop3/examples/src/main/python/streaming/recoverable_network_wordcount.py\n","spark-3.3.0-bin-hadoop3/examples/src/main/python/sql/\n","spark-3.3.0-bin-hadoop3/examples/src/main/python/sql/datasource.py\n","spark-3.3.0-bin-hadoop3/examples/src/main/python/sql/__init__.py\n","spark-3.3.0-bin-hadoop3/examples/src/main/python/sql/hive.py\n","spark-3.3.0-bin-hadoop3/examples/src/main/python/sql/arrow.py\n","spark-3.3.0-bin-hadoop3/examples/src/main/python/sql/streaming/\n","spark-3.3.0-bin-hadoop3/examples/src/main/python/sql/streaming/structured_network_wordcount_windowed.py\n","spark-3.3.0-bin-hadoop3/examples/src/main/python/sql/streaming/structured_network_wordcount.py\n","spark-3.3.0-bin-hadoop3/examples/src/main/python/sql/streaming/structured_sessionization.py\n","spark-3.3.0-bin-hadoop3/examples/src/main/python/sql/streaming/__init__,py\n","spark-3.3.0-bin-hadoop3/examples/src/main/python/sql/streaming/structured_kafka_wordcount.py\n","spark-3.3.0-bin-hadoop3/examples/src/main/python/sql/basic.py\n","spark-3.3.0-bin-hadoop3/examples/src/main/python/parquet_inputformat.py\n","spark-3.3.0-bin-hadoop3/examples/jars/\n","spark-3.3.0-bin-hadoop3/examples/jars/spark-examples_2.12-3.3.0.jar\n","spark-3.3.0-bin-hadoop3/examples/jars/scopt_2.12-3.7.1.jar\n","spark-3.3.0-bin-hadoop3/conf/\n","spark-3.3.0-bin-hadoop3/conf/metrics.properties.template\n","spark-3.3.0-bin-hadoop3/conf/workers.template\n","spark-3.3.0-bin-hadoop3/conf/fairscheduler.xml.template\n","spark-3.3.0-bin-hadoop3/conf/spark-defaults.conf.template\n","spark-3.3.0-bin-hadoop3/conf/spark-env.sh.template\n","spark-3.3.0-bin-hadoop3/conf/log4j2.properties.template\n","spark-3.3.0-bin-hadoop3/bin/\n","spark-3.3.0-bin-hadoop3/bin/sparkR.cmd\n","spark-3.3.0-bin-hadoop3/bin/sparkR\n","spark-3.3.0-bin-hadoop3/bin/spark-submit\n","spark-3.3.0-bin-hadoop3/bin/pyspark2.cmd\n","spark-3.3.0-bin-hadoop3/bin/spark-class\n","spark-3.3.0-bin-hadoop3/bin/pyspark.cmd\n","spark-3.3.0-bin-hadoop3/bin/spark-submit2.cmd\n","spark-3.3.0-bin-hadoop3/bin/load-spark-env.cmd\n","spark-3.3.0-bin-hadoop3/bin/spark-sql\n","spark-3.3.0-bin-hadoop3/bin/docker-image-tool.sh\n","spark-3.3.0-bin-hadoop3/bin/find-spark-home.cmd\n","spark-3.3.0-bin-hadoop3/bin/load-spark-env.sh\n","spark-3.3.0-bin-hadoop3/bin/pyspark\n","spark-3.3.0-bin-hadoop3/bin/spark-shell.cmd\n","spark-3.3.0-bin-hadoop3/bin/spark-shell2.cmd\n","spark-3.3.0-bin-hadoop3/bin/spark-submit.cmd\n","spark-3.3.0-bin-hadoop3/bin/beeline.cmd\n","spark-3.3.0-bin-hadoop3/bin/find-spark-home\n","spark-3.3.0-bin-hadoop3/bin/spark-class.cmd\n","spark-3.3.0-bin-hadoop3/bin/sparkR2.cmd\n","spark-3.3.0-bin-hadoop3/bin/beeline\n","spark-3.3.0-bin-hadoop3/bin/spark-class2.cmd\n","spark-3.3.0-bin-hadoop3/bin/spark-sql.cmd\n","spark-3.3.0-bin-hadoop3/bin/run-example\n","spark-3.3.0-bin-hadoop3/bin/spark-shell\n","spark-3.3.0-bin-hadoop3/bin/run-example.cmd\n","spark-3.3.0-bin-hadoop3/bin/spark-sql2.cmd\n","spark-3.3.0-bin-hadoop3/python/\n","spark-3.3.0-bin-hadoop3/python/.gitignore\n","spark-3.3.0-bin-hadoop3/python/run-tests-with-coverage\n","spark-3.3.0-bin-hadoop3/python/mypy.ini\n","spark-3.3.0-bin-hadoop3/python/MANIFEST.in\n","spark-3.3.0-bin-hadoop3/python/README.md\n","spark-3.3.0-bin-hadoop3/python/test_coverage/\n","spark-3.3.0-bin-hadoop3/python/test_coverage/coverage_daemon.py\n","spark-3.3.0-bin-hadoop3/python/test_coverage/conf/\n","spark-3.3.0-bin-hadoop3/python/test_coverage/conf/spark-defaults.conf\n","spark-3.3.0-bin-hadoop3/python/test_coverage/sitecustomize.py\n","spark-3.3.0-bin-hadoop3/python/run-tests.py\n","spark-3.3.0-bin-hadoop3/python/setup.py\n","spark-3.3.0-bin-hadoop3/python/test_support/\n","spark-3.3.0-bin-hadoop3/python/test_support/userlibrary.py\n","spark-3.3.0-bin-hadoop3/python/test_support/hello/\n","spark-3.3.0-bin-hadoop3/python/test_support/hello/sub_hello/\n","spark-3.3.0-bin-hadoop3/python/test_support/hello/sub_hello/sub_hello.txt\n","spark-3.3.0-bin-hadoop3/python/test_support/hello/hello.txt\n","spark-3.3.0-bin-hadoop3/python/test_support/userlib-0.1.zip\n","spark-3.3.0-bin-hadoop3/python/test_support/SimpleHTTPServer.py\n","spark-3.3.0-bin-hadoop3/python/test_support/sql/\n","spark-3.3.0-bin-hadoop3/python/test_support/sql/people.json\n","spark-3.3.0-bin-hadoop3/python/test_support/sql/people_array.json\n","spark-3.3.0-bin-hadoop3/python/test_support/sql/people_array_utf16le.json\n","spark-3.3.0-bin-hadoop3/python/test_support/sql/text-test.txt\n","spark-3.3.0-bin-hadoop3/python/test_support/sql/ages.csv\n","spark-3.3.0-bin-hadoop3/python/test_support/sql/parquet_partitioned/\n","spark-3.3.0-bin-hadoop3/python/test_support/sql/parquet_partitioned/year=2015/\n","spark-3.3.0-bin-hadoop3/python/test_support/sql/parquet_partitioned/year=2015/month=10/\n","spark-3.3.0-bin-hadoop3/python/test_support/sql/parquet_partitioned/year=2015/month=10/day=26/\n","spark-3.3.0-bin-hadoop3/python/test_support/sql/parquet_partitioned/year=2015/month=10/day=26/.part-r-00005.gz.parquet.crc\n","spark-3.3.0-bin-hadoop3/python/test_support/sql/parquet_partitioned/year=2015/month=10/day=26/part-r-00005.gz.parquet\n","spark-3.3.0-bin-hadoop3/python/test_support/sql/parquet_partitioned/year=2015/month=10/day=25/\n","spark-3.3.0-bin-hadoop3/python/test_support/sql/parquet_partitioned/year=2015/month=10/day=25/.part-r-00002.gz.parquet.crc\n","spark-3.3.0-bin-hadoop3/python/test_support/sql/parquet_partitioned/year=2015/month=10/day=25/.part-r-00004.gz.parquet.crc\n","spark-3.3.0-bin-hadoop3/python/test_support/sql/parquet_partitioned/year=2015/month=10/day=25/part-r-00004.gz.parquet\n","spark-3.3.0-bin-hadoop3/python/test_support/sql/parquet_partitioned/year=2015/month=10/day=25/part-r-00002.gz.parquet\n","spark-3.3.0-bin-hadoop3/python/test_support/sql/parquet_partitioned/year=2015/month=9/\n","spark-3.3.0-bin-hadoop3/python/test_support/sql/parquet_partitioned/year=2015/month=9/day=1/\n","spark-3.3.0-bin-hadoop3/python/test_support/sql/parquet_partitioned/year=2015/month=9/day=1/.part-r-00007.gz.parquet.crc\n","spark-3.3.0-bin-hadoop3/python/test_support/sql/parquet_partitioned/year=2015/month=9/day=1/part-r-00007.gz.parquet\n","spark-3.3.0-bin-hadoop3/python/test_support/sql/parquet_partitioned/_metadata\n","spark-3.3.0-bin-hadoop3/python/test_support/sql/parquet_partitioned/year=2014/\n","spark-3.3.0-bin-hadoop3/python/test_support/sql/parquet_partitioned/year=2014/month=9/\n","spark-3.3.0-bin-hadoop3/python/test_support/sql/parquet_partitioned/year=2014/month=9/day=1/\n","spark-3.3.0-bin-hadoop3/python/test_support/sql/parquet_partitioned/year=2014/month=9/day=1/.part-r-00008.gz.parquet.crc\n","spark-3.3.0-bin-hadoop3/python/test_support/sql/parquet_partitioned/year=2014/month=9/day=1/part-r-00008.gz.parquet\n","spark-3.3.0-bin-hadoop3/python/test_support/sql/parquet_partitioned/_common_metadata\n","spark-3.3.0-bin-hadoop3/python/test_support/sql/parquet_partitioned/_SUCCESS\n","spark-3.3.0-bin-hadoop3/python/test_support/sql/orc_partitioned/\n","spark-3.3.0-bin-hadoop3/python/test_support/sql/orc_partitioned/b=1/\n","spark-3.3.0-bin-hadoop3/python/test_support/sql/orc_partitioned/b=1/c=1/\n","spark-3.3.0-bin-hadoop3/python/test_support/sql/orc_partitioned/b=1/c=1/part-r-00000-829af031-b970-49d6-ad39-30460a0be2c8.orc\n","spark-3.3.0-bin-hadoop3/python/test_support/sql/orc_partitioned/b=1/c=1/.part-r-00000-829af031-b970-49d6-ad39-30460a0be2c8.orc.crc\n","spark-3.3.0-bin-hadoop3/python/test_support/sql/orc_partitioned/b=0/\n","spark-3.3.0-bin-hadoop3/python/test_support/sql/orc_partitioned/b=0/c=0/\n","spark-3.3.0-bin-hadoop3/python/test_support/sql/orc_partitioned/b=0/c=0/part-r-00000-829af031-b970-49d6-ad39-30460a0be2c8.orc\n","spark-3.3.0-bin-hadoop3/python/test_support/sql/orc_partitioned/b=0/c=0/.part-r-00000-829af031-b970-49d6-ad39-30460a0be2c8.orc.crc\n","spark-3.3.0-bin-hadoop3/python/test_support/sql/orc_partitioned/_SUCCESS\n","spark-3.3.0-bin-hadoop3/python/test_support/sql/ages_newlines.csv\n","spark-3.3.0-bin-hadoop3/python/test_support/sql/streaming/\n","spark-3.3.0-bin-hadoop3/python/test_support/sql/streaming/text-test.txt\n","spark-3.3.0-bin-hadoop3/python/test_support/sql/people1.json\n","spark-3.3.0-bin-hadoop3/python/pyspark/\n","spark-3.3.0-bin-hadoop3/python/pyspark/tests/\n","spark-3.3.0-bin-hadoop3/python/pyspark/tests/test_rddbarrier.py\n","spark-3.3.0-bin-hadoop3/python/pyspark/tests/test_worker.py\n","spark-3.3.0-bin-hadoop3/python/pyspark/tests/test_serializers.py\n","spark-3.3.0-bin-hadoop3/python/pyspark/tests/test_util.py\n","spark-3.3.0-bin-hadoop3/python/pyspark/tests/test_rdd.py\n","spark-3.3.0-bin-hadoop3/python/pyspark/tests/test_statcounter.py\n","spark-3.3.0-bin-hadoop3/python/pyspark/tests/__init__.py\n","spark-3.3.0-bin-hadoop3/python/pyspark/tests/test_broadcast.py\n","spark-3.3.0-bin-hadoop3/python/pyspark/tests/test_appsubmit.py\n","spark-3.3.0-bin-hadoop3/python/pyspark/tests/test_profiler.py\n","spark-3.3.0-bin-hadoop3/python/pyspark/tests/test_pin_thread.py\n","spark-3.3.0-bin-hadoop3/python/pyspark/tests/test_install_spark.py\n","spark-3.3.0-bin-hadoop3/python/pyspark/tests/typing/\n","spark-3.3.0-bin-hadoop3/python/pyspark/tests/typing/test_resultiterable.yml\n","spark-3.3.0-bin-hadoop3/python/pyspark/tests/typing/test_context.yml\n","spark-3.3.0-bin-hadoop3/python/pyspark/tests/typing/test_rdd.yml\n","spark-3.3.0-bin-hadoop3/python/pyspark/tests/typing/test_core.yml\n","spark-3.3.0-bin-hadoop3/python/pyspark/tests/test_shuffle.py\n","spark-3.3.0-bin-hadoop3/python/pyspark/tests/test_join.py\n","spark-3.3.0-bin-hadoop3/python/pyspark/tests/test_taskcontext.py\n","spark-3.3.0-bin-hadoop3/python/pyspark/tests/test_context.py\n","spark-3.3.0-bin-hadoop3/python/pyspark/tests/test_readwrite.py\n","spark-3.3.0-bin-hadoop3/python/pyspark/tests/test_conf.py\n","spark-3.3.0-bin-hadoop3/python/pyspark/tests/test_daemon.py\n","spark-3.3.0-bin-hadoop3/python/pyspark/pandas/\n","spark-3.3.0-bin-hadoop3/python/pyspark/pandas/tests/\n","spark-3.3.0-bin-hadoop3/python/pyspark/pandas/tests/test_series.py\n","spark-3.3.0-bin-hadoop3/python/pyspark/pandas/tests/test_utils.py\n","spark-3.3.0-bin-hadoop3/python/pyspark/pandas/tests/test_dataframe.py\n","spark-3.3.0-bin-hadoop3/python/pyspark/pandas/tests/indexes/\n","spark-3.3.0-bin-hadoop3/python/pyspark/pandas/tests/indexes/__init__.py\n","spark-3.3.0-bin-hadoop3/python/pyspark/pandas/tests/indexes/test_timedelta.py\n","spark-3.3.0-bin-hadoop3/python/pyspark/pandas/tests/indexes/test_category.py\n","spark-3.3.0-bin-hadoop3/python/pyspark/pandas/tests/indexes/test_datetime.py\n","spark-3.3.0-bin-hadoop3/python/pyspark/pandas/tests/indexes/test_base.py\n","spark-3.3.0-bin-hadoop3/python/pyspark/pandas/tests/test_window.py\n","spark-3.3.0-bin-hadoop3/python/pyspark/pandas/tests/test_indexops_spark.py\n","spark-3.3.0-bin-hadoop3/python/pyspark/pandas/tests/__init__.py\n","spark-3.3.0-bin-hadoop3/python/pyspark/pandas/tests/test_stats.py\n","spark-3.3.0-bin-hadoop3/python/pyspark/pandas/tests/test_extension.py\n","spark-3.3.0-bin-hadoop3/python/pyspark/pandas/tests/test_default_index.py\n","spark-3.3.0-bin-hadoop3/python/pyspark/pandas/tests/test_expanding.py\n","spark-3.3.0-bin-hadoop3/python/pyspark/pandas/tests/test_series_datetime.py\n","spark-3.3.0-bin-hadoop3/python/pyspark/pandas/tests/test_indexing.py\n","spark-3.3.0-bin-hadoop3/python/pyspark/pandas/tests/test_typedef.py\n","spark-3.3.0-bin-hadoop3/python/pyspark/pandas/tests/test_numpy_compat.py\n","spark-3.3.0-bin-hadoop3/python/pyspark/pandas/tests/test_dataframe_spark_io.py\n","spark-3.3.0-bin-hadoop3/python/pyspark/pandas/tests/test_repr.py\n","spark-3.3.0-bin-hadoop3/python/pyspark/pandas/tests/test_ops_on_diff_frames_groupby_expanding.py\n","spark-3.3.0-bin-hadoop3/python/pyspark/pandas/tests/test_namespace.py\n","spark-3.3.0-bin-hadoop3/python/pyspark/pandas/tests/test_ops_on_diff_frames_groupby.py\n","spark-3.3.0-bin-hadoop3/python/pyspark/pandas/tests/test_series_conversion.py\n","spark-3.3.0-bin-hadoop3/python/pyspark/pandas/tests/test_internal.py\n","spark-3.3.0-bin-hadoop3/python/pyspark/pandas/tests/test_series_string.py\n","spark-3.3.0-bin-hadoop3/python/pyspark/pandas/tests/test_groupby.py\n","spark-3.3.0-bin-hadoop3/python/pyspark/pandas/tests/test_csv.py\n","spark-3.3.0-bin-hadoop3/python/pyspark/pandas/tests/test_ops_on_diff_frames_groupby_rolling.py\n","spark-3.3.0-bin-hadoop3/python/pyspark/pandas/tests/test_dataframe_conversion.py\n","spark-3.3.0-bin-hadoop3/python/pyspark/pandas/tests/data_type_ops/\n","spark-3.3.0-bin-hadoop3/python/pyspark/pandas/tests/data_type_ops/test_categorical_ops.py\n","spark-3.3.0-bin-hadoop3/python/pyspark/pandas/tests/data_type_ops/test_complex_ops.py\n","spark-3.3.0-bin-hadoop3/python/pyspark/pandas/tests/data_type_ops/test_boolean_ops.py\n","spark-3.3.0-bin-hadoop3/python/pyspark/pandas/tests/data_type_ops/test_string_ops.py\n","spark-3.3.0-bin-hadoop3/python/pyspark/pandas/tests/data_type_ops/__init__.py\n","spark-3.3.0-bin-hadoop3/python/pyspark/pandas/tests/data_type_ops/testing_utils.py\n","spark-3.3.0-bin-hadoop3/python/pyspark/pandas/tests/data_type_ops/test_udt_ops.py\n","spark-3.3.0-bin-hadoop3/python/pyspark/pandas/tests/data_type_ops/test_num_ops.py\n","spark-3.3.0-bin-hadoop3/python/pyspark/pandas/tests/data_type_ops/test_date_ops.py\n","spark-3.3.0-bin-hadoop3/python/pyspark/pandas/tests/data_type_ops/test_timedelta_ops.py\n","spark-3.3.0-bin-hadoop3/python/pyspark/pandas/tests/data_type_ops/test_null_ops.py\n","spark-3.3.0-bin-hadoop3/python/pyspark/pandas/tests/data_type_ops/test_datetime_ops.py\n","spark-3.3.0-bin-hadoop3/python/pyspark/pandas/tests/data_type_ops/test_binary_ops.py\n","spark-3.3.0-bin-hadoop3/python/pyspark/pandas/tests/data_type_ops/test_base.py\n","spark-3.3.0-bin-hadoop3/python/pyspark/pandas/tests/test_frame_spark.py\n","spark-3.3.0-bin-hadoop3/python/pyspark/pandas/tests/test_config.py\n","spark-3.3.0-bin-hadoop3/python/pyspark/pandas/tests/test_categorical.py\n","spark-3.3.0-bin-hadoop3/python/pyspark/pandas/tests/plot/\n","spark-3.3.0-bin-hadoop3/python/pyspark/pandas/tests/plot/__init__.py\n","spark-3.3.0-bin-hadoop3/python/pyspark/pandas/tests/plot/test_frame_plot.py\n","spark-3.3.0-bin-hadoop3/python/pyspark/pandas/tests/plot/test_frame_plot_matplotlib.py\n","spark-3.3.0-bin-hadoop3/python/pyspark/pandas/tests/plot/test_frame_plot_plotly.py\n","spark-3.3.0-bin-hadoop3/python/pyspark/pandas/tests/plot/test_series_plot_matplotlib.py\n","spark-3.3.0-bin-hadoop3/python/pyspark/pandas/tests/plot/test_series_plot_plotly.py\n","spark-3.3.0-bin-hadoop3/python/pyspark/pandas/tests/plot/test_series_plot.py\n","spark-3.3.0-bin-hadoop3/python/pyspark/pandas/tests/test_ops_on_diff_frames.py\n","spark-3.3.0-bin-hadoop3/python/pyspark/pandas/tests/test_rolling.py\n","spark-3.3.0-bin-hadoop3/python/pyspark/pandas/tests/test_reshape.py\n","spark-3.3.0-bin-hadoop3/python/pyspark/pandas/tests/test_sql.py\n","spark-3.3.0-bin-hadoop3/python/pyspark/pandas/tests/test_spark_functions.py\n","spark-3.3.0-bin-hadoop3/python/pyspark/pandas/sql_processor.py\n","spark-3.3.0-bin-hadoop3/python/pyspark/pandas/numpy_compat.py\n","spark-3.3.0-bin-hadoop3/python/pyspark/pandas/usage_logging/\n","spark-3.3.0-bin-hadoop3/python/pyspark/pandas/usage_logging/__init__.py\n","spark-3.3.0-bin-hadoop3/python/pyspark/pandas/usage_logging/usage_logger.py\n","spark-3.3.0-bin-hadoop3/python/pyspark/pandas/spark/\n","spark-3.3.0-bin-hadoop3/python/pyspark/pandas/spark/functions.py\n","spark-3.3.0-bin-hadoop3/python/pyspark/pandas/spark/__init__.py\n","spark-3.3.0-bin-hadoop3/python/pyspark/pandas/spark/accessors.py\n","spark-3.3.0-bin-hadoop3/python/pyspark/pandas/spark/utils.py\n","spark-3.3.0-bin-hadoop3/python/pyspark/pandas/base.py\n","spark-3.3.0-bin-hadoop3/python/pyspark/pandas/sql_formatter.py\n","spark-3.3.0-bin-hadoop3/python/pyspark/pandas/frame.py\n","spark-3.3.0-bin-hadoop3/python/pyspark/pandas/window.py\n","spark-3.3.0-bin-hadoop3/python/pyspark/pandas/indexes/\n","spark-3.3.0-bin-hadoop3/python/pyspark/pandas/indexes/base.py\n","spark-3.3.0-bin-hadoop3/python/pyspark/pandas/indexes/__init__.py\n","spark-3.3.0-bin-hadoop3/python/pyspark/pandas/indexes/timedelta.py\n","spark-3.3.0-bin-hadoop3/python/pyspark/pandas/indexes/numeric.py\n","spark-3.3.0-bin-hadoop3/python/pyspark/pandas/indexes/datetimes.py\n","spark-3.3.0-bin-hadoop3/python/pyspark/pandas/indexes/category.py\n","spark-3.3.0-bin-hadoop3/python/pyspark/pandas/indexes/multi.py\n","spark-3.3.0-bin-hadoop3/python/pyspark/pandas/namespace.py\n","spark-3.3.0-bin-hadoop3/python/pyspark/pandas/mlflow.py\n","spark-3.3.0-bin-hadoop3/python/pyspark/pandas/indexing.py\n","spark-3.3.0-bin-hadoop3/python/pyspark/pandas/strings.py\n","spark-3.3.0-bin-hadoop3/python/pyspark/pandas/__init__.py\n","spark-3.3.0-bin-hadoop3/python/pyspark/pandas/extensions.py\n","spark-3.3.0-bin-hadoop3/python/pyspark/pandas/accessors.py\n","spark-3.3.0-bin-hadoop3/python/pyspark/pandas/config.py\n","spark-3.3.0-bin-hadoop3/python/pyspark/pandas/groupby.py\n","spark-3.3.0-bin-hadoop3/python/pyspark/pandas/internal.py\n","spark-3.3.0-bin-hadoop3/python/pyspark/pandas/missing/\n","spark-3.3.0-bin-hadoop3/python/pyspark/pandas/missing/frame.py\n","spark-3.3.0-bin-hadoop3/python/pyspark/pandas/missing/window.py\n","spark-3.3.0-bin-hadoop3/python/pyspark/pandas/missing/__init__.py\n","spark-3.3.0-bin-hadoop3/python/pyspark/pandas/missing/groupby.py\n","spark-3.3.0-bin-hadoop3/python/pyspark/pandas/missing/series.py\n","spark-3.3.0-bin-hadoop3/python/pyspark/pandas/missing/indexes.py\n","spark-3.3.0-bin-hadoop3/python/pyspark/pandas/missing/common.py\n","spark-3.3.0-bin-hadoop3/python/pyspark/pandas/ml.py\n","spark-3.3.0-bin-hadoop3/python/pyspark/pandas/series.py\n","spark-3.3.0-bin-hadoop3/python/pyspark/pandas/_typing.py\n","spark-3.3.0-bin-hadoop3/python/pyspark/pandas/datetimes.py\n","spark-3.3.0-bin-hadoop3/python/pyspark/pandas/utils.py\n","spark-3.3.0-bin-hadoop3/python/pyspark/pandas/data_type_ops/\n","spark-3.3.0-bin-hadoop3/python/pyspark/pandas/data_type_ops/complex_ops.py\n","spark-3.3.0-bin-hadoop3/python/pyspark/pandas/data_type_ops/binary_ops.py\n","spark-3.3.0-bin-hadoop3/python/pyspark/pandas/data_type_ops/base.py\n","spark-3.3.0-bin-hadoop3/python/pyspark/pandas/data_type_ops/categorical_ops.py\n","spark-3.3.0-bin-hadoop3/python/pyspark/pandas/data_type_ops/string_ops.py\n","spark-3.3.0-bin-hadoop3/python/pyspark/pandas/data_type_ops/timedelta_ops.py\n","spark-3.3.0-bin-hadoop3/python/pyspark/pandas/data_type_ops/__init__.py\n","spark-3.3.0-bin-hadoop3/python/pyspark/pandas/data_type_ops/date_ops.py\n","spark-3.3.0-bin-hadoop3/python/pyspark/pandas/data_type_ops/udt_ops.py\n","spark-3.3.0-bin-hadoop3/python/pyspark/pandas/data_type_ops/null_ops.py\n","spark-3.3.0-bin-hadoop3/python/pyspark/pandas/data_type_ops/datetime_ops.py\n","spark-3.3.0-bin-hadoop3/python/pyspark/pandas/data_type_ops/num_ops.py\n","spark-3.3.0-bin-hadoop3/python/pyspark/pandas/data_type_ops/boolean_ops.py\n","spark-3.3.0-bin-hadoop3/python/pyspark/pandas/plot/\n","spark-3.3.0-bin-hadoop3/python/pyspark/pandas/plot/core.py\n","spark-3.3.0-bin-hadoop3/python/pyspark/pandas/plot/__init__.py\n","spark-3.3.0-bin-hadoop3/python/pyspark/pandas/plot/plotly.py\n","spark-3.3.0-bin-hadoop3/python/pyspark/pandas/plot/matplotlib.py\n","spark-3.3.0-bin-hadoop3/python/pyspark/pandas/typedef/\n","spark-3.3.0-bin-hadoop3/python/pyspark/pandas/typedef/__init__.py\n","spark-3.3.0-bin-hadoop3/python/pyspark/pandas/typedef/typehints.py\n","spark-3.3.0-bin-hadoop3/python/pyspark/pandas/categorical.py\n","spark-3.3.0-bin-hadoop3/python/pyspark/pandas/generic.py\n","spark-3.3.0-bin-hadoop3/python/pyspark/pandas/exceptions.py\n","spark-3.3.0-bin-hadoop3/python/pyspark/_typing.pyi\n","spark-3.3.0-bin-hadoop3/python/pyspark/testing/\n","spark-3.3.0-bin-hadoop3/python/pyspark/testing/mlutils.py\n","spark-3.3.0-bin-hadoop3/python/pyspark/testing/__init__.py\n","spark-3.3.0-bin-hadoop3/python/pyspark/testing/mllibutils.py\n","spark-3.3.0-bin-hadoop3/python/pyspark/testing/pandasutils.py\n","spark-3.3.0-bin-hadoop3/python/pyspark/testing/utils.py\n","spark-3.3.0-bin-hadoop3/python/pyspark/testing/sqlutils.py\n","spark-3.3.0-bin-hadoop3/python/pyspark/testing/streamingutils.py\n","spark-3.3.0-bin-hadoop3/python/pyspark/accumulators.py\n","spark-3.3.0-bin-hadoop3/python/pyspark/rddsampler.py\n","spark-3.3.0-bin-hadoop3/python/pyspark/install.py\n","spark-3.3.0-bin-hadoop3/python/pyspark/ml/\n","spark-3.3.0-bin-hadoop3/python/pyspark/ml/tests/\n","spark-3.3.0-bin-hadoop3/python/pyspark/ml/tests/test_algorithms.py\n","spark-3.3.0-bin-hadoop3/python/pyspark/ml/tests/test_evaluation.py\n","spark-3.3.0-bin-hadoop3/python/pyspark/ml/tests/test_util.py\n","spark-3.3.0-bin-hadoop3/python/pyspark/ml/tests/test_feature.py\n","spark-3.3.0-bin-hadoop3/python/pyspark/ml/tests/test_pipeline.py\n","spark-3.3.0-bin-hadoop3/python/pyspark/ml/tests/test_wrapper.py\n","spark-3.3.0-bin-hadoop3/python/pyspark/ml/tests/__init__.py\n","spark-3.3.0-bin-hadoop3/python/pyspark/ml/tests/test_tuning.py\n","spark-3.3.0-bin-hadoop3/python/pyspark/ml/tests/test_persistence.py\n","spark-3.3.0-bin-hadoop3/python/pyspark/ml/tests/test_param.py\n","spark-3.3.0-bin-hadoop3/python/pyspark/ml/tests/typing/\n","spark-3.3.0-bin-hadoop3/python/pyspark/ml/tests/typing/test_clustering.yaml\n","spark-3.3.0-bin-hadoop3/python/pyspark/ml/tests/typing/test_regression.yml\n","spark-3.3.0-bin-hadoop3/python/pyspark/ml/tests/typing/test_param.yml\n","spark-3.3.0-bin-hadoop3/python/pyspark/ml/tests/typing/test_evaluation.yml\n","spark-3.3.0-bin-hadoop3/python/pyspark/ml/tests/typing/test_classification.yml\n","spark-3.3.0-bin-hadoop3/python/pyspark/ml/tests/typing/test_readable.yml\n","spark-3.3.0-bin-hadoop3/python/pyspark/ml/tests/typing/test_feature.yml\n","spark-3.3.0-bin-hadoop3/python/pyspark/ml/tests/test_training_summary.py\n","spark-3.3.0-bin-hadoop3/python/pyspark/ml/tests/test_linalg.py\n","spark-3.3.0-bin-hadoop3/python/pyspark/ml/tests/test_image.py\n","spark-3.3.0-bin-hadoop3/python/pyspark/ml/tests/test_stat.py\n","spark-3.3.0-bin-hadoop3/python/pyspark/ml/tests/test_base.py\n","spark-3.3.0-bin-hadoop3/python/pyspark/ml/_typing.pyi\n","spark-3.3.0-bin-hadoop3/python/pyspark/ml/functions.py\n","spark-3.3.0-bin-hadoop3/python/pyspark/ml/tuning.py\n","spark-3.3.0-bin-hadoop3/python/pyspark/ml/pipeline.py\n","spark-3.3.0-bin-hadoop3/python/pyspark/ml/base.py\n","spark-3.3.0-bin-hadoop3/python/pyspark/ml/feature.py\n","spark-3.3.0-bin-hadoop3/python/pyspark/ml/__init__.py\n","spark-3.3.0-bin-hadoop3/python/pyspark/ml/stat.py\n","spark-3.3.0-bin-hadoop3/python/pyspark/ml/image.py\n","spark-3.3.0-bin-hadoop3/python/pyspark/ml/classification.py\n","spark-3.3.0-bin-hadoop3/python/pyspark/ml/recommendation.py\n","spark-3.3.0-bin-hadoop3/python/pyspark/ml/regression.py\n","spark-3.3.0-bin-hadoop3/python/pyspark/ml/param/\n","spark-3.3.0-bin-hadoop3/python/pyspark/ml/param/_shared_params_code_gen.py\n","spark-3.3.0-bin-hadoop3/python/pyspark/ml/param/__init__.py\n","spark-3.3.0-bin-hadoop3/python/pyspark/ml/param/shared.py\n","spark-3.3.0-bin-hadoop3/python/pyspark/ml/tree.py\n","spark-3.3.0-bin-hadoop3/python/pyspark/ml/fpm.py\n","spark-3.3.0-bin-hadoop3/python/pyspark/ml/wrapper.py\n","spark-3.3.0-bin-hadoop3/python/pyspark/ml/clustering.py\n","spark-3.3.0-bin-hadoop3/python/pyspark/ml/common.py\n","spark-3.3.0-bin-hadoop3/python/pyspark/ml/linalg/\n","spark-3.3.0-bin-hadoop3/python/pyspark/ml/linalg/__init__.py\n","spark-3.3.0-bin-hadoop3/python/pyspark/ml/evaluation.py\n","spark-3.3.0-bin-hadoop3/python/pyspark/ml/util.py\n","spark-3.3.0-bin-hadoop3/python/pyspark/find_spark_home.py\n","spark-3.3.0-bin-hadoop3/python/pyspark/serializers.py\n","spark-3.3.0-bin-hadoop3/python/pyspark/java_gateway.py\n","spark-3.3.0-bin-hadoop3/python/pyspark/traceback_utils.py\n","spark-3.3.0-bin-hadoop3/python/pyspark/resource/\n","spark-3.3.0-bin-hadoop3/python/pyspark/resource/tests/\n","spark-3.3.0-bin-hadoop3/python/pyspark/resource/tests/__init__.py\n","spark-3.3.0-bin-hadoop3/python/pyspark/resource/tests/test_resources.py\n","spark-3.3.0-bin-hadoop3/python/pyspark/resource/profile.py\n","spark-3.3.0-bin-hadoop3/python/pyspark/resource/__init__.py\n","spark-3.3.0-bin-hadoop3/python/pyspark/resource/information.py\n","spark-3.3.0-bin-hadoop3/python/pyspark/resource/requests.py\n","spark-3.3.0-bin-hadoop3/python/pyspark/conf.py\n","spark-3.3.0-bin-hadoop3/python/pyspark/__init__.py\n","spark-3.3.0-bin-hadoop3/python/pyspark/__pycache__/\n","spark-3.3.0-bin-hadoop3/python/pyspark/__pycache__/install.cpython-38.pyc\n","spark-3.3.0-bin-hadoop3/python/pyspark/mllib/\n","spark-3.3.0-bin-hadoop3/python/pyspark/mllib/tests/\n","spark-3.3.0-bin-hadoop3/python/pyspark/mllib/tests/test_algorithms.py\n","spark-3.3.0-bin-hadoop3/python/pyspark/mllib/tests/test_streaming_algorithms.py\n","spark-3.3.0-bin-hadoop3/python/pyspark/mllib/tests/test_util.py\n","spark-3.3.0-bin-hadoop3/python/pyspark/mllib/tests/test_feature.py\n","spark-3.3.0-bin-hadoop3/python/pyspark/mllib/tests/__init__.py\n","spark-3.3.0-bin-hadoop3/python/pyspark/mllib/tests/test_linalg.py\n","spark-3.3.0-bin-hadoop3/python/pyspark/mllib/tests/test_stat.py\n","spark-3.3.0-bin-hadoop3/python/pyspark/mllib/_typing.pyi\n","spark-3.3.0-bin-hadoop3/python/pyspark/mllib/recommendation.pyi\n","spark-3.3.0-bin-hadoop3/python/pyspark/mllib/feature.py\n","spark-3.3.0-bin-hadoop3/python/pyspark/mllib/__init__.py\n","spark-3.3.0-bin-hadoop3/python/pyspark/mllib/classification.py\n","spark-3.3.0-bin-hadoop3/python/pyspark/mllib/random.pyi\n","spark-3.3.0-bin-hadoop3/python/pyspark/mllib/recommendation.py\n","spark-3.3.0-bin-hadoop3/python/pyspark/mllib/regression.py\n","spark-3.3.0-bin-hadoop3/python/pyspark/mllib/tree.py\n","spark-3.3.0-bin-hadoop3/python/pyspark/mllib/fpm.py\n","spark-3.3.0-bin-hadoop3/python/pyspark/mllib/random.py\n","spark-3.3.0-bin-hadoop3/python/pyspark/mllib/stat/\n","spark-3.3.0-bin-hadoop3/python/pyspark/mllib/stat/distribution.py\n","spark-3.3.0-bin-hadoop3/python/pyspark/mllib/stat/__init__.py\n","spark-3.3.0-bin-hadoop3/python/pyspark/mllib/stat/KernelDensity.py\n","spark-3.3.0-bin-hadoop3/python/pyspark/mllib/stat/test.py\n","spark-3.3.0-bin-hadoop3/python/pyspark/mllib/stat/_statistics.py\n","spark-3.3.0-bin-hadoop3/python/pyspark/mllib/clustering.py\n","spark-3.3.0-bin-hadoop3/python/pyspark/mllib/common.py\n","spark-3.3.0-bin-hadoop3/python/pyspark/mllib/linalg/\n","spark-3.3.0-bin-hadoop3/python/pyspark/mllib/linalg/__init__.py\n","spark-3.3.0-bin-hadoop3/python/pyspark/mllib/linalg/distributed.py\n","spark-3.3.0-bin-hadoop3/python/pyspark/mllib/evaluation.py\n","spark-3.3.0-bin-hadoop3/python/pyspark/mllib/util.py\n","spark-3.3.0-bin-hadoop3/python/pyspark/resultiterable.py\n","spark-3.3.0-bin-hadoop3/python/pyspark/profiler.py\n","spark-3.3.0-bin-hadoop3/python/pyspark/statcounter.py\n","spark-3.3.0-bin-hadoop3/python/pyspark/join.py\n","spark-3.3.0-bin-hadoop3/python/pyspark/daemon.py\n","spark-3.3.0-bin-hadoop3/python/pyspark/rdd.py\n","spark-3.3.0-bin-hadoop3/python/pyspark/context.py\n","spark-3.3.0-bin-hadoop3/python/pyspark/instrumentation_utils.py\n","spark-3.3.0-bin-hadoop3/python/pyspark/cloudpickle/\n","spark-3.3.0-bin-hadoop3/python/pyspark/cloudpickle/cloudpickle_fast.py\n","spark-3.3.0-bin-hadoop3/python/pyspark/cloudpickle/__init__.py\n","spark-3.3.0-bin-hadoop3/python/pyspark/cloudpickle/compat.py\n","spark-3.3.0-bin-hadoop3/python/pyspark/cloudpickle/cloudpickle.py\n","spark-3.3.0-bin-hadoop3/python/pyspark/version.py\n","spark-3.3.0-bin-hadoop3/python/pyspark/py.typed\n","spark-3.3.0-bin-hadoop3/python/pyspark/files.py\n","spark-3.3.0-bin-hadoop3/python/pyspark/worker.py\n","spark-3.3.0-bin-hadoop3/python/pyspark/shell.py\n","spark-3.3.0-bin-hadoop3/python/pyspark/streaming/\n","spark-3.3.0-bin-hadoop3/python/pyspark/streaming/tests/\n","spark-3.3.0-bin-hadoop3/python/pyspark/streaming/tests/test_listener.py\n","spark-3.3.0-bin-hadoop3/python/pyspark/streaming/tests/test_kinesis.py\n","spark-3.3.0-bin-hadoop3/python/pyspark/streaming/tests/__init__.py\n","spark-3.3.0-bin-hadoop3/python/pyspark/streaming/tests/test_dstream.py\n","spark-3.3.0-bin-hadoop3/python/pyspark/streaming/tests/test_context.py\n","spark-3.3.0-bin-hadoop3/python/pyspark/streaming/dstream.py\n","spark-3.3.0-bin-hadoop3/python/pyspark/streaming/__init__.py\n","spark-3.3.0-bin-hadoop3/python/pyspark/streaming/kinesis.py\n","spark-3.3.0-bin-hadoop3/python/pyspark/streaming/listener.py\n","spark-3.3.0-bin-hadoop3/python/pyspark/streaming/context.py\n","spark-3.3.0-bin-hadoop3/python/pyspark/streaming/util.py\n","spark-3.3.0-bin-hadoop3/python/pyspark/status.py\n","spark-3.3.0-bin-hadoop3/python/pyspark/sql/\n","spark-3.3.0-bin-hadoop3/python/pyspark/sql/tests/\n","spark-3.3.0-bin-hadoop3/python/pyspark/sql/tests/test_functions.py\n","spark-3.3.0-bin-hadoop3/python/pyspark/sql/tests/test_readwriter.py\n","spark-3.3.0-bin-hadoop3/python/pyspark/sql/tests/test_utils.py\n","spark-3.3.0-bin-hadoop3/python/pyspark/sql/tests/test_pandas_grouped_map.py\n","spark-3.3.0-bin-hadoop3/python/pyspark/sql/tests/test_dataframe.py\n","spark-3.3.0-bin-hadoop3/python/pyspark/sql/tests/test_pandas_map.py\n","spark-3.3.0-bin-hadoop3/python/pyspark/sql/tests/test_udf.py\n","spark-3.3.0-bin-hadoop3/python/pyspark/sql/tests/test_streaming.py\n","spark-3.3.0-bin-hadoop3/python/pyspark/sql/tests/__init__.py\n","spark-3.3.0-bin-hadoop3/python/pyspark/sql/tests/test_serde.py\n","spark-3.3.0-bin-hadoop3/python/pyspark/sql/tests/test_pandas_udf_window.py\n","spark-3.3.0-bin-hadoop3/python/pyspark/sql/tests/test_group.py\n","spark-3.3.0-bin-hadoop3/python/pyspark/sql/tests/test_pandas_udf.py\n","spark-3.3.0-bin-hadoop3/python/pyspark/sql/tests/test_pandas_cogrouped_map.py\n","spark-3.3.0-bin-hadoop3/python/pyspark/sql/tests/test_pandas_udf_grouped_agg.py\n","spark-3.3.0-bin-hadoop3/python/pyspark/sql/tests/test_pandas_udf_scalar.py\n","spark-3.3.0-bin-hadoop3/python/pyspark/sql/tests/typing/\n","spark-3.3.0-bin-hadoop3/python/pyspark/sql/tests/typing/test_column.yml\n","spark-3.3.0-bin-hadoop3/python/pyspark/sql/tests/typing/test_session.yml\n","spark-3.3.0-bin-hadoop3/python/pyspark/sql/tests/typing/test_dataframe.yml\n","spark-3.3.0-bin-hadoop3/python/pyspark/sql/tests/typing/test_functions.yml\n","spark-3.3.0-bin-hadoop3/python/pyspark/sql/tests/typing/test_readwriter.yml\n","spark-3.3.0-bin-hadoop3/python/pyspark/sql/tests/typing/test_udf.yml\n","spark-3.3.0-bin-hadoop3/python/pyspark/sql/tests/test_udf_profiler.py\n","spark-3.3.0-bin-hadoop3/python/pyspark/sql/tests/test_pandas_udf_typehints_with_future_annotations.py\n","spark-3.3.0-bin-hadoop3/python/pyspark/sql/tests/test_catalog.py\n","spark-3.3.0-bin-hadoop3/python/pyspark/sql/tests/test_datasources.py\n","spark-3.3.0-bin-hadoop3/python/pyspark/sql/tests/test_pandas_udf_typehints.py\n","spark-3.3.0-bin-hadoop3/python/pyspark/sql/tests/test_arrow_map.py\n","spark-3.3.0-bin-hadoop3/python/pyspark/sql/tests/test_types.py\n","spark-3.3.0-bin-hadoop3/python/pyspark/sql/tests/test_column.py\n","spark-3.3.0-bin-hadoop3/python/pyspark/sql/tests/test_context.py\n","spark-3.3.0-bin-hadoop3/python/pyspark/sql/tests/test_conf.py\n","spark-3.3.0-bin-hadoop3/python/pyspark/sql/tests/test_arrow.py\n","spark-3.3.0-bin-hadoop3/python/pyspark/sql/tests/test_session.py\n","spark-3.3.0-bin-hadoop3/python/pyspark/sql/pandas/\n","spark-3.3.0-bin-hadoop3/python/pyspark/sql/pandas/functions.py\n","spark-3.3.0-bin-hadoop3/python/pyspark/sql/pandas/serializers.py\n","spark-3.3.0-bin-hadoop3/python/pyspark/sql/pandas/__init__.py\n","spark-3.3.0-bin-hadoop3/python/pyspark/sql/pandas/typehints.py\n","spark-3.3.0-bin-hadoop3/python/pyspark/sql/pandas/map_ops.py\n","spark-3.3.0-bin-hadoop3/python/pyspark/sql/pandas/types.py\n","spark-3.3.0-bin-hadoop3/python/pyspark/sql/pandas/group_ops.py\n","spark-3.3.0-bin-hadoop3/python/pyspark/sql/pandas/functions.pyi\n","spark-3.3.0-bin-hadoop3/python/pyspark/sql/pandas/utils.py\n","spark-3.3.0-bin-hadoop3/python/pyspark/sql/pandas/conversion.py\n","spark-3.3.0-bin-hadoop3/python/pyspark/sql/pandas/_typing/\n","spark-3.3.0-bin-hadoop3/python/pyspark/sql/pandas/_typing/__init__.pyi\n","spark-3.3.0-bin-hadoop3/python/pyspark/sql/pandas/_typing/protocols/\n","spark-3.3.0-bin-hadoop3/python/pyspark/sql/pandas/_typing/protocols/__init__.pyi\n","spark-3.3.0-bin-hadoop3/python/pyspark/sql/pandas/_typing/protocols/series.pyi\n","spark-3.3.0-bin-hadoop3/python/pyspark/sql/pandas/_typing/protocols/frame.pyi\n","spark-3.3.0-bin-hadoop3/python/pyspark/sql/_typing.pyi\n","spark-3.3.0-bin-hadoop3/python/pyspark/sql/functions.py\n","spark-3.3.0-bin-hadoop3/python/pyspark/sql/readwriter.py\n","spark-3.3.0-bin-hadoop3/python/pyspark/sql/catalog.py\n","spark-3.3.0-bin-hadoop3/python/pyspark/sql/sql_formatter.py\n","spark-3.3.0-bin-hadoop3/python/pyspark/sql/window.py\n","spark-3.3.0-bin-hadoop3/python/pyspark/sql/udf.py\n","spark-3.3.0-bin-hadoop3/python/pyspark/sql/conf.py\n","spark-3.3.0-bin-hadoop3/python/pyspark/sql/__init__.py\n","spark-3.3.0-bin-hadoop3/python/pyspark/sql/session.py\n","spark-3.3.0-bin-hadoop3/python/pyspark/sql/column.py\n","spark-3.3.0-bin-hadoop3/python/pyspark/sql/group.py\n","spark-3.3.0-bin-hadoop3/python/pyspark/sql/context.py\n","spark-3.3.0-bin-hadoop3/python/pyspark/sql/types.py\n","spark-3.3.0-bin-hadoop3/python/pyspark/sql/observation.py\n","spark-3.3.0-bin-hadoop3/python/pyspark/sql/dataframe.py\n","spark-3.3.0-bin-hadoop3/python/pyspark/sql/avro/\n","spark-3.3.0-bin-hadoop3/python/pyspark/sql/avro/functions.py\n","spark-3.3.0-bin-hadoop3/python/pyspark/sql/avro/__init__.py\n","spark-3.3.0-bin-hadoop3/python/pyspark/sql/utils.py\n","spark-3.3.0-bin-hadoop3/python/pyspark/sql/streaming.py\n","spark-3.3.0-bin-hadoop3/python/pyspark/python/\n","spark-3.3.0-bin-hadoop3/python/pyspark/python/pyspark/\n","spark-3.3.0-bin-hadoop3/python/pyspark/python/pyspark/shell.py\n","spark-3.3.0-bin-hadoop3/python/pyspark/shuffle.py\n","spark-3.3.0-bin-hadoop3/python/pyspark/taskcontext.py\n","spark-3.3.0-bin-hadoop3/python/pyspark/_globals.py\n","spark-3.3.0-bin-hadoop3/python/pyspark/broadcast.py\n","spark-3.3.0-bin-hadoop3/python/pyspark/util.py\n","spark-3.3.0-bin-hadoop3/python/pyspark/storagelevel.py\n","spark-3.3.0-bin-hadoop3/python/.coveragerc\n","spark-3.3.0-bin-hadoop3/python/docs/\n","spark-3.3.0-bin-hadoop3/python/docs/make2.bat\n","spark-3.3.0-bin-hadoop3/python/docs/source/\n","spark-3.3.0-bin-hadoop3/python/docs/source/reference/\n","spark-3.3.0-bin-hadoop3/python/docs/source/reference/pyspark.sql/\n","spark-3.3.0-bin-hadoop3/python/docs/source/reference/pyspark.sql/configuration.rst\n","spark-3.3.0-bin-hadoop3/python/docs/source/reference/pyspark.sql/window.rst\n","spark-3.3.0-bin-hadoop3/python/docs/source/reference/pyspark.sql/spark_session.rst\n","spark-3.3.0-bin-hadoop3/python/docs/source/reference/pyspark.sql/index.rst\n","spark-3.3.0-bin-hadoop3/python/docs/source/reference/pyspark.sql/grouping.rst\n","spark-3.3.0-bin-hadoop3/python/docs/source/reference/pyspark.sql/catalog.rst\n","spark-3.3.0-bin-hadoop3/python/docs/source/reference/pyspark.sql/column.rst\n","spark-3.3.0-bin-hadoop3/python/docs/source/reference/pyspark.sql/io.rst\n","spark-3.3.0-bin-hadoop3/python/docs/source/reference/pyspark.sql/observation.rst\n","spark-3.3.0-bin-hadoop3/python/docs/source/reference/pyspark.sql/core_classes.rst\n","spark-3.3.0-bin-hadoop3/python/docs/source/reference/pyspark.sql/row.rst\n","spark-3.3.0-bin-hadoop3/python/docs/source/reference/pyspark.sql/avro.rst\n","spark-3.3.0-bin-hadoop3/python/docs/source/reference/pyspark.sql/functions.rst\n","spark-3.3.0-bin-hadoop3/python/docs/source/reference/pyspark.sql/dataframe.rst\n","spark-3.3.0-bin-hadoop3/python/docs/source/reference/pyspark.sql/data_types.rst\n","spark-3.3.0-bin-hadoop3/python/docs/source/reference/index.rst\n","spark-3.3.0-bin-hadoop3/python/docs/source/reference/pyspark.ml.rst\n","spark-3.3.0-bin-hadoop3/python/docs/source/reference/pyspark.rst\n","spark-3.3.0-bin-hadoop3/python/docs/source/reference/pyspark.ss/\n","spark-3.3.0-bin-hadoop3/python/docs/source/reference/pyspark.ss/index.rst\n","spark-3.3.0-bin-hadoop3/python/docs/source/reference/pyspark.ss/query_management.rst\n","spark-3.3.0-bin-hadoop3/python/docs/source/reference/pyspark.ss/io.rst\n","spark-3.3.0-bin-hadoop3/python/docs/source/reference/pyspark.ss/core_classes.rst\n","spark-3.3.0-bin-hadoop3/python/docs/source/reference/pyspark.pandas/\n","spark-3.3.0-bin-hadoop3/python/docs/source/reference/pyspark.pandas/window.rst\n","spark-3.3.0-bin-hadoop3/python/docs/source/reference/pyspark.pandas/index.rst\n","spark-3.3.0-bin-hadoop3/python/docs/source/reference/pyspark.pandas/extensions.rst\n","spark-3.3.0-bin-hadoop3/python/docs/source/reference/pyspark.pandas/ml.rst\n","spark-3.3.0-bin-hadoop3/python/docs/source/reference/pyspark.pandas/frame.rst\n","spark-3.3.0-bin-hadoop3/python/docs/source/reference/pyspark.pandas/io.rst\n","spark-3.3.0-bin-hadoop3/python/docs/source/reference/pyspark.pandas/series.rst\n","spark-3.3.0-bin-hadoop3/python/docs/source/reference/pyspark.pandas/indexing.rst\n","spark-3.3.0-bin-hadoop3/python/docs/source/reference/pyspark.pandas/general_functions.rst\n","spark-3.3.0-bin-hadoop3/python/docs/source/reference/pyspark.pandas/groupby.rst\n","spark-3.3.0-bin-hadoop3/python/docs/source/reference/pyspark.mllib.rst\n","spark-3.3.0-bin-hadoop3/python/docs/source/reference/pyspark.streaming.rst\n","spark-3.3.0-bin-hadoop3/python/docs/source/reference/pyspark.resource.rst\n","spark-3.3.0-bin-hadoop3/python/docs/source/index.rst\n","spark-3.3.0-bin-hadoop3/python/docs/source/getting_started/\n","spark-3.3.0-bin-hadoop3/python/docs/source/getting_started/index.rst\n","spark-3.3.0-bin-hadoop3/python/docs/source/getting_started/quickstart_ps.ipynb\n","spark-3.3.0-bin-hadoop3/python/docs/source/getting_started/quickstart_df.ipynb\n","spark-3.3.0-bin-hadoop3/python/docs/source/getting_started/install.rst\n","spark-3.3.0-bin-hadoop3/python/docs/source/conf.py\n","spark-3.3.0-bin-hadoop3/python/docs/source/_templates/\n","spark-3.3.0-bin-hadoop3/python/docs/source/_templates/autosummary/\n","spark-3.3.0-bin-hadoop3/python/docs/source/_templates/autosummary/class.rst\n","spark-3.3.0-bin-hadoop3/python/docs/source/_templates/autosummary/class_with_docs.rst\n","spark-3.3.0-bin-hadoop3/python/docs/source/_static/\n","spark-3.3.0-bin-hadoop3/python/docs/source/_static/copybutton.js\n","spark-3.3.0-bin-hadoop3/python/docs/source/_static/css/\n","spark-3.3.0-bin-hadoop3/python/docs/source/_static/css/pyspark.css\n","spark-3.3.0-bin-hadoop3/python/docs/source/development/\n","spark-3.3.0-bin-hadoop3/python/docs/source/development/setting_ide.rst\n","spark-3.3.0-bin-hadoop3/python/docs/source/development/index.rst\n","spark-3.3.0-bin-hadoop3/python/docs/source/development/debugging.rst\n","spark-3.3.0-bin-hadoop3/python/docs/source/development/testing.rst\n","spark-3.3.0-bin-hadoop3/python/docs/source/development/contributing.rst\n","spark-3.3.0-bin-hadoop3/python/docs/source/user_guide/\n","spark-3.3.0-bin-hadoop3/python/docs/source/user_guide/arrow_pandas.rst\n","spark-3.3.0-bin-hadoop3/python/docs/source/user_guide/index.rst\n","spark-3.3.0-bin-hadoop3/python/docs/source/user_guide/pandas_on_spark/\n","spark-3.3.0-bin-hadoop3/python/docs/source/user_guide/pandas_on_spark/types.rst\n","spark-3.3.0-bin-hadoop3/python/docs/source/user_guide/pandas_on_spark/best_practices.rst\n","spark-3.3.0-bin-hadoop3/python/docs/source/user_guide/pandas_on_spark/options.rst\n","spark-3.3.0-bin-hadoop3/python/docs/source/user_guide/pandas_on_spark/pandas_pyspark.rst\n","spark-3.3.0-bin-hadoop3/python/docs/source/user_guide/pandas_on_spark/index.rst\n","spark-3.3.0-bin-hadoop3/python/docs/source/user_guide/pandas_on_spark/typehints.rst\n","spark-3.3.0-bin-hadoop3/python/docs/source/user_guide/pandas_on_spark/transform_apply.rst\n","spark-3.3.0-bin-hadoop3/python/docs/source/user_guide/pandas_on_spark/supported_pandas_api.rst\n","spark-3.3.0-bin-hadoop3/python/docs/source/user_guide/pandas_on_spark/from_to_dbms.rst\n","spark-3.3.0-bin-hadoop3/python/docs/source/user_guide/pandas_on_spark/faq.rst\n","spark-3.3.0-bin-hadoop3/python/docs/source/user_guide/python_packaging.rst\n","spark-3.3.0-bin-hadoop3/python/docs/source/user_guide/sql/\n","spark-3.3.0-bin-hadoop3/python/docs/source/user_guide/sql/arrow_pandas.rst\n","spark-3.3.0-bin-hadoop3/python/docs/source/user_guide/sql/index.rst\n","spark-3.3.0-bin-hadoop3/python/docs/source/migration_guide/\n","spark-3.3.0-bin-hadoop3/python/docs/source/migration_guide/pyspark_2.3.0_to_2.3.1_above.rst\n","spark-3.3.0-bin-hadoop3/python/docs/source/migration_guide/index.rst\n","spark-3.3.0-bin-hadoop3/python/docs/source/migration_guide/pyspark_2.4_to_3.0.rst\n","spark-3.3.0-bin-hadoop3/python/docs/source/migration_guide/pyspark_1.0_1.2_to_1.3.rst\n","spark-3.3.0-bin-hadoop3/python/docs/source/migration_guide/pyspark_3.2_to_3.3.rst\n","spark-3.3.0-bin-hadoop3/python/docs/source/migration_guide/pyspark_2.3_to_2.4.rst\n","spark-3.3.0-bin-hadoop3/python/docs/source/migration_guide/koalas_to_pyspark.rst\n","spark-3.3.0-bin-hadoop3/python/docs/source/migration_guide/pyspark_1.4_to_1.5.rst\n","spark-3.3.0-bin-hadoop3/python/docs/source/migration_guide/pyspark_2.2_to_2.3.rst\n","spark-3.3.0-bin-hadoop3/python/docs/source/migration_guide/pyspark_3.1_to_3.2.rst\n","spark-3.3.0-bin-hadoop3/python/docs/make.bat\n","spark-3.3.0-bin-hadoop3/python/docs/Makefile\n","spark-3.3.0-bin-hadoop3/python/lib/\n","spark-3.3.0-bin-hadoop3/python/lib/PY4J_LICENSE.txt\n","spark-3.3.0-bin-hadoop3/python/lib/py4j-0.10.9.5-src.zip\n","spark-3.3.0-bin-hadoop3/python/lib/pyspark.zip\n","spark-3.3.0-bin-hadoop3/python/run-tests\n","spark-3.3.0-bin-hadoop3/python/setup.cfg\n","spark-3.3.0-bin-hadoop3/python/dist/\n","spark-3.3.0-bin-hadoop3/python/pyspark.egg-info/\n","spark-3.3.0-bin-hadoop3/python/pyspark.egg-info/dependency_links.txt\n","spark-3.3.0-bin-hadoop3/python/pyspark.egg-info/PKG-INFO\n","spark-3.3.0-bin-hadoop3/python/pyspark.egg-info/top_level.txt\n","spark-3.3.0-bin-hadoop3/python/pyspark.egg-info/requires.txt\n","spark-3.3.0-bin-hadoop3/python/pyspark.egg-info/SOURCES.txt\n","spark-3.3.0-bin-hadoop3/licenses/\n","spark-3.3.0-bin-hadoop3/licenses/LICENSE-respond.txt\n","spark-3.3.0-bin-hadoop3/licenses/LICENSE-sbt-launch-lib.txt\n","spark-3.3.0-bin-hadoop3/licenses/LICENSE-antlr.txt\n","spark-3.3.0-bin-hadoop3/licenses/LICENSE-dagre-d3.txt\n","spark-3.3.0-bin-hadoop3/licenses/LICENSE-pyrolite.txt\n","spark-3.3.0-bin-hadoop3/licenses/LICENSE-sorttable.js.txt\n","spark-3.3.0-bin-hadoop3/licenses/LICENSE-janino.txt\n","spark-3.3.0-bin-hadoop3/licenses/LICENSE-protobuf.txt\n","spark-3.3.0-bin-hadoop3/licenses/LICENSE-jquery.txt\n","spark-3.3.0-bin-hadoop3/licenses/LICENSE-scopt.txt\n","spark-3.3.0-bin-hadoop3/licenses/LICENSE-netlib.txt\n","spark-3.3.0-bin-hadoop3/licenses/LICENSE-d3.min.js.txt\n","spark-3.3.0-bin-hadoop3/licenses/LICENSE-graphlib-dot.txt\n","spark-3.3.0-bin-hadoop3/licenses/LICENSE-AnchorJS.txt\n","spark-3.3.0-bin-hadoop3/licenses/LICENSE-datatables.txt\n","spark-3.3.0-bin-hadoop3/licenses/LICENSE-pmml-model.txt\n","spark-3.3.0-bin-hadoop3/licenses/LICENSE-paranamer.txt\n","spark-3.3.0-bin-hadoop3/licenses/LICENSE-jakarta-ws-rs-api\n","spark-3.3.0-bin-hadoop3/licenses/LICENSE-dnsjava.txt\n","spark-3.3.0-bin-hadoop3/licenses/LICENSE-jakarta.xml.bind-api.txt\n","spark-3.3.0-bin-hadoop3/licenses/LICENSE-jakarta-annotation-api\n","spark-3.3.0-bin-hadoop3/licenses/LICENSE-CC0.txt\n","spark-3.3.0-bin-hadoop3/licenses/LICENSE-jodd.txt\n","spark-3.3.0-bin-hadoop3/licenses/LICENSE-f2j.txt\n","spark-3.3.0-bin-hadoop3/licenses/LICENSE-machinist.txt\n","spark-3.3.0-bin-hadoop3/licenses/LICENSE-javolution.txt\n","spark-3.3.0-bin-hadoop3/licenses/LICENSE-modernizr.txt\n","spark-3.3.0-bin-hadoop3/licenses/LICENSE-spire.txt\n","spark-3.3.0-bin-hadoop3/licenses/LICENSE-leveldbjni.txt\n","spark-3.3.0-bin-hadoop3/licenses/LICENSE-join.txt\n","spark-3.3.0-bin-hadoop3/licenses/LICENSE-zstd-jni.txt\n","spark-3.3.0-bin-hadoop3/licenses/LICENSE-slf4j.txt\n","spark-3.3.0-bin-hadoop3/licenses/LICENSE-arpack.txt\n","spark-3.3.0-bin-hadoop3/licenses/LICENSE-jsp-api.txt\n","spark-3.3.0-bin-hadoop3/licenses/LICENSE-JTransforms.txt\n","spark-3.3.0-bin-hadoop3/licenses/LICENSE-JLargeArrays.txt\n","spark-3.3.0-bin-hadoop3/licenses/LICENSE-bootstrap.txt\n","spark-3.3.0-bin-hadoop3/licenses/LICENSE-reflectasm.txt\n","spark-3.3.0-bin-hadoop3/licenses/LICENSE-javassist.html\n","spark-3.3.0-bin-hadoop3/licenses/LICENSE-zstd.txt\n","spark-3.3.0-bin-hadoop3/licenses/LICENSE-json-formatter.txt\n","spark-3.3.0-bin-hadoop3/licenses/LICENSE-matchMedia-polyfill.txt\n","spark-3.3.0-bin-hadoop3/licenses/LICENSE-scala.txt\n","spark-3.3.0-bin-hadoop3/licenses/LICENSE-jakarta.activation-api.txt\n","spark-3.3.0-bin-hadoop3/licenses/LICENSE-automaton.txt\n","spark-3.3.0-bin-hadoop3/licenses/LICENSE-javax-transaction-transaction-api.txt\n","spark-3.3.0-bin-hadoop3/licenses/LICENSE-jaxb-runtime.txt\n","spark-3.3.0-bin-hadoop3/licenses/LICENSE-minlog.txt\n","spark-3.3.0-bin-hadoop3/licenses/LICENSE-mustache.txt\n","spark-3.3.0-bin-hadoop3/licenses/LICENSE-xmlenc.txt\n","spark-3.3.0-bin-hadoop3/licenses/LICENSE-jline.txt\n","spark-3.3.0-bin-hadoop3/licenses/LICENSE-istack-commons-runtime.txt\n","spark-3.3.0-bin-hadoop3/licenses/LICENSE-py4j.txt\n","spark-3.3.0-bin-hadoop3/licenses/LICENSE-vis-timeline.txt\n","spark-3.3.0-bin-hadoop3/licenses/LICENSE-blas.txt\n","spark-3.3.0-bin-hadoop3/licenses/LICENSE-re2j.txt\n","spark-3.3.0-bin-hadoop3/licenses/LICENSE-kryo.txt\n","spark-3.3.0-bin-hadoop3/licenses/LICENSE-cloudpickle.txt\n"]}],"source":["!apt update\n","!apt-get install openjdk-11-jdk-headless -qq > /dev/null\n","!wget -q http://archive.apache.org/dist/spark/spark-3.3.0/spark-3.3.0-bin-hadoop3.tgz\n","!tar -xvf spark-3.3.0-bin-hadoop3.tgz\n","!pip install -q findspark\n","import os\n","os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-11-openjdk-amd64\"\n","os.environ[\"SPARK_HOME\"] = \"/content/spark-3.3.0-bin-hadoop3\"\n","import findspark\n","findspark.init()"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5003,"status":"ok","timestamp":1684893039026,"user":{"displayName":"Quang Pham","userId":"17287612277936898722"},"user_tz":-420},"id":"voYLidK7i2zZ","outputId":"019948a2-1429-4f86-bf95-90635c73b67c"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/gdrive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/gdrive', force_remount=True)"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TcERzv8Li25x","executionInfo":{"status":"ok","timestamp":1684893039026,"user_tz":-420,"elapsed":4,"user":{"displayName":"Quang Pham","userId":"17287612277936898722"}},"outputId":"9b56eeb0-b050-4c4f-f31f-ae88de62a7cf"},"outputs":[{"output_type":"stream","name":"stdout","text":["/content/gdrive/My Drive/LDS9\n"]}],"source":["%cd '/content/gdrive/My Drive/LDS9/'"]},{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":677,"status":"ok","timestamp":1684893039701,"user":{"displayName":"Quang Pham","userId":"17287612277936898722"},"user_tz":-420},"id":"-n0Jl3nii28Z"},"outputs":[],"source":["import findspark\n","findspark.init()"]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":5,"status":"ok","timestamp":1684893039701,"user":{"displayName":"Quang Pham","userId":"17287612277936898722"},"user_tz":-420},"id":"qKmxyzlAi2-g"},"outputs":[],"source":["import pyspark"]},{"cell_type":"code","execution_count":6,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1684893039702,"user":{"displayName":"Quang Pham","userId":"17287612277936898722"},"user_tz":-420},"id":"sLbMHuOXi3A4"},"outputs":[],"source":["from pyspark.sql import SparkSession"]},{"cell_type":"code","execution_count":7,"metadata":{"executionInfo":{"elapsed":10083,"status":"ok","timestamp":1684893049782,"user":{"displayName":"Quang Pham","userId":"17287612277936898722"},"user_tz":-420},"id":"1BBU0OC0i3DQ"},"outputs":[],"source":["spark = SparkSession.builder.appName('cau2').getOrCreate()"]},{"cell_type":"code","execution_count":8,"metadata":{"executionInfo":{"elapsed":13562,"status":"ok","timestamp":1684893063342,"user":{"displayName":"Quang Pham","userId":"17287612277936898722"},"user_tz":-420},"id":"iB8A2BSzi3Fo"},"outputs":[],"source":["fake = spark.read.csv(\"fake-and-real-news-dataset/Fake.csv\",inferSchema=True, header=True)"]},{"cell_type":"code","execution_count":9,"metadata":{"executionInfo":{"elapsed":13,"status":"ok","timestamp":1684893063342,"user":{"displayName":"Quang Pham","userId":"17287612277936898722"},"user_tz":-420},"id":"wX08Ya7c4i6B"},"outputs":[],"source":["from pyspark.sql.functions import lit\n","fake = fake.withColumn(\"class\",lit('fake'))"]},{"cell_type":"code","execution_count":10,"metadata":{"executionInfo":{"elapsed":1531,"status":"ok","timestamp":1684893064862,"user":{"displayName":"Quang Pham","userId":"17287612277936898722"},"user_tz":-420},"id":"6Ob9V1Fd4DDf"},"outputs":[],"source":["true = spark.read.csv(\"fake-and-real-news-dataset/True.csv\",inferSchema=True, header=True)"]},{"cell_type":"code","execution_count":11,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1684893064863,"user":{"displayName":"Quang Pham","userId":"17287612277936898722"},"user_tz":-420},"id":"lYAIVdW247Kg"},"outputs":[],"source":["from pyspark.sql.functions import lit\n","true = true.withColumn(\"class\",lit('true'))"]},{"cell_type":"code","execution_count":12,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":800,"status":"ok","timestamp":1684893065661,"user":{"displayName":"Quang Pham","userId":"17287612277936898722"},"user_tz":-420},"id":"BETPS3JYj0Ug","outputId":"0c83845b-42dc-46ba-c98a-b9fbcb3b942e"},"outputs":[{"output_type":"stream","name":"stdout","text":["+------------------------------------------------------------------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------------------------------+------------------+-----+\n","|title                                                                         |text                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                |subject                                                                                                       |date              |class|\n","+------------------------------------------------------------------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------------------------------+------------------+-----+\n","|As U.S. budget fight looms, Republicans flip their fiscal script              |WASHINGTON (Reuters) - The head of a conservative Republican faction in the U.S. Congress, who voted this month for a huge expansion of the national debt to pay for tax cuts, called himself a “fiscal conservative” on Sunday and urged budget restraint in 2018. In keeping with a sharp pivot under way among Republicans, U.S. Representative Mark Meadows, speaking on CBS’ “Face the Nation,” drew a hard line on federal spending, which lawmakers are bracing to do battle over in January. When they return from the holidays on Wednesday, lawmakers will begin trying to pass a federal budget in a fight likely to be linked to other issues, such as immigration policy, even as the November congressional election campaigns approach in which Republicans will seek to keep control of Congress. President Donald Trump and his Republicans want a big budget increase in military spending, while Democrats also want proportional increases for non-defense “discretionary” spending on programs that support education, scientific research, infrastructure, public health and environmental protection. “The (Trump) administration has already been willing to say: ‘We’re going to increase non-defense discretionary spending ... by about 7 percent,’” Meadows, chairman of the small but influential House Freedom Caucus, said on the program. “Now, Democrats are saying that’s not enough, we need to give the government a pay raise of 10 to 11 percent. For a fiscal conservative, I don’t see where the rationale is. ... Eventually you run out of other people’s money,” he said. Meadows was among Republicans who voted in late December for their party’s debt-financed tax overhaul, which is expected to balloon the federal budget deficit and add about $1.5 trillion over 10 years to the $20 trillion national debt. “It’s interesting to hear Mark talk about fiscal responsibility,” Democratic U.S. Representative Joseph Crowley said on CBS. Crowley said the Republican tax bill would require the  United States to borrow $1.5 trillion, to be paid off by future generations, to finance tax cuts for corporations and the rich. “This is one of the least ... fiscally responsible bills we’ve ever seen passed in the history of the House of Representatives. I think we’re going to be paying for this for many, many years to come,” Crowley said. Republicans insist the tax package, the biggest U.S. tax overhaul in more than 30 years,  will boost the economy and job growth. House Speaker Paul Ryan, who also supported the tax bill, recently went further than Meadows, making clear in a radio interview that welfare or “entitlement reform,” as the party often calls it, would be a top Republican priority in 2018. In Republican parlance, “entitlement” programs mean food stamps, housing assistance, Medicare and Medicaid health insurance for the elderly, poor and disabled, as well as other programs created by Washington to assist the needy. Democrats seized on Ryan’s early December remarks, saying they showed Republicans would try to pay for their tax overhaul by seeking spending cuts for social programs. But the goals of House Republicans may have to take a back seat to the Senate, where the votes of some Democrats will be needed to approve a budget and prevent a government shutdown. Democrats will use their leverage in the Senate, which Republicans narrowly control, to defend both discretionary non-defense programs and social spending, while tackling the issue of the “Dreamers,” people brought illegally to the country as children. Trump in September put a March 2018 expiration date on the Deferred Action for Childhood Arrivals, or DACA, program, which protects the young immigrants from deportation and provides them with work permits. The president has said in recent Twitter messages he wants funding for his proposed Mexican border wall and other immigration law changes in exchange for agreeing to help the Dreamers. Representative Debbie Dingell told CBS she did not favor linking that issue to other policy objectives, such as wall funding. “We need to do DACA clean,” she said.  On Wednesday, Trump aides will meet with congressional leaders to discuss those issues. That will be followed by a weekend of strategy sessions for Trump and Republican leaders on Jan. 6 and 7, the White House said. Trump was also scheduled to meet on Sunday with Florida Republican Governor Rick Scott, who wants more emergency aid. The House has passed an $81 billion aid package after hurricanes in Florida, Texas and Puerto Rico, and wildfires in California. The package far exceeded the $44 billion requested by the Trump administration. The Senate has not yet voted on the aid.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  |politicsNews                                                                                                  |December 31, 2017 |true |\n","|U.S. military to accept transgender recruits on Monday: Pentagon              |WASHINGTON (Reuters) - Transgender people will be allowed for the first time to enlist in the U.S. military starting on Monday as ordered by federal courts, the Pentagon said on Friday, after President Donald Trump’s administration decided not to appeal rulings that blocked his transgender ban. Two federal appeals courts, one in Washington and one in Virginia, last week rejected the administration’s request to put on hold orders by lower court judges requiring the military to begin accepting transgender recruits on Jan. 1. A Justice Department official said the administration will not challenge those rulings. “The Department of Defense has announced that it will be releasing an independent study of these issues in the coming weeks. So rather than litigate this interim appeal before that occurs, the administration has decided to wait for DOD’s study and will continue to defend the president’s lawful authority in District Court in the meantime,” the official said, speaking on condition of anonymity. In September, the Pentagon said it had created a panel of senior officials to study how to implement a directive by Trump to prohibit transgender individuals from serving. The Defense Department has until Feb. 21 to submit a plan to Trump. Lawyers representing currently-serving transgender service members and aspiring recruits said they had expected the administration to appeal the rulings to the conservative-majority Supreme Court, but were hoping that would not happen. Pentagon spokeswoman Heather Babb said in a statement: “As mandated by court order, the Department of Defense is prepared to begin accessing transgender applicants for military service Jan. 1. All applicants must meet all accession standards.” Jennifer Levi, a lawyer with gay, lesbian and transgender advocacy group GLAD, called the decision not to appeal “great news.” “I’m hoping it means the government has come to see that there is no way to justify a ban and that it’s not good for the military or our country,” Levi said. Both GLAD and the American Civil Liberties Union represent plaintiffs in the lawsuits filed against the administration. In a move that appealed to his hard-line conservative supporters, Trump announced in July that he would prohibit transgender people from serving in the military, reversing Democratic President Barack Obama’s policy of accepting them. Trump said on Twitter at the time that the military “cannot be burdened with the tremendous medical costs and disruption that transgender in the military would entail.” Four federal judges - in Baltimore, Washington, D.C., Seattle and Riverside, California - have issued rulings blocking Trump’s ban while legal challenges to the Republican president’s policy proceed. The judges said the ban would likely violate the right under the U.S. Constitution to equal protection under the law. The Pentagon on Dec. 8 issued guidelines to recruitment personnel in order to enlist transgender applicants by Jan. 1. The memo outlined medical requirements and specified how the applicants’ sex would be identified and even which undergarments they would wear. The Trump administration previously said in legal papers that the armed forces were not prepared to train thousands of personnel on the medical standards needed to process transgender applicants and might have to accept “some individuals who are not medically fit for service.” The Obama administration had set a deadline of July 1, 2017, to begin accepting transgender recruits. But Trump’s defense secretary, James Mattis, postponed that date to Jan. 1, 2018, which the president’s ban then put off indefinitely. Trump has taken other steps aimed at rolling back transgender rights. In October, his administration said a federal law banning gender-based workplace discrimination does not protect transgender employees, reversing another Obama-era position. In February, Trump rescinded guidance issued by the Obama administration saying that public schools should allow transgender students to use the restroom that corresponds to their gender identity.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        |politicsNews                                                                                                  |December 29, 2017 |true |\n","|Senior U.S. Republican senator: 'Let Mr. Mueller do his job'                  |WASHINGTON (Reuters) - The special counsel investigation of links between Russia and President Trump’s 2016 election campaign should continue without interference in 2018, despite calls from some Trump administration allies and Republican lawmakers to shut it down, a prominent Republican senator said on Sunday. Lindsey Graham, who serves on the Senate armed forces and judiciary committees, said Department of Justice Special Counsel Robert Mueller needs to carry on with his Russia investigation without political interference. “This investigation will go forward. It will be an investigation conducted without political influence,” Graham said on CBS’s Face the Nation news program. “And we all need to let Mr. Mueller do his job. I think he’s the right guy at the right time.”  The question of how Russia may have interfered in the election, and how Trump’s campaign may have had links with or co-ordinated any such effort, has loomed over the White House since Trump took office in January. It shows no sign of receding as Trump prepares for his second year in power, despite intensified rhetoric from some Trump allies in recent weeks accusing Mueller’s team of bias against the Republican president. Trump himself seemed to undercut his supporters in an interview last week with the New York Times in which he said he expected Mueller was “going to be fair.”    Russia’s role in the election and the question of possible links to the Trump campaign are the focus of multiple inquiries in Washington. Three committees of the Senate and the House of Representatives are investigating, as well as Mueller, whose team in May took over an earlier probe launched by the U.S. Federal Bureau of Investigation (FBI). Several members of the Trump campaign and administration have been convicted or indicted in the investigation.  Trump and his allies deny any collusion with Russia during the campaign, and the Kremlin has denied meddling in the election. Graham said he still wants an examination of the FBI’s use of a dossier on links between Trump and Russia that was compiled by a former British spy, Christopher Steele, which prompted Trump allies and some Republicans to question Mueller’s inquiry.   On Saturday, the New York Times reported that it was not that dossier that triggered an early FBI probe, but a tip from former Trump campaign foreign policy adviser George Papadopoulos to an Australian diplomat that Russia had damaging information about former Trump rival Hillary Clinton.  “I want somebody to look at the way the Department of Justice used this dossier. It bothers me greatly the way they used it, and I want somebody to look at it,” Graham said. But he said the Russia investigation must continue. “As a matter of fact, it would hurt us if we ignored it,” he said.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                |politicsNews                                                                                                  |December 31, 2017 |true |\n","|FBI Russia probe helped by Australian diplomat tip-off: NYT                   |WASHINGTON (Reuters) - Trump campaign adviser George Papadopoulos told an Australian diplomat in May 2016 that Russia had political dirt on Democratic presidential candidate Hillary Clinton, the New York Times reported on Saturday. The conversation between Papadopoulos and the diplomat, Alexander Downer, in London was a driving factor behind the FBI’s decision to open a counter-intelligence investigation of Moscow’s contacts with the Trump campaign, the Times reported. Two months after the meeting, Australian officials passed the information that came from Papadopoulos to their American counterparts when leaked Democratic emails began appearing online, according to the newspaper, which cited four current and former U.S. and foreign officials. Besides the information from the Australians, the probe by the Federal Bureau of Investigation was also propelled by intelligence from other friendly governments, including the British and Dutch, the Times said. Papadopoulos, a Chicago-based international energy lawyer, pleaded guilty on Oct. 30 to lying to FBI agents about contacts with people who claimed to have ties to top Russian officials. It was the first criminal charge alleging links between the Trump campaign and Russia. The White House has played down the former aide’s campaign role, saying it was “extremely limited” and that any actions he took would have been on his own. The New York Times, however, reported that Papadopoulos helped set up a meeting between then-candidate Donald Trump and Egyptian President Abdel Fattah al-Sisi and edited the outline of Trump’s first major foreign policy speech in April 2016. The federal investigation, which is now being led by Special Counsel Robert Mueller, has hung over Trump’s White House since he took office almost a year ago. Some Trump allies have recently accused Mueller’s team of being biased against the Republican president. Lawyers for Papadopoulos did not immediately respond to requests by Reuters for comment. Mueller’s office declined to comment. Trump’s White House attorney, Ty Cobb, declined to comment on the New York Times report. “Out of respect for the special counsel and his process, we are not commenting on matters such as this,” he said in a statement. Mueller has charged four Trump associates, including Papadopoulos, in his investigation. Russia has denied interfering in the U.S. election and Trump has said there was no collusion between his campaign and Moscow.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        |politicsNews                                                                                                  |December 30, 2017 |true |\n","|Trump wants Postal Service to charge 'much more' for Amazon shipments         |SEATTLE/WASHINGTON (Reuters) - President Donald Trump called on the U.S. Postal Service on Friday to charge “much more” to ship packages for Amazon (AMZN.O), picking another fight with an online retail giant he has criticized in the past.     “Why is the United States Post Office, which is losing many billions of dollars a year, while charging Amazon and others so little to deliver their packages, making Amazon richer and the Post Office dumber and poorer? Should be charging MUCH MORE!” Trump wrote on Twitter.  The president’s tweet drew fresh attention to the fragile finances of the Postal Service at a time when tens of millions of parcels have just been shipped all over the country for the holiday season.  The U.S. Postal Service, which runs at a big loss, is an independent agency within the federal government and does not receive tax dollars for operating expenses, according to its website.  Package delivery has become an increasingly important part of its business as the Internet has led to a sharp decline in the amount of first-class letters. The president does not determine postal rates. They are set by the Postal Regulatory Commission, an independent government agency with commissioners selected by the president from both political parties. That panel raised prices on packages by almost 2 percent in November.  Amazon was founded by Jeff Bezos, who remains the chief executive officer of the retail company and is the richest person in the world, according to Bloomberg News. Bezos also owns The Washington Post, a newspaper Trump has repeatedly railed against in his criticisms of the news media. In tweets over the past year, Trump has said the “Amazon Washington Post” fabricated stories. He has said Amazon does not pay sales tax, which is not true, and so hurts other retailers, part of a pattern by the former businessman and reality television host of periodically turning his ire on big American companies since he took office in January. Daniel Ives, a research analyst at GBH Insights, said Trump’s comment could be taken as a warning to the retail giant. However, he said he was not concerned for Amazon. “We do not see any price hikes in the future. However, that is a risk that Amazon is clearly aware of and (it) is building out its distribution (system) aggressively,” he said. Amazon has shown interest in the past in shifting into its own delivery service, including testing drones for deliveries. In 2015, the company spent $11.5 billion on shipping, 46 percent of its total operating expenses that year.  Amazon shares were down 0.86 percent to $1,175.90 by early afternoon. Overall, U.S. stock prices were down slightly on Friday.  Satish Jindel, president of ShipMatrix Inc, which analyzes shipping data, disputed the idea that the Postal Service charges less than United Parcel Service Inc (UPS.N) and FedEx Corp (FDX.N), the other biggest players in the parcel delivery business in the United States. Many customers get lower rates from UPS and FedEx than they would get from the post office for comparable services, he said. The Postal Service delivers about 62 percent of Amazon packages, for about 3.5 to 4 million a day during the current peak year-end holiday shipping season, Jindel said. The Seattle-based company and the post office have an agreement in which mail carriers take Amazon packages on the last leg of their journeys, from post offices to customers’ doorsteps. Amazon’s No. 2 carrier is UPS, at 21 percent, and FedEx is third, with 8 percent or so, according to Jindel. Trump’s comment tapped into a debate over whether Postal Service pricing has kept pace with the rise of e-commerce, which has flooded the mail with small packages.Private companies like UPS have long claimed the current system unfairly undercuts their business. Steve Gaut, a spokesman for UPS, noted that the company values its “productive relationship” with the postal service, but that it has filed with the Postal Regulatory Commission its concerns about the postal service’s methods for covering costs. Representatives for Amazon, the White House, the U.S. Postal Service and FedEx declined comment or were not immediately available for comment on Trump’s tweet. According to its annual report, the Postal Service lost $2.74 billion this year, and its deficit has ballooned to $61.86 billion.  While the Postal Service’s revenue for first class mail, marketing mail and periodicals is flat or declining, revenue from package delivery is up 44 percent since 2014 to $19.5 billion in the fiscal year ended Sept. 30, 2017. But it also lost about $2 billion in revenue when a temporary surcharge expired in April 2016. According to a Government Accountability Office report in February, the service is facing growing personnel expenses, particularly $73.4 billion in unfunded pension and benefits liabilities. The Postal Service has not announced any plans to cut costs. By law, the Postal Service has to set prices for package delivery to cover the costs attributable to that service. But the postal service allocates only 5.5 percent of its total costs to its business of shipping packages even though that line of business is 28 percent of its total revenue. |politicsNews                                                                                                  |December 29, 2017 |true |\n","|White House, Congress prepare for talks on spending, immigration              |WEST PALM BEACH, Fla./WASHINGTON (Reuters) - The White House said on Friday it was set to kick off talks next week with Republican and Democratic congressional leaders on immigration policy, government spending and other issues that need to be wrapped up early in the new year. The expected flurry of legislative activity comes as Republicans and Democrats begin to set the stage for midterm congressional elections in November. President Donald Trump’s Republican Party is eager to maintain control of Congress while Democrats look for openings to wrest seats away in the Senate and the House of Representatives. On Wednesday, Trump’s budget chief Mick Mulvaney and legislative affairs director Marc Short will meet with Senate Majority Leader Mitch McConnell and House Speaker Paul Ryan - both Republicans - and their Democratic counterparts, Senator Chuck Schumer and Representative Nancy Pelosi, the White House said. That will be followed up with a weekend of strategy sessions for Trump, McConnell and Ryan on Jan. 6 and 7 at the Camp David presidential retreat in Maryland, according to the White House. The Senate returns to work on Jan. 3 and the House on Jan. 8. Congress passed a short-term government funding bill last week before taking its Christmas break, but needs to come to an agreement on defense spending and various domestic programs by Jan. 19, or the government will shut down. Also on the agenda for lawmakers is disaster aid for people hit by hurricanes in Puerto Rico, Texas and Florida, and by wildfires in California. The House passed an $81 billion package in December, which the Senate did not take up. The White House has asked for a smaller figure, $44 billion. Deadlines also loom for soon-to-expire protections for young adult immigrants who entered the country illegally as children, known as “Dreamers.” In September, Trump ended Democratic former President Barack Obama’s Deferred Action for Childhood Arrivals (DACA) program, which protected Dreamers from deportation and provided work permits, effective in March, giving Congress until then to devise a long-term solution. Democrats, some Republicans and a number of large companies have pushed for DACA protections to continue. Trump and other Republicans have said that will not happen without Congress approving broader immigration policy changes and tougher border security. Democrats oppose funding for a wall promised by Trump along the U.S.-Mexican border.  “The Democrats have been told, and fully understand, that there can be no DACA without the desperately needed WALL at the Southern Border and an END to the horrible Chain Migration & ridiculous Lottery System of Immigration etc,” Trump said in a Twitter post on Friday. Trump wants to overhaul immigration rules for extended families and others seeking to live in the United States. Republican U.S. Senator Jeff Flake, a frequent critic of the president, said he would work with Trump to protect Dreamers. “We can fix DACA in a way that beefs up border security, stops chain migration for the DREAMers, and addresses the unfairness of the diversity lottery. If POTUS (Trump) wants to protect these kids, we want to help him keep that promise,” Flake wrote on Twitter. Congress in early 2018 also must raise the U.S. debt ceiling to avoid a government default. The U.S. Treasury would exhaust all of its borrowing options and run dry of cash to pay its bills by late March or early April if Congress does not raise the debt ceiling before then, according to the nonpartisan Congressional Budget Office. Trump, who won his first major legislative victory with the passage of a major tax overhaul this month, has also promised a major infrastructure plan.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          |politicsNews                                                                                                  |December 29, 2017 |true |\n","|Trump says Russia probe will be fair, but timeline unclear: NYT               |WEST PALM BEACH, Fla (Reuters) - President Donald Trump said on Thursday he believes he will be fairly treated in a special counsel investigation into Russian meddling in the U.S. presidential election, but said he did not know how long the probe would last. The federal investigation has hung over Trump’s White House since he took office almost a year ago, and some Trump allies have in recent weeks accused the team of Justice Department Special Counsel Robert Mueller of being biased against the Republican president. But in an interview with the New York Times, Trump appeared to shrug off concerns about the investigation, which was prompted by U.S. intelligence agencies’ conclusion that Russia tried to help Trump defeat Democrat Hillary Clinton by hacking and releasing embarrassing emails and disseminating propaganda. “There’s been no collusion. But I think he’s going to be fair,” Trump said in what the Times described as a 30-minute impromptu interview at his golf club in West Palm Beach, Florida. Mueller has charged four Trump associates in his investigation. Russia has denied interfering in the U.S. election. U.S. Deputy Attorney General Rod Rosenstein said this month that he was not aware of any impropriety by Mueller’s team. Trump’s lawyers have been saying for weeks that they had expected the Mueller investigation to wrap up quickly, possibly by the end of 2017. Mueller has not commented on how long it will last. Trump told the Times that he did not know how long the investigation would take. “Timing-wise, I can’t tell you. I just don’t know,” he said. Trump said he thought a prolonged probe “makes the country look bad” but said it has energized his core supporters. “What it’s done is, it’s really angered the base and made the base stronger. My base is strong than it’s ever been,” he said. The interview was a rare break in Trump’s Christmas vacation in Florida. He has golfed each day aside from Christmas Day, and mainly kept a low profile, apart from the occasional flurry of tweets. He spent one day golfing with Republican Senator David Perdue from Georgia, who has pushed legislation to cap immigration numbers, and had dinner on Thursday with Commerce Secretary Wilbur Ross, an international trade hawk. Trump told the Times he hoped to work with Democrats in the U.S. Congress on a spending plan to fix roads and other infrastructure, and on protections for a group of undocumented immigrants who were brought to the United States as children. Trump spoke about trade issues, saying he had backed off his hard line on Chinese trade practices in the hope that Beijing would do more to pressure North Korea to end its nuclear and missile testing program. He said he had been disappointed in the results. He also complained about the North American Free Trade Agreement (NAFTA), which his administration is attempting to renegotiate in talks with Mexico and Canada. Trump said Canadian Prime Minister Justin Trudeau had played down the importance of Canadian oil and lumber exports to the United States when looking at the balance of trade between the two countries. “If I don’t make the right deal, I’ll terminate NAFTA in two seconds. But we’re doing pretty good,” Trump said.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 |politicsNews                                                                                                  |December 29, 2017 |true |\n","|Factbox: Trump on Twitter (Dec 29) - Approval rating, Amazon                  |The following statements were posted to the verified Twitter accounts of U.S. President Donald Trump, @realDonaldTrump and @POTUS.  The opinions expressed are his own. Reuters has not edited the statements or confirmed their accuracy.  @realDonaldTrump : - While the Fake News loves to talk about my so-called low approval rating, @foxandfriends just showed that my rating on Dec. 28, 2017, was approximately the same as President Obama on Dec. 28, 2009, which was 47%...and this despite massive negative Trump coverage & Russia hoax! [0746 EST] - Why is the United States Post Office, which is losing many billions of dollars a year, while charging Amazon and others so little to deliver their packages, making Amazon richer and the Post Office dumber and poorer? Should be charging MUCH MORE! [0804 EST] -- Source link: (bit.ly/2jBh4LU) (bit.ly/2jpEXYR)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             |politicsNews                                                                                                  |December 29, 2017 |true |\n","|Trump on Twitter (Dec 28) - Global Warming                                    |The following statements were posted to the verified Twitter accounts of U.S. President Donald Trump, @realDonaldTrump and @POTUS.  The opinions expressed are his own. Reuters has not edited the statements or confirmed their accuracy.  @realDonaldTrump : - Together, we are MAKING AMERICA GREAT AGAIN! bit.ly/2lnpKaq [1814 EST] - In the East, it could be the COLDEST New Year’s Eve on record. Perhaps we could use a little bit of that good old Global Warming that our Country, but not other countries, was going to pay TRILLIONS OF DOLLARS to protect against. Bundle up! [1901 EST] -- Source link: (bit.ly/2jBh4LU) (bit.ly/2jpEXYR)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             |politicsNews                                                                                                  |December 29, 2017 |true |\n","|Alabama official to certify Senator-elect Jones today despite challenge: CNN  |WASHINGTON (Reuters) - Alabama Secretary of State John Merrill said he will certify Democratic Senator-elect Doug Jones as winner on Thursday despite opponent Roy Moore’s challenge, in a phone call on CNN. Moore, a conservative who had faced allegations of groping teenage girls when he was in his 30s, filed a court challenge late on Wednesday to the outcome of a U.S. Senate election he unexpectedly lost.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             |politicsNews                                                                                                  |December 28, 2017 |true |\n","|Jones certified U.S. Senate winner despite Moore challenge                    |(Reuters) - Alabama officials on Thursday certified Democrat Doug Jones the winner of the state’s U.S. Senate race, after a state judge denied a challenge by Republican Roy Moore, whose campaign was derailed by accusations of sexual misconduct with teenage girls. Jones won the vacant seat by about 22,000 votes, or 1.6 percentage points, election officials said. That made him the first Democrat in a quarter of a century to win a Senate seat in Alabama.  The seat was previously held by Republican Jeff Sessions, who was tapped by U.S. President Donald Trump as attorney general. A state canvassing board composed of Alabama Secretary of State John Merrill, Governor Kay Ivey and Attorney General Steve Marshall certified the election results. Seating Jones will narrow the Republican majority in the Senate to 51 of 100 seats. In a statement, Jones called his victory “a new chapter” and pledged to work with both parties. Moore declined to concede defeat even after Trump urged him to do so. He stood by claims of a fraudulent election in a statement released after the certification and said he had no regrets, media outlets reported. An Alabama judge denied Moore’s request to block certification of the results of the Dec. 12 election in a decision shortly before the canvassing board met. Moore’s challenge alleged there had been potential voter fraud that denied him a chance of victory. His filing on Wednesday in the Montgomery Circuit Court sought to halt the meeting scheduled to ratify Jones’ win on Thursday. Moore could ask for a recount, in addition to possible other court challenges, Merrill said in an interview with Fox News Channel. He would have to complete paperwork “within a timed period” and show he has the money for a challenge, Merrill said. “We’ve not been notified yet of their intention to do that,” Merrill said. Regarding the claim of voter fraud, Merrill told CNN that more than 100 cases had been reported. “We’ve adjudicated more than 60 of those. We will continue to do that,” he said.  Republican lawmakers in Washington had distanced themselves from Moore and called for him to drop out of the race after several women accused him of sexual assault or misconduct dating back to when they were teenagers and he was in his early 30s.  Moore has denied wrongdoing and Reuters has not been able to independently verify the allegations.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                |politicsNews                                                                                                  |December 28, 2017 |true |\n","|New York governor questions the constitutionality of federal tax overhaul     |NEW YORK/WASHINGTON (Reuters) - The new U.S. tax code targets high-tax states and may be unconstitutional, New York Governor Andrew Cuomo said on Thursday, saying that the bill may violate New York residents’ rights to due process and equal protection.  The sweeping Republican tax bill signed into law by U.S. President Donald Trump on Friday introduces a cap, of $10,000,  on deductions of state and local income and property taxes, known as SALT. The tax overhaul was the party’s first major legislative victory since Trump took office in January.  The SALT provision will hit many taxpayers in states with high incomes, high property values and high taxes, like New York, New Jersey and California. Those states are generally Democratic leaning.  “I’m not even sure what they did is legally constitutional and that’s something we’re looking at now,” Cuomo said in an interview with CNN. In an interview with CNBC, Cuomo suggested why the bill may be unconstitutional.  “Politics does not trump the law,” Cuomo said on CNBC. “You have the constitution, you have the law, you have due process, you have equal protection. You can’t use politics just because the majority controls to override the law.” The Fifth Amendment of the Constitution, better known for its protection against self-incrimination, also protects individuals from seizure of life, liberty or property without due process and has been interpreted by the Supreme Court as guaranteeing equal protection by the law.  Cuomo and California Governor Jerry Brown, both Democrats, have previously said they were exploring legal challenges to SALT deduction limits.  Law professors have said legal challenges would likely rest on arguing that the provision interferes with the protection of states’ rights under the U.S. Constitution’s 10th Amendment. Tax attorneys said Cuomo’s legal argument against the tax bill could be that it discriminates and places an unjust tax burden on states that heavily voted for Democrats in the past - known as “blue states.” “The de facto effect of this legislation is to discriminate against blue states and particularly from (Cuomo’s) perspective the state of New York,” said Joseph Callahan, an attorney with the law firm Mackay, Caswell & Callahan in New York.  But some tax experts noted the U.S. Supreme Court has interpreted the 16th Amendment to give Congress broad latitude to tax as it sees fit. In a frequently cited 1934 decision, the Supreme Court called tax deductions a “legislative grace” rather than a vested right. “I don’t understand how they think they have a valid lawsuit here,” David Gamage, a professor of tax law at Indiana University’s Maurer School of Law, told Reuters last week, speaking generally about governors in blue states that could challenge the tax bill.  Cuomo also said on Thursday that New York is proposing a restructuring of its tax code. He provided no details.  A group of 13 law professors on Dec. 18 published a paper suggesting ways that high-tax states could minimize the effects of the SALT deduction cap.  Their suggestions included shifting more of the tax burden onto businesses in the form of higher employer-side payroll taxes, since the federal tax bill’s cap on SALT deductions only applies to individuals and not businesses. States also could raise taxes on pass-through entities, which the federal tax bill specifically benefits with a lower rate on a portion of their income. On Friday, Cuomo said he would allow state residents to make a partial or full pre-payment on their property tax bill before Jan. 1, allowing taxpapyers to deduct such payments for 2017 before the cap kicks in, prompting a wave of residents to pay early.  However, the U.S. Internal Revenue Service on Wednesday advised homeowners that the pre-payment of 2018 property taxes may not be deductible.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                |politicsNews                                                                                                  |December 28, 2017 |true |\n","|Factbox: Trump on Twitter (Dec 28) - Vanity Fair, Hillary Clinton             |The following statements were posted to the verified Twitter accounts of U.S. President Donald Trump, @realDonaldTrump and @POTUS.  The opinions expressed are his own. Reuters has not edited the statements or confirmed their accuracy.  @realDonaldTrump : - Vanity Fair, which looks like it is on its last legs, is bending over backwards in apologizing for the minor hit they took at Crooked H. Anna Wintour, who was all set to be Amb to Court of St James’s & a big fundraiser for CH, is beside herself in grief & begging for forgiveness! [1024 EST] -- Source link: (bit.ly/2jBh4LU) (bit.ly/2jpEXYR)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              |politicsNews                                                                                                  |December 28, 2017 |true |\n","|Trump on Twitter (Dec 27) - Trump, Iraq, Syria                                |\"The following statements were posted to the verified Twitter accounts of U.S. President Donald Trump, @realDonaldTrump and @POTUS.  The opinions expressed are his own. Reuters has not edited the statements or confirmed their accuracy.  @realDonaldTrump : - “On 1/20 - the day Trump was inaugurated - an estimated 35,000 ISIS fighters held approx 17,500 square miles of territory in both Iraq and Syria. As of 12/21, the U.S. military estimates the remaining 1,000 or so fighters occupy roughly 1,900 square miles...” via @jamiejmcintyre  [1749 EST] - Just left West Palm Beach Fire & Rescue #2. Met with great men and women as representatives of those who do so much for all of us. Firefighters, paramedics, first responders - what amazing people they are! [1811 EST] - “On 1/20 - the day Trump was inaugurated - an estimated 35,000 ISIS fighters held approx 17,500 square miles of territory in both Iraq and Syria. As of 12/21, the U.S. military est the remaining 1,000 or so fighters occupy roughly 1,900 square miles..” @jamiejmcintyre @dcexaminer [2109 EST] - \"\"Arrests of MS-13 Members                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 | Associates Up 83% Under Trump\"\" bit.ly/2liRH3b [2146 EST] -- Source link: (bit.ly/2jBh4LU) (bit.ly/2jpEXYR) \"|politicsNews      |true |\n","|Man says he delivered manure to Mnuchin to protest new U.S. tax law           | (In Dec. 25 story, in second paragraph, corrects name of Strong’s employer to Mental Health Department, not Public Health Department.) By Bernie Woodall (Reuters) - A man claiming to be the person who delivered a gift-wrapped package of horse manure at the Los Angeles home of U.S. Treasury Secretary Steven Mnuchin said on Monday he did it to protest the federal tax overhaul signed into law last week by President Donald Trump. Robert Strong, 45, a psychologist for the Los Angeles County Mental Health Department, said by telephone he left the poop-filled parcel addressed to Mnuchin and Trump in the driveway outside Mnuchin’s home in the posh Bel Air community.  KNBC-TV, an NBC television affiliate in Los Angeles, reported Mnuchin was not home at the time. The package was found by Mnuchin’s neighbor.    “Protest really should be funny,” Strong told Reuters. “People’s eyes glaze over when they just see angry people in the streets.” He believes the new tax law will hurt poor people. Neither the U.S. Secret Service nor the Los Angeles Police Department, both of which investigated the incident, would confirm Strong was responsible. The Secret Service interviewed an individual who admitted delivering the package, but no charges had been filed against him as of Monday afternoon. LAPD Lieutenant Rob Weise said it was possible whoever left the package did not break any criminal laws. While he is not assigned to investigate the incident, Weise said if the box did not present any danger, it would not be illegal. The LAPD bomb squad X-rayed the box before opening it on Saturday. In a photo of the card Strong posted on Twitter, he wrote “Misters Mnuchin & Trump, We’re returning the ‘gift’ of the Christmas tax bill” and signed it “Warmest wishes, The American people.”      Strong said a Secret Service agent, accompanied by six police officers, showed up at his house to question him on Sunday night, and the agent chided him, asking, “‘Are you ashamed of your behavior?’”     The White House declined to comment on Monday and officials with the Treasury Department could not be reached.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             |politicsNews                                                                                                  |December 25, 2017 |true |\n","|Virginia officials postpone lottery drawing to decide tied statehouse election|(Reuters) - A lottery drawing to settle a tied Virginia legislative race that could shift the statehouse balance of power has been indefinitely postponed, state election officials said on Tuesday, after the Democratic candidate mounted a legal fight. The decision to put off the high-stakes lotto, originally scheduled for Wednesday, marks the latest twist in a dramatic election recount that at one point showed Democrat Shelly Simonds beating Republican incumbent David Yancey by a single vote.  A victory by Simonds would shift Republicans’ slim control of the 100-member House of Delegates to an even 50-50 split with the Democrats, forcing the two parties into a rare power-sharing arrangement. A day after Simonds emerged as the victor of a recount, a three-judge panel ruled that a disputed ballot should be counted for Yancey. That decision left the two candidates tied with 11,608 votes each in a district that encompasses the shipping hub of Newport News in southeastern Virginia, setting the stage for the equivalent of a coin toss to pick a final winner. Simonds asked a state court to reconsider on Tuesday, arguing that the disputed ballot was wrongly included. An image filed in court showed that the ballot had bubbles filled in beside both names, with a slash mark by Simonds’ name. The voter selected Republicans for other offices. Simonds told reporters that the case had implications not only for her contest but for the integrity of state elections as a whole, saying that without a court ruling in her favor, “recounts would become a never-ending spiral of courtroom challenges.” The chairman of the Virginia Board of Elections, James Alcorn, said in a statement that while holding a lottery would be in keeping with state law, such a move should be considered “an action of last resort.” He added: “Any substantive concerns regarding the election or recount should be resolved before a random drawing is conducted.”    Yancey’s campaign did not immediately respond to requests for comment. The Virginia House Republican Caucus said in a statement that it was reviewing the new court filings. “We believe the court acted appropriately and that the integrity of the process is without question,” spokesman Parker Slaybaugh said. Virginia Department of Elections spokeswoman Andrea Gaines said in an email that no new date for a drawing has been set. Democrats notched historic gains in Virginia’s statehouse elections last month, part of the party’s first big wave of political victories since Republican Donald Trump won the White House last year. Before the Nov. 7 general election, Virginia Republicans held 66 seats to the Democrats’ 34 in the House of Delegates, along with a majority in the state Senate.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         |politicsNews                                                                                                  |December 27, 2017 |true |\n","|U.S. lawmakers question businessman at 2016 Trump Tower meeting: sources      |WASHINGTON (Reuters) - A Georgian-American businessman who met then-Miss Universe pageant owner Donald Trump in 2013, has been questioned by congressional investigators about whether he helped organize a meeting between Russians and Trump’s eldest son during the 2016 election campaign, four sources familiar with the matter said. The meeting at Trump Tower in New York involving Donald Trump Jr. and other campaign advisers is a focus of probes by Congress and Special Counsel Robert Mueller on whether campaign officials colluded with Russia when it sought to interfere in the U.S. election, the sources said. Russia denies allegations by U.S. intelligence agencies that it meddled in the election and President Donald Trump denies any collusion. The Senate and House of Representatives intelligence committees recently questioned behind closed doors Irakly Kaveladze, a U.S. citizen born in the former Soviet republic of Georgia, the sources said. He is a U.S.-based representative of Azerbaijani oligarch Aras Agalarov’s real estate firm, the Crocus Group. The panels knew Kaveladze was at the June 9, 2016 meeting but became more interested in him after learning he also attended a private dinner in Las Vegas in 2013 with Trump and Agalarov as they celebrated an agreement to hold that year’s Miss Universe pageant in Moscow, the sources said.  Committee members now want to know more about the extent of Kaveladze’s contacts with the Trump family and whether he had a bigger role than previously believed in setting up the Trump Tower meeting when Trump was a Republican candidate for president. The White House declined to comment. Mueller’s office also declined to comment. Scott Balber, a New York lawyer who represents Kaveladze, confirmed that his client attended both the dinner in Las Vegas and the Trump Tower meeting but said he did not set up the second meeting. Trump’s son-in-law Jared Kushner, other Trump campaign aides, and Russian lawyer Natalia Veselnitskaya were also at that meeting. Lawyer Balber also said the committees were only seeking Kaveladze’s input as a witness and were not targeting him for investigation. “No-one has ever told me that they have any interest in him other than as a witness,” Balber said. Lawyers for Trump Jr. and Kushner did not respond to requests for comment about their contacts with Kaveladze. A lawyer for President Trump declined to comment. One photograph from the 2013 dinner, when Trump still owned the Miss Universe pageant, shows Agalarov and his pop singer son Emin along with Trump, two Trump aides and several other people at the dining table. Another shows Kaveladze standing behind Trump and Emin Agalarov as they speak. The pictures were found by a University of California at Irvine student and blogger Scott Stedman, who posted them on Nov. 22. Aras Agalarov is a billionaire property developer in Russia who was awarded the Order of Honor by Russian President Vladimir Putin. Several U.S. officials who spoke on condition of anonymity said Mueller’s team and the committees are looking for any evidence of a link between the Trump Tower meeting and the release six weeks later of emails stolen from Democratic Party organizations. They are also trying to determine whether there was any discussion at the New York meeting of lifting U.S. economic sanctions on Russia, a top priority for Putin, the officials said. Rob Goldstone, a British publicist, told Trump Jr. ahead of the New York meeting that Russian lawyer Veselnitskaya would be bringing damaging information about donations to a charity linked to Trump’s Democratic rival Hillary Clinton, according to emails later released by Trump Jr. Trump Jr. initially said the meeting was about Russian adoptions but later said it also included Veselnitskaya’s promises of information on the donations to the Clinton charity. He said he ultimately never received the information, although it was later posted on the Internet. In a statement issued after meeting with the Senate Judiciary Committee on Sept. 7, Trump, Jr. said Goldstone and Veselnitskaya were in a conference room with him as well as Kaveladze and a translator. Balber said Kaveladze attended expecting to serve as a translator, although he did not do so in the end because Veselnitskaya brought her own.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             |politicsNews                                                                                                  |December 27, 2017 |true |\n","|Trump on Twitter (Dec 26) - Hillary Clinton, Tax Cut Bill                     |The following statements were posted to the verified Twitter accounts of U.S. President Donald Trump, @realDonaldTrump and @POTUS.  The opinions expressed are his own. Reuters has not edited the statements or confirmed their accuracy.  @realDonaldTrump : - Based on the fact that the very unfair and unpopular Individual Mandate has been terminated as part of our Tax Cut Bill, which essentially Repeals (over time) ObamaCare, the Democrats & Republicans will eventually come together and develop a great new HealthCare plan! [0658 EST] - WOW, @foxandfrlends “Dossier is bogus. Clinton Campaign, DNC funded Dossier. FBI CANNOT (after all of this time) VERIFY CLAIMS IN DOSSIER OF RUSSIA/TRUMP COLLUSION. FBI TAINTED.” And they used this Crooked Hillary pile of garbage as the basis for going after the Trump Campaign! [0824 EST] - All signs are that business is looking really good for next year, only to be helped further by our Tax Cut Bill. Will be a great year for Companies and JOBS! Stock Market is poised for another year of SUCCESS! [17:17 EST] -- Source link: (bit.ly/2jBh4LU) (bit.ly/2jpEXYR)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      |politicsNews                                                                                                  |December 26, 2017 |true |\n","|U.S. appeals court rejects challenge to Trump voter fraud panel               |(Reuters) - A U.S. appeals court in Washington on Tuesday upheld a lower court’s decision to allow President Donald Trump’s commission investigating voter fraud to request data on voter rolls from U.S. states. The U.S. Court of Appeals for the District of Columbia Circuit said the Electronic Privacy Information Center (EPIC) watchdog group, which filed the lawsuit, did not have legal standing to seek to force the presidential commission to review privacy concerns before collecting individuals’ voter data. EPIC had argued that under federal law, the commission was required to conduct a privacy-impact assessment before gathering personal data. But the three-judge appeals court panel ruled unanimously that the privacy law at issue was intended to protect individuals, not groups like EPIC. “EPIC is not a voter,” Judge Karen Henderson wrote in the ruling.  Washington-based U.S. District Judge Colleen Kollar-Kotelly first denied EPIC’s injunction request in July, in part because the collection of data by the commission was not technically an action by a government agency so was not bound by laws that govern what such entities can do.  Kollar-Kotelly noted that the commission, headed by Vice President Mike Pence, was an advisory body that lacks legal authority to compel states to hand over the data. Most state officials who oversee elections and election law experts say that voter fraud is rare in the United States. Trump, a Republican, set up the commission in May after charging, without evidence, that millions of people voted unlawfully in the 2016 presidential election in which he defeated Democratic opponent Hillary Clinton despite losing the popular vote.  The commission’s vice chair, Kris Kobach, the Republican secretary of state for Kansas and an advocate of tougher laws on immigration and voter identification, asked states in June to turn over voter information. The data requested by Kobach included names, the last four digits of Social Security numbers, addresses, birth dates, political affiliation, felony convictions and voting histories.  More than 20 states refused outright and others said they needed to study whether they could provide the data. Civil rights groups and Democratic lawmakers have said the commission’s eventual findings could lead to new ID requirements and other measures making it harder for groups that tend to favor Democratic candidates to cast ballots. EPIC executive director Marc Rotenberg could not immediately be reached for comment.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             |politicsNews                                                                                                  |December 26, 2017 |true |\n","|Treasury Secretary Mnuchin was sent gift-wrapped box of horse manure: reports |(Reuters) - A gift-wrapped package addressed to U.S. Treasury Secretary Steven Mnuchin’s home in a posh Los Angeles neighborhood that was suspected of being a bomb was instead filled with horse manure, police told local media. The package was found Saturday evening in a next-door neighbor’s driveway in Bel Air, the Los Angeles Police Department told the Los Angeles Times and KNBC television, the NBC affiliate in Los Angeles. The package also included a Christmas card with negative comments about President Donald Trump and the new U.S. tax law signed by Trump last week. Reuters could not reach LAPD officials for comment on Sunday. An LAPD bomb squad X-rayed the package before opening it and found the horse manure inside, police told local media. Aerial footage from KNBC showed officers investigating a large box in wrapping paper, then dumping a large amount of what they later identified as the manure and opening the card that was included inside.  Mnuchin, who KNBC said was not home when the package was discovered, is a former Goldman Sachs Group Inc executive and Hollywood film financier. A road in Bel Air was closed for about two hours, KNBC reported. The U.S. Secret Service is also investigating the incident, according to the TV station.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         |politicsNews                                                                                                  |December 24, 2017 |true |\n","+------------------------------------------------------------------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------------------------------+------------------+-----+\n","only showing top 20 rows\n","\n"]}],"source":["df = true.union(fake)\n","df.show(truncate=False)"]},{"cell_type":"code","execution_count":13,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1684893065661,"user":{"displayName":"Quang Pham","userId":"17287612277936898722"},"user_tz":-420},"id":"k7MrKKHRj0XH"},"outputs":[],"source":["#select 2 cột cần\n","df = df.select(['class','text'])"]},{"cell_type":"markdown","metadata":{"id":"H0ME-xyu7Afr"},"source":["### clean data"]},{"cell_type":"code","source":["from pyspark.sql.functions import col, udf\n","from pyspark.sql.functions import isnan, when, count, col"],"metadata":{"id":"A3A7w74DAl1t","executionInfo":{"status":"ok","timestamp":1684893065662,"user_tz":-420,"elapsed":4,"user":{"displayName":"Quang Pham","userId":"17287612277936898722"}}},"execution_count":14,"outputs":[]},{"cell_type":"code","source":["df.select([count(when(isnan(c), c)).alias(c) for c in df.columns]).toPandas().T"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":112},"id":"FxrfKtWmApb8","executionInfo":{"status":"ok","timestamp":1684893073207,"user_tz":-420,"elapsed":7548,"user":{"displayName":"Quang Pham","userId":"17287612277936898722"}},"outputId":"1cbe0ad5-ce1b-4141-f270-04437b7fdd57"},"execution_count":15,"outputs":[{"output_type":"execute_result","data":{"text/plain":["       0\n","class  0\n","text   0"],"text/html":["\n","  <div id=\"df-1647c561-e0ce-4a0c-a29a-31b730229874\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>0</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>class</th>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>text</th>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-1647c561-e0ce-4a0c-a29a-31b730229874')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-1647c561-e0ce-4a0c-a29a-31b730229874 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-1647c561-e0ce-4a0c-a29a-31b730229874');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":15}]},{"cell_type":"code","source":["df.select([count(when(col(c).isNull(), c)).alias(c) for c in df.columns]).toPandas().T"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":112},"id":"tziePg04Apee","executionInfo":{"status":"ok","timestamp":1684893075011,"user_tz":-420,"elapsed":1810,"user":{"displayName":"Quang Pham","userId":"17287612277936898722"}},"outputId":"81a08066-b35d-4600-da20-00698a8ed1a3"},"execution_count":16,"outputs":[{"output_type":"execute_result","data":{"text/plain":["       0\n","class  0\n","text   8"],"text/html":["\n","  <div id=\"df-ab4324f1-5fa2-4569-a6ab-e6ef786f5ef5\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>0</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>class</th>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>text</th>\n","      <td>8</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-ab4324f1-5fa2-4569-a6ab-e6ef786f5ef5')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-ab4324f1-5fa2-4569-a6ab-e6ef786f5ef5 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-ab4324f1-5fa2-4569-a6ab-e6ef786f5ef5');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":16}]},{"cell_type":"code","source":["#số lượng dòng null ít nên ta drop luôn\n","df = df.dropna(how=\"any\", subset=[\"text\"])"],"metadata":{"id":"GiwxfiyRAu4r","executionInfo":{"status":"ok","timestamp":1684893075012,"user_tz":-420,"elapsed":6,"user":{"displayName":"Quang Pham","userId":"17287612277936898722"}}},"execution_count":17,"outputs":[]},{"cell_type":"code","execution_count":18,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1684893075012,"user":{"displayName":"Quang Pham","userId":"17287612277936898722"},"user_tz":-420},"id":"IudRXjNuj0Zq"},"outputs":[],"source":["from pyspark.sql.functions import length\n","df = df.withColumn('length',length(df['text']))"]},{"cell_type":"code","execution_count":19,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4162,"status":"ok","timestamp":1684893079171,"user":{"displayName":"Quang Pham","userId":"17287612277936898722"},"user_tz":-420},"id":"MCKRIG1Nj0cn","outputId":"1d1aab74-82eb-4894-d695-167add6d75ec"},"outputs":[{"output_type":"stream","name":"stdout","text":["+-----+------------------+\n","|class|       avg(length)|\n","+-----+------------------+\n","| true| 2372.866507914274|\n","| fake|2476.0307482645544|\n","+-----+------------------+\n","\n"]}],"source":["df.groupby('class').mean().show()"]},{"cell_type":"markdown","metadata":{"id":"DLbkuwwG7RBi"},"source":["### Feature Transformations"]},{"cell_type":"code","execution_count":20,"metadata":{"executionInfo":{"elapsed":1043,"status":"ok","timestamp":1684893080212,"user":{"displayName":"Quang Pham","userId":"17287612277936898722"},"user_tz":-420},"id":"F0cYkmX1j0fQ"},"outputs":[],"source":["from pyspark.ml.feature import Tokenizer,StopWordsRemover\n","from pyspark.ml.feature import CountVectorizer, IDF, StringIndexer\n","tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"token_text\")\n","stopremove = StopWordsRemover(inputCol='token_text',outputCol='stop_tokens')\n","count_vec = CountVectorizer(inputCol='stop_tokens',outputCol='c_vec')\n","idf = IDF(inputCol=\"c_vec\", outputCol=\"tf_idf\")\n","fake_true_to_num = StringIndexer(inputCol='class',outputCol='label')"]},{"cell_type":"code","execution_count":21,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1684893080212,"user":{"displayName":"Quang Pham","userId":"17287612277936898722"},"user_tz":-420},"id":"kgJRDL_v7NkC"},"outputs":[],"source":["from pyspark.ml.feature import VectorAssembler\n","from pyspark.ml.linalg import Vector"]},{"cell_type":"code","execution_count":22,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1684893080212,"user":{"displayName":"Quang Pham","userId":"17287612277936898722"},"user_tz":-420},"id":"TAeuSvVn7NmK"},"outputs":[],"source":["clean_up = VectorAssembler(inputCols=['tf_idf','length'],\n"," outputCol='features')"]},{"cell_type":"markdown","metadata":{"id":"pdteGB8e7aEK"},"source":["### thuật toán"]},{"cell_type":"code","execution_count":23,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1684893080212,"user":{"displayName":"Quang Pham","userId":"17287612277936898722"},"user_tz":-420},"id":"bdmYySek7Nor"},"outputs":[],"source":["from pyspark.ml.classification import NaiveBayes"]},{"cell_type":"code","execution_count":24,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1684893080213,"user":{"displayName":"Quang Pham","userId":"17287612277936898722"},"user_tz":-420},"id":"jLprOpwl7Nqx"},"outputs":[],"source":["nb = NaiveBayes()"]},{"cell_type":"markdown","metadata":{"id":"SB0sWktn7iIZ"},"source":["### pipeline"]},{"cell_type":"code","execution_count":25,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1684893080213,"user":{"displayName":"Quang Pham","userId":"17287612277936898722"},"user_tz":-420},"id":"bDd8x13S7uVD"},"outputs":[],"source":["from pyspark.ml import Pipeline"]},{"cell_type":"code","execution_count":26,"metadata":{"executionInfo":{"elapsed":642,"status":"ok","timestamp":1684893080852,"user":{"displayName":"Quang Pham","userId":"17287612277936898722"},"user_tz":-420},"id":"7BnhhjXE7NtJ"},"outputs":[],"source":["data_prep_pipe = Pipeline(stages=[fake_true_to_num,\n"," tokenizer,\n"," stopremove,\n"," count_vec,\n"," idf,\n"," clean_up])"]},{"cell_type":"code","execution_count":27,"metadata":{"id":"pGq9d3uZ7Nva","executionInfo":{"status":"ok","timestamp":1684893159789,"user_tz":-420,"elapsed":78938,"user":{"displayName":"Quang Pham","userId":"17287612277936898722"}}},"outputs":[],"source":["cleaner = data_prep_pipe.fit(df)"]},{"cell_type":"code","execution_count":28,"metadata":{"id":"vPnlgJob78RZ","executionInfo":{"status":"ok","timestamp":1684893162601,"user_tz":-420,"elapsed":2825,"user":{"displayName":"Quang Pham","userId":"17287612277936898722"}}},"outputs":[],"source":["clean_data = cleaner.transform(df)\n"]},{"cell_type":"markdown","source":["### Training and Evaluation!"],"metadata":{"id":"GxAIV203B_y8"}},{"cell_type":"code","execution_count":29,"metadata":{"executionInfo":{"elapsed":5,"status":"ok","timestamp":1684893162601,"user":{"displayName":"Quang Pham","userId":"17287612277936898722"},"user_tz":-420},"id":"UHV6f_FW78Ty"},"outputs":[],"source":["clean_data = clean_data.select(['label','features'])"]},{"cell_type":"code","source":["clean_data.show(10)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XuQSJfVBB7Pc","executionInfo":{"status":"ok","timestamp":1684893164453,"user_tz":-420,"elapsed":1856,"user":{"displayName":"Quang Pham","userId":"17287612277936898722"}},"outputId":"623182b8-be99-4a3e-b444-4586b1075714"},"execution_count":30,"outputs":[{"output_type":"stream","name":"stdout","text":["+-----+--------------------+\n","|label|            features|\n","+-----+--------------------+\n","|  1.0|(262145,[0,1,2,3,...|\n","|  1.0|(262145,[1,2,3,4,...|\n","|  1.0|(262145,[0,1,2,3,...|\n","|  1.0|(262145,[1,2,3,4,...|\n","|  1.0|(262145,[0,1,2,3,...|\n","|  1.0|(262145,[0,1,2,3,...|\n","|  1.0|(262145,[1,2,3,4,...|\n","|  1.0|(262145,[0,1,3,4,...|\n","|  1.0|(262145,[0,3,4,7,...|\n","|  1.0|(262145,[2,3,7,11...|\n","+-----+--------------------+\n","only showing top 10 rows\n","\n"]}]},{"cell_type":"code","execution_count":31,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1684893164454,"user":{"displayName":"Quang Pham","userId":"17287612277936898722"},"user_tz":-420},"id":"x7vHB0PK78WJ"},"outputs":[],"source":["(training,testing) = clean_data.randomSplit([0.7,0.3])"]},{"cell_type":"code","execution_count":32,"metadata":{"executionInfo":{"elapsed":54532,"status":"ok","timestamp":1684893218983,"user":{"displayName":"Quang Pham","userId":"17287612277936898722"},"user_tz":-420},"id":"4-kWgXbj78Yr"},"outputs":[],"source":["fake_predictor = nb.fit(training)"]},{"cell_type":"code","source":["test_results = fake_predictor.transform(testing)"],"metadata":{"id":"u_0P9qu1CHqO","executionInfo":{"status":"ok","timestamp":1684893218983,"user_tz":-420,"elapsed":13,"user":{"displayName":"Quang Pham","userId":"17287612277936898722"}}},"execution_count":33,"outputs":[]},{"cell_type":"code","source":["test_results.show(10)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"petSMM5SCHsW","executionInfo":{"status":"ok","timestamp":1684893234512,"user_tz":-420,"elapsed":15540,"user":{"displayName":"Quang Pham","userId":"17287612277936898722"}},"outputId":"dc7cd420-a5b6-46a8-a8ee-635bf883654c"},"execution_count":34,"outputs":[{"output_type":"stream","name":"stdout","text":["+-----+--------------------+--------------------+-----------+----------+\n","|label|            features|       rawPrediction|probability|prediction|\n","+-----+--------------------+--------------------+-----------+----------+\n","|  1.0|(262145,[0,1,2,3,...|[-28601.139384559...|  [0.0,1.0]|       1.0|\n","|  1.0|(262145,[0,1,2,3,...|[-45395.329228463...|  [0.0,1.0]|       1.0|\n","|  1.0|(262145,[0,1,2,3,...|[-33501.941258961...|  [0.0,1.0]|       1.0|\n","|  1.0|(262145,[0,1,2,3,...|[-34502.162250968...|  [0.0,1.0]|       1.0|\n","|  1.0|(262145,[0,1,2,3,...|[-29563.726578407...|  [0.0,1.0]|       1.0|\n","|  1.0|(262145,[0,1,2,3,...|[-25836.502210834...|  [0.0,1.0]|       1.0|\n","|  1.0|(262145,[0,1,2,3,...|[-22927.274778115...|  [0.0,1.0]|       1.0|\n","|  1.0|(262145,[0,1,2,3,...|[-25741.586223776...|  [0.0,1.0]|       1.0|\n","|  1.0|(262145,[0,1,2,3,...|[-26242.228394680...|  [0.0,1.0]|       1.0|\n","|  1.0|(262145,[0,1,2,3,...|[-34961.549325950...|  [0.0,1.0]|       1.0|\n","+-----+--------------------+--------------------+-----------+----------+\n","only showing top 10 rows\n","\n"]}]},{"cell_type":"code","source":["test_results.groupBy(\"label\", \"prediction\").count().show()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sAqER0B4CHu0","executionInfo":{"status":"ok","timestamp":1684893282822,"user_tz":-420,"elapsed":48322,"user":{"displayName":"Quang Pham","userId":"17287612277936898722"}},"outputId":"a2713834-514b-4119-e784-37b2ea1e8eea"},"execution_count":35,"outputs":[{"output_type":"stream","name":"stdout","text":["+-----+----------+-----+\n","|label|prediction|count|\n","+-----+----------+-----+\n","|  1.0|       1.0| 6319|\n","|  1.0|       0.0|   45|\n","|  0.0|       1.0|  164|\n","|  0.0|       0.0| 7011|\n","+-----+----------+-----+\n","\n"]}]},{"cell_type":"code","source":["from pyspark.ml.evaluation import MulticlassClassificationEvaluator"],"metadata":{"id":"3BrLWUdyCeT0","executionInfo":{"status":"ok","timestamp":1684893282822,"user_tz":-420,"elapsed":10,"user":{"displayName":"Quang Pham","userId":"17287612277936898722"}}},"execution_count":36,"outputs":[]},{"cell_type":"code","source":["acc_eval = MulticlassClassificationEvaluator()\n","acc = acc_eval.evaluate(test_results)\n","print(\"Accuracy of model at predicting fake was: {}\".format(acc))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tLbVByFFCeWT","executionInfo":{"status":"ok","timestamp":1684893327726,"user_tz":-420,"elapsed":44913,"user":{"displayName":"Quang Pham","userId":"17287612277936898722"}},"outputId":"ba991590-e27f-4760-c2f6-0ccd6dfd98c5"},"execution_count":37,"outputs":[{"output_type":"stream","name":"stdout","text":["Accuracy of model at predicting fake was: 0.9845700670211968\n"]}]},{"cell_type":"markdown","source":["thuật toán naive bayes đoán khá chính xác"],"metadata":{"id":"JzKfCK69itdO"}},{"cell_type":"markdown","source":["### sử dụng random forest"],"metadata":{"id":"ni7wfG4qCocV"}},{"cell_type":"code","source":["from pyspark.ml.classification import LinearSVC\n","LinearSVC_classifier = LinearSVC(labelCol=\"label\", maxIter=50)"],"metadata":{"id":"2bwP0b0zjw1P","executionInfo":{"status":"ok","timestamp":1684896198543,"user_tz":-420,"elapsed":415,"user":{"displayName":"Quang Pham","userId":"17287612277936898722"}}},"execution_count":54,"outputs":[]},{"cell_type":"code","source":["fake_predictor = LinearSVC_classifier.fit(training)"],"metadata":{"id":"kEAzHHU0Cebb","executionInfo":{"status":"ok","timestamp":1684897504763,"user_tz":-420,"elapsed":1303781,"user":{"displayName":"Quang Pham","userId":"17287612277936898722"}}},"execution_count":55,"outputs":[]},{"cell_type":"code","source":["test_results = fake_predictor.transform(testing)"],"metadata":{"id":"NgGD7Xy5Cedz","executionInfo":{"status":"ok","timestamp":1684897504764,"user_tz":-420,"elapsed":8,"user":{"displayName":"Quang Pham","userId":"17287612277936898722"}}},"execution_count":56,"outputs":[]},{"cell_type":"code","source":["test_results.show(10)"],"metadata":{"id":"Q3khMKD6CegR","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1684897521553,"user_tz":-420,"elapsed":16795,"user":{"displayName":"Quang Pham","userId":"17287612277936898722"}},"outputId":"0810b257-3c9e-4b28-9b1d-6a121b394e53"},"execution_count":57,"outputs":[{"output_type":"stream","name":"stdout","text":["+-----+--------------------+--------------------+----------+\n","|label|            features|       rawPrediction|prediction|\n","+-----+--------------------+--------------------+----------+\n","|  1.0|(262145,[0,1,2,3,...|[6.21094190306376...|       0.0|\n","|  1.0|(262145,[0,1,2,3,...|[1.06525454119501...|       0.0|\n","|  1.0|(262145,[0,1,2,3,...|[1.47649427896008...|       0.0|\n","|  1.0|(262145,[0,1,2,3,...|[4.65248243134752...|       0.0|\n","|  1.0|(262145,[0,1,2,3,...|[3.49845185026153...|       0.0|\n","|  1.0|(262145,[0,1,2,3,...|[5.22877763998388...|       0.0|\n","|  1.0|(262145,[0,1,2,3,...|[-5606036.1218384...|       1.0|\n","|  1.0|(262145,[0,1,2,3,...|[2.31473820466824...|       0.0|\n","|  1.0|(262145,[0,1,2,3,...|[4.27825186582687...|       0.0|\n","|  1.0|(262145,[0,1,2,3,...|[6.09289824271324...|       0.0|\n","+-----+--------------------+--------------------+----------+\n","only showing top 10 rows\n","\n"]}]},{"cell_type":"code","source":["test_results.groupBy(\"label\", \"prediction\").count().show()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vXzu3VS2aN7N","executionInfo":{"status":"ok","timestamp":1684897572477,"user_tz":-420,"elapsed":50928,"user":{"displayName":"Quang Pham","userId":"17287612277936898722"}},"outputId":"d33cc1b2-8ee4-4a97-dd10-10716becd369"},"execution_count":58,"outputs":[{"output_type":"stream","name":"stdout","text":["+-----+----------+-----+\n","|label|prediction|count|\n","+-----+----------+-----+\n","|  1.0|       1.0| 5259|\n","|  1.0|       0.0| 1105|\n","|  0.0|       1.0| 2726|\n","|  0.0|       0.0| 4449|\n","+-----+----------+-----+\n","\n"]}]},{"cell_type":"code","source":["acc_eval = MulticlassClassificationEvaluator()\n","test_results = test_results.withColumn(\"prediction\", test_results[\"prediction\"].cast('double'))\n","acc = acc_eval.evaluate(test_results)\n","print(\"Accuracy of model at predicting fake was: {}\".format(acc))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7DqGlymqaN9y","executionInfo":{"status":"ok","timestamp":1684897617660,"user_tz":-420,"elapsed":45203,"user":{"displayName":"Quang Pham","userId":"17287612277936898722"}},"outputId":"3c4f3461-7f32-4741-eaab-783e3016edda"},"execution_count":59,"outputs":[{"output_type":"stream","name":"stdout","text":["Accuracy of model at predicting fake was: 0.7150055380174876\n"]}]},{"cell_type":"markdown","source":["thuật toán linearSVC tỷ lệ chính xác cũng tương đối\n","\n"],"metadata":{"id":"tllRMfSti9tO"}}],"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyM8Hn7eiy+/o6XY0tItSBxg"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}